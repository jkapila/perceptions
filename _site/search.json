[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Percieve It!",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCorssTab Sparsity for Classification\nCan our metric help us in making a classification problem work better ?\n\n \n\n\nclassification\n\nmetric\n\nfeature selection\n\n\n \n\n\n\n\n\nJan 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCorssTab Sparsity\nA label and data type agnostic metric for evaluating clustering performance!\n\n \n\n\neda\n\nhypothesis\n\n\n \n\n\n\n\n\nMay 3, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA flow to Test Your Hypothesis in Python\nMaking life easy to do some serious hypothesis testing in python.\n\n \n\n\neda\n\nhypothesis\n\nanalysis\n\npython\n\n\n \n\nA simple code to run your hypothesis test.\n\n\n\nAug 10, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdaptive Regression\nWe recently put through our observation on Regression Problem in our research. This post is a nonformal attempt to explain it.\n\n\n \n\n\nregression\n\nresearch\n\nanalysis\n\n\n \n\nIf things are simple lets keep it simple. [Paper here](https://arxiv.org/abs/1805.01618)\n\n\n\nMay 1, 2018\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/03_crosstab_sparsity/index.html",
    "href": "posts/03_crosstab_sparsity/index.html",
    "title": "CorssTab Sparsity",
    "section": "",
    "text": "Cluster analysis has always fascinated me as a window into the hidden structures of data. During my collaboration with Kumarjit Pathak, we grappled with a persistent challenge in unsupervised learning: how to objectively evaluate clustering quality across different algorithms. Traditional metrics like the Silhouette Index or Bayesian Information Criterion felt restrictive—they were siloed within specific methodologies, making cross-algorithm comparisons unreliable.\nThis frustration led us to develop a universal cluster evaluation metric, detailed in our paper “Cross Comparison of Results from Different Clustering Approaches”. Our goal was to create a framework that transcends algorithmic biases, enabling:\n- Direct comparison of K-Means vs GMM vs DBSCAN vs PAM vs SOM vs Anything results\n- Identification of variables muddying cluster separation\n- Automated determination of optimal cluster counts\nIn this blog, I’ll walk you through our journey—from conceptualization to real-world validation—and share insights that didn’t make it into the final paper."
  },
  {
    "objectID": "posts/03_crosstab_sparsity/index.html#our-aha-moment---crosstab-sparsity",
    "href": "posts/03_crosstab_sparsity/index.html#our-aha-moment---crosstab-sparsity",
    "title": "CorssTab Sparsity",
    "section": "Our “Aha!” Moment - Crosstab Sparsity",
    "text": "Our “Aha!” Moment - Crosstab Sparsity\n\n\n\nBest Cluster for K-means Using Crosstab sparsity\n\n\nWhile analyzing cross-tab matrices of variable distributions across clusters, we noticed a pattern: well-segregated clusters consistently showed higher frequencies along matrix diagonals. This inspired our two-part metric:\n\n\n\nSegregation Factor:\n# Simplified calculation from our codebase  \nmedian = np.median(cross_tab)  \nN_vk = np.sum(cross_tab &gt; median)  # Count \"well-segregated\" instances  \nExplanation Factor:\nexplanation = np.log(len(data) / (bins * clusters))  \n\n\nSegregation Factor: Measures how distinctly clusters separate data points. We used the median (not mean) to avoid skew from outlier-dominated matrices.\nExplanation Factor: Quantifies how well clusters capture data variability. The logarithmic term penalizes overfitting—a critical insight from debugging early over-segmented clusters.\n\nAnd the Final Formula:\nFor variable \\(v\\) with \\(k\\) clusters:\n\\[\nS_v^k = \\underbrace{\\frac{N_v^k}{\\max(l, k)}}_{\\text{Segregation}} \\times \\underbrace{\\ln\\left(\\frac{N_d}{l \\times k}\\right)}_{\\text{Explanation}}\n\\]\nwhere:\n- \\(N_v^k\\): Segregated instances (values above cross-tab matrix median)\n- \\(l\\): Number of value intervals for variable \\(v\\)\n- \\(N_d\\): Total observations\nThis formulation ensures algorithmic invariance, allowing comparison across methods like K-Means (distance-based) and GMM (probability-based). Also, now you can see from the formula two scenarios happens: 1. If each variable crosstab is too dense then their is no separation between classes 2. If each variable crosstab is too sparse then we loose on explanation.\nHence the curve reaches a maximum and then falls down giving use the separability that the cluster can produce:"
  },
  {
    "objectID": "posts/01_adaptive_regression/index.html",
    "href": "posts/01_adaptive_regression/index.html",
    "title": "Adaptive Regression",
    "section": "",
    "text": "Introduction\nHere I am trying to express our logic to find such Observation. Lets dive in.\nThere are different value estimation technique like regression analysis and time-series analysis. Everyone of us has experimented on regression using OLS ,MLE, Ridge, LASSO, Robust etc., and also might have evaluated them using RMSE (Root Mean/Median Square Error), MAD (Mean/Median Absolute Deviation), MAE (Mean / Median Absolute Error) and MAPE (Mean/Median Absolute Percentage Error), etc…\nBut all of these gives a single point estimate that what is the overall error looks like. Just a different thought!! can we be sure that this single value of MAPE or MAE? How easy it is to infer that our trained model has fitted well across the distribution of dependent variable?\n\n\n\n\n\nPlot of Anscombe’s Quartet\n\n\n\n\n\nSome Descriptive Stats for Anscombe’s Quartet\n\n\nLet me give you a pretty small data-set to play with “The Anscombe’s quartet”. This is a very famous data-set by Francis Anscombe. Please refer the plots below to understand the distribution of y1, y2, y3, y4. Isn’t it different?\nWould the measure of central tendency and disportion be same for this data? I am sure none of us would believe but to our utter surprise we see all the descriptive stats are kind of same. Don’t believe me !!! Please see the results below ( Source: Wikipedia ):\n\n\nSo what we do Now!\nAstonished !!! Don’t be. This is what has been hiding behind those numbers. And this is why we really won’t be able to cross certain performance level. Unless you change some features or even do a lot of hyper parameter tuning, your results won’t vary much.\nIf you look at the average value of MAPE in each decile you would see an interesting pattern. Let us show you what we see that pattern. One day while working on a business problem where I was using regression on a discussion with Kumarjit, we deviced a different way of model diagnosis. We worked together to give this a shape and build on it.\n\nAs you can see it is absolutely evident that either of the side in the distribution of MAPE values is going wild!!!!!!! Still overall MAPE is good (18%).\n\n\nSeeking Scope of Improvement\nWe worked together to build a different framework to address such issues on the go and reduce the MAPE deterioration on the edge of the distribution.\nThis problems gives rise to a concept we named as Distribution Assertive Regression (DAR).\nDAR is a framework that is based on cancelling the weakness of one point summaries by using the classical concepts of Reliability Engineering : The Bath Tub Curve.\n\n\n\n\n\nPlot for Classical Bath Tub Curve using a Hazard Function\n\n\nThe Specialty of this curve is that it gives you the likelihood which areas one tends to have high failure rates. In our experiments when we replace failure with MAPE value and the Time with sorted (ascending) value of target / dependent variable, we observe the same phenomenon. This is likely to happen because most of regression techniques assumes Normal (Gaussian) Distribution of data and fits itself towards the central tendency of this distribution.\nBecause of this tendency, any regression methods tends to learn less about data which are away from the central tendency of the target.\nLets look at BostonHousing data from “mlbench” package in R.\n\n\n\nPlot for MAPE Bath Tub Curve for Decile Split “mdev” from Data\n\n\nHere the MAPE is calculated for each decile split of ordered target variable. As you can observe it is following the bath tub curve. Hence the validates our hypothesis that the regression method is not able to understand much about the data at the either ends of the distribution.\n\n\nFinal Analysis\nNow the DAR framework essentially fixes this weakness of regression method and understands the behavior of data which is stable and can be tweak in a fashion that can be use in general practice.\nPlot of MAPE Bath Tub Curve after applying DAR Framework for Decile Split “mdev” from Data\n\nHow this framework with same method reduced MAPEs so much and made model much more stable…?? Well here it is:\nThe DAR framework splits the data at either ends of the order target variable and performs regression on these “split” data individually. This inherently reduces the so called “noise” part of the data and treat it as an individual data.\n\n\nScoring on New Data\nNow you might be thinking while applying regression this sounds good but how will one score this on new data. Well to answer that we used our most simple yet very effective friend “KNN” (Though any multiclass Classifier can be used here). So ideally scoring involves two step method :\n\nScore new value against each KNN / Multiclass Classifier model of the data\nBased on closeness we score it with the regression method used for that part of data.\n\nSo now we know how we can improve the prediction power of data for regression.\n\n\nCode and Flowchart\nRefer flowchart and code below for implementation of this framework!\n\nHere is the codeHere is the Flow Chart\n\n\n\n\n\n\n\n\nR and Python Code\n\n\n\n\n\n\nR CodePython Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngraph TB\n    \n    subgraph Testing\n        p1(Finding bucket of model to choose)\n        p1 --&gt; p2([Making predictions &lt;br&gt; based on selected model for inference])\n        p2 --&gt; p3(Consolidate final score of prediction)\n    end\n\n    subgraph Training\n        md([Fitting a &lt;br&gt;Regression model])==&gt; di\n        di{Binning Data via &lt;br/&gt; evaluating Distribution &lt;br/&gt; MAPE values }\n        di --&gt; md2([Fitting a Buckteing model &lt;br/&gt; to Binned MAPE Buckets])\n        md2 --&gt; md3([Fitting Regression &lt;br&gt; Models on Binned Data])\n        md == Keeping main&lt;br/&gt;model ==&gt; ro        \n        md3 ==&gt; ro(Final Models &lt;br&gt; Binning Data Models + &lt;br&gt; Set of Regressoin Models)\n    end\n\n    \n    od([Data Input]) -- Training&lt;br&gt; Data--&gt; md\n    od -- Testing&lt;br&gt; Data--&gt; p1\n    ro -.-&gt; p1\n    ro -.-&gt; p2\n\n    classDef green fill:#9f6,stroke:#333,stroke-width:2px;\n    classDef yellow fill:#ff6,stroke:#333,stroke-width:2px;\n    classDef blue fill:#00f,stroke:#333,stroke-width:2px,color:#fff;\n    classDef orange fill:#f96,stroke:#333,stroke-width:4px;\n    class md,md2,md3 green\n    class di orange\n    class p1,p2 yellow\n    class ro,p3 blue\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "posts/02_hypothesis_test/index.html",
    "href": "posts/02_hypothesis_test/index.html",
    "title": "A flow to Test Your Hypothesis in Python",
    "section": "",
    "text": "Overview\nAll the practioners of data science always hit one giant thing to do with data and you know it well its EDA -Exploratory Data Analysis. This word EDA1 was coined by Tukey himself in his seminal book publised in 1983. But do you think that before that EDA dosen’t exsisted ?\n1 Emerson, J. D., & Hoaglin, D. C. (1983). Stem-and-leaf displays. In D. C. Hoaglin, F. Mosteller, & J. W. Tukey (Eds.) Understanding Robust and Exploratory Data Analysis, pp. 7–32. New York: Wiley. Book is here.Well glad you thought. Before that all were doing what is called as Hypothesis Tesing. Yes, before this the race was majorly to fit the data and make most unbiased and robust estimate. But remember one thing when you talk about Hypothesis Testing it was always and majorly would be related to RCTs (Randomized Controlled Trials) a.k.a Randomized Clinical Trials and is Gold Standard of data.\n\n\n\n\n\n\nMore on RCTs and ODs\n\n\n\n\n\nNow let me now not hijack the discussion to what is RCTs and Observational Data (ODs) as it is more of Philosphical Reasoning rather than other quality of data, but essentially what we are trying to find is that can we by, using stats, identify interesting patterns in data.\nThe only thing happens wit RCT data is that we tend to believe these intresting patterns coincide with some sort of ‘Cause-Effect’ kind of relationship. But essentially due to bia nature of ODs, we certainly cant conclude this. And hence, can only find intresting patterns.\n\n\n\nLets move on. The big question is, for whatever reason you are doing HT , you are doing it for finding something intresting. And that something intresting is usually found by using Post-Hoc Tests. Now there are variety of Post-Hocs available but what is more know and hence easily found to be implemented in Tukey’s HSD.\nSo lets directly jump to how to follow this procedure. We’ll be using bioinfokit for this, as it is much simpler wrapper around whats impelmented in statsmodels.\n\n\nWhat are the results\nPheww… Thats too much code right. But that would save a lot of your time in real life. So in reallife you would write code as 3 steps below:\n\n# import libraries\nimport pandas as pd\n\n# Getting car data from UCI\ndf = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data',\n                 sep='\\s+',header=None,\n                 names=['mpg','cylinders','displacement','horsepower','weight',\n                 'acceleration','model_year','origin','car_name'])\ndf.head()\n\n# Syntax to do anove with validating the assumption, doing test and a post-hoc\nresults = do_anova_test(df=df, res_var='mpg',xfac_var='cylinders', \n                        anova_model='mpg ~ C(cylinders)+C(origin)+C(cylinders):C(origin)',\n                        ss_typ=3, result_full=True)\n\nResults form the do_anova_test\nLevens Test Result:\n                 Parameter    Value\n0      Test statistics (W)  14.5856\n1  Degrees of freedom (Df)   4.0000\n2                  p value   0.0000\n\nBartletts Test Result:\n                 Parameter    Value\n0      Test statistics (T)  61.2143\n1  Degrees of freedom (Df)   4.0000\n2                  p value   0.0000\n\nANOVA\\ANCOVA Test Result:\n                           df     sum_sq    mean_sq         F  PR(&gt;F)      n2\nIntercept                 1.0  6195.1701  6195.1701  296.3452  0.0000  0.2727\nC(cylinders)              4.0  7574.5864  1893.6466   90.5824  0.0000  0.3334\nC(origin)                 2.0   241.0703   120.5351    5.7658  0.0034  0.0106\nC(cylinders):C(origin)    8.0   577.4821    72.1853    3.4530  0.0046  0.0254\nResidual                389.0  8132.1404    20.9052       NaN     NaN     NaN\n\nTukey HSD Result:\n   group1  group2     Diff    Lower    Upper  q-value  p-value\n0       8       4  14.3237  12.8090  15.8383  36.6527   0.0010\n1       8       6   5.0226   3.1804   6.8648  10.5671   0.0010\n2       8       3   5.5869  -0.7990  11.9728   3.3909   0.1183\n3       8       5  12.4036   5.0643  19.7428   6.5503   0.0010\n4       4       6   9.3011   7.6765  10.9256  22.1910   0.0010\n5       4       3   8.7368   2.4102  15.0633   5.3524   0.0017\n6       4       5   1.9201  -5.3676   9.2078   1.0212   0.9000\n7       6       3   0.5643  -5.8486   6.9772   0.3410   0.9000\n8       6       5   7.3810   0.0182  14.7437   3.8854   0.0491\n9       3       5   6.8167  -2.7539  16.3873   2.7606   0.2919\nNice!!!\n\nAnd plotting is even easier\n\n# Numbers are clumsy for most. Making more interpretable plot on above results.\nplot_hsd(results.tukeyhsd.sort_values('Diff'), title=\"Tukey HSD resutls Anova of MPG ~ Cylinder\")\n\nResults form the plot_hsd\n\n\n\nTukey’s HSD comparision based on Anova Results\n\n\nPlots look good with ‘p-values’.\n\n\nConclusion\nNow since we applied the above to a Non RCT we cannot conclude that Difference in mpg based on cylinder is huge specially as number of cylinders goes up. But this statement might not be as explicit as might be apperiang from plot. Unless you have a strong believe that the data follows with rules and assumptions of RCTs, we should be only seeking intresting as in associated results and not cause-effet resluts.\n\n\nGive me “The Code”\n\nPerforming AnovaPlotting Results\n\n\nanova_test.py\n\nfrom bioinfokit import analys\n\nimport numpy as np\nfrom scipy import stats\n\n\n# Anova test code\ndef do_anova_test(df, res_var, xfac_var, anova_model,ss_typ=3,\n                  effectsize='n2',result_full=False,add_res=False):\n    \"\"\"\n    Do all sequential anova tests\n    \n    Step 1) Leven's/ bartellet test for checking weather variance is homogenous or not\n    Step 2) Main ANOVA/ANCOVA test\n    Step 3) Tukey's HSD for individual combinations\n    \n    :param df: Pandas DataFrame holding all the columns\n    :param res_var: Variable for which we are checking ANOVA\n    :param xfac_var: Grouping Variables for which we want to do the comparisions\n    :param anova_model: SM forula for the model. This is life savour to make all things work\n    :param result_full: To provide all the results of intermediate steps\n    \n    \"\"\"\n    \n    class KeyResults:\n        \"\"\"\n        A basic class to hold all the results\n        \"\"\"\n        \n        def __init__(self,result_full):\n            self.keys = []\n            self.result_full = result_full\n        \n        def add_result(self,name,result):\n            if name == 'tukeyhsd':\n                self.keys.append(name)\n                setattr(self, name, result)\n            elif self.result_full:\n                self.keys.append(name)\n                setattr(self, name, result)\n            \n    results = KeyResults(result_full)\n    \n    # initiallize stat method\n    res = analys.stat()\n    \n    # doing levens test\n    res.levene(df=df, res_var=res_var,xfac_var=xfac_var)\n    print('\\nLeven\\'s Test Result:')\n    print(res.levene_summary)\n    results.add_result('levene',res.levene_summary)\n\n    # doing bartlett test\n    res.bartlett(df=df, res_var=res_var,xfac_var=xfac_var)\n    print('\\nBartlett\\'s Test Result:')\n    print(res.bartlett_summary)\n    results.add_result('bartlett',res.bartlett_summary)\n    \n    # doing anova / ancova\n    res.anova_stat(df=df, res_var=res_var, anova_model=anova_model,ss_typ=ss_typ)\n    aov_res = res.anova_summary\n    \n    # Add effect sizes\n    if effectsize == \"n2\":\n        all_effsize = (aov_res['sum_sq'] / aov_res['sum_sq'].sum()).to_numpy()\n        all_effsize[-1] = np.nan\n    else:\n        ss_resid = aov_res['sum_sq'].iloc[-1]\n        all_effsize = aov_res['sum_sq'].apply(lambda x: x / (x + ss_resid)).to_numpy()\n        all_effsize[-1] = np.nan\n    aov_res[effectsize] = all_effsize\n    #aov_res['bw_'] = res.anova_model_out.params.iloc[-1]\n    aov_res = aov_res.round(4)\n    \n    # printing results\n    print('\\nANOVA\\ANCOVA Test Result:')\n    print(aov_res)\n    results.add_result('anova',res.anova_summary.round(4))\n    results.add_result('anova_model',res.anova_model_out)\n    \n    # doing tukey's hsd top compare the groups\n    res.tukey_hsd(df=df, res_var=res_var,xfac_var=xfac_var, anova_model=anova_model,ss_typ=ss_typ)\n    print('\\nTukey HSD Result:')\n    print(res.tukey_summary.round(4))\n    results.add_result('tukeyhsd',res.tukey_summary.round(4))\n    \n    # add all result componets again if needed \n    if add_res:\n        results.add_result('allresult',res)\n    \n    return results\n\n\n\nplot_hsd.py\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.style.use('seaborn-bright')\n\ndef plot_hsd(hsdres,p_cutoff=0.05,title=None,ax=None,figsize=(10,7)):\n     \"\"\"\n     Do plotting of tukeyhsd results\n    \n  \n    :param hsdres: 'tukeyhsd' result form the do_anova_test function\n    :param p_cutoff: Cutoff at which we get say a combination is significant\n    :param title: Title of the plot\n    :param ax: Define or get the matplotlib axes\n    :param figsize: Mention Figure size to draw\n    \n    \"\"\"\n\n    if ax is None:\n        fig,axp = plt.subplots(figsize=figsize)\n    else:\n        axp = ax\n    \n    # helper func\n    p_ind = lambda x : '' if x &gt; 0.1 else ('+' if x &gt; 0.05 else ('*' if x &gt; 0.01 else ('**' if x &gt;0.001 else '***')))\n    label_gen  = lambda x: f\"${x[0]} - {x[1]}\\ |\\ p:{x[2]:0.2f}{p_ind(x[2]):5s}$\"\n    \n    #setting values\n    mask = hsdres['p-value'] &lt;= p_cutoff\n    yticklabs = hsdres[['group1','group2','p-value']].apply(label_gen,axis=1).values\n    ys = np.arange(len(hsdres))\n    \n    # adding plot to axes\n    axp.errorbar(hsdres[~mask]['Diff'],ys[~mask],xerr=np.abs(hsdres[~mask][['Lower',\"Upper\"]]).values.T,\n                fmt='o', color='black', ecolor='lightgray', elinewidth=2, capsize=0)\n    axp.errorbar(hsdres[mask]['Diff'],ys[mask],xerr=np.abs(hsdres[mask][['Lower',\"Upper\"]]).values.T,\n                fmt='o', color='red', ecolor='pink', elinewidth=2, capsize=5)\n    axp.axvline(x=0,linestyle='--',c='skyblue')\n    axp.set_yticks([])\n    (l,u) = axp.get_xlim()\n    axp.set_xlim(l+1.5*l,u)\n    (l,u) = axp.get_xlim()\n    for idx,labs in enumerate(yticklabs):\n        axp.text(l-0.1*l,ys[idx],labs)\n    axp.set_yticklabels([])\n    \n    # finally doing what is needed\n    if ax is None:\n        plt.title('' if title is None else title,fontsize=14)\n        plt.show()\n    else:\n        return axp\n\n\n\n\nHope this give you kickstart to find you intresting patterns. Happy Learning!\n\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "posts/04_crosstab_sparsity_classification/index.html",
    "href": "posts/04_crosstab_sparsity_classification/index.html",
    "title": "CorssTab Sparsity for Classification",
    "section": "",
    "text": "Picture this: you’re standing on the icy shores of Antarctica, the wind whipping around you as you watch a colony of Palmer Penguins waddling about, oblivious to the data detective work you’re about to embark on. As a data science architect, you’re not just an observer; you’re a sleuth armed with algorithms and insights, ready to unravel the mysteries hidden within data. Today, we’ll transform raw numbers into powerful narratives using CrossTab Sparsity as our guiding compass. This blog post will demonstrate how this metric can revolutionize classification tasks, shedding light on many fascinating datasets—the charming Palmer Penguins and the serious Obesity, Credit cards data and many more."
  },
  {
    "objectID": "posts/04_crosstab_sparsity_classification/index.html#footnotes",
    "href": "posts/04_crosstab_sparsity_classification/index.html#footnotes",
    "title": "CorssTab Sparsity for Classification",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is based on work in Unique Metric for Health Analysis with Optimization of Clustering Activity and Cross Comparison of Results from Different Approach. Paper Link]↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "“Tao Te Ching, chapter 36, translation by Stephen Mitchell” &gt; As Lao Tzu says:\n If you want to shrink something, you must first allow it to expand.\nIf you want to get rid of something, you must first allow it to flourish.\nIf you want to take something, you must first allow it to be given.\nThis is called the subtle perception of the way things are.\n The soft overcomes the hard. The slow overcomes the fast.\nLet your workings remain a mystery. Just show people the results. \nWell I am born and bought up in Bhilai, C.G (a Tire-II City of India). A guy who just loved to look around and Observe. Though at a very young age it just made me lose my interest in standard schooling and constrained studies, yet it’s the exactly same thing that has helped me build my career.\nI have graduated as a Mechanical Engineer, completed my Post Grads in Thermal Engineering which eventually took a twisted turn to make me Dive into Data Science. I am presently working with Ascena Retail GIC, as Senior Data Scientist, where I am responsible to create End-to-End analytical solutions that help The Business make Informed Decisions.\nI have worked on various projects from Retail Analytics to Fault Detection to Forecasting. Over the years my understanding of various concepts in Machine Learning and Deep Learning has gained those Eyes of Observations. This blog is my attempt to make those concepts more intuitive and easy, yet it would help you to form the basis of The Work we do that creates an Impact.\nTill next time, stay observant, stay crazy, stay safe, stay healthy."
  }
]