[
  {
    "objectID": "posts/10_treeknn/index.html",
    "href": "posts/10_treeknn/index.html",
    "title": "TrieKNN: Unleashing KNN’s Power on Mixed Data Types",
    "section": "",
    "text": "Photo by Gelgas Airlangga"
  },
  {
    "objectID": "posts/10_treeknn/index.html#the-allure-and-limitation-of-knn",
    "href": "posts/10_treeknn/index.html#the-allure-and-limitation-of-knn",
    "title": "TrieKNN: Unleashing KNN’s Power on Mixed Data Types",
    "section": "The Allure and Limitation of KNN",
    "text": "The Allure and Limitation of KNN\nIn the realm of machine learning, the K-Nearest Neighbors (KNN) algorithm stands out for its intuitive nature and ease of implementation. Its principle is simple: classify a data point based on the majority class among its ‘k’ nearest neighbors in the feature space. This non-parametric approach makes no assumptions about the underlying data distribution, rendering it versatile for various applications. KNN is very popular, but it comes with some limitations.\nHowever, KNN’s Achilles’ heel lies in its reliance on distance metrics, which are inherently designed for numerical data. Real-world datasets often contain a mix of numerical and categorical features, posing a significant challenge for KNN. How do you measure the distance between ‘red’ and ‘blue,’ or ‘large’ and ‘small’?\n\nPrior Art\nSeveral strategies have been proposed to adapt KNN for mixed data:\n\nOne-Hot Encoding: Converts categorical features into numerical vectors, but can lead to high dimensionality.\nDistance Functions for Mixed Data: Develops and apply custom distance metrics that can handle both numerical and categorical features such as HEOM and many others.\nUsing mean/mode values: Replace the missing values with mean/mode.\n\nThese methods often involve compromises, either distorting the data’s inherent structure or adding computational overhead."
  },
  {
    "objectID": "posts/10_treeknn/index.html#enter-trieknn-a-novel-approach",
    "href": "posts/10_treeknn/index.html#enter-trieknn-a-novel-approach",
    "title": "TrieKNN: Unleashing KNN’s Power on Mixed Data Types",
    "section": "Enter TrieKNN: A Novel Approach",
    "text": "Enter TrieKNN: A Novel Approach\nWhat if we could cleverly sidestep the distance calculation problem for categorical features, while still leveraging KNN’s power? TrieKNN offers just that—a way to perform KNN on any mixed data!\nTrieKNN combines the strengths of Trie data structures and KNN to handle mixed data types gracefully. Here’s the core idea:\n\nTrie-Based Categorical Encoding: A Trie is used to store the categorical features of the data. Each node in the Trie represents a category.\nLeaf-Node KNN Models: At the leaf nodes of the Trie, where specific combinations of categorical features are found, we fit individual KNN models using only the numerical features.\nWeighted Prediction: To classify a new data point, we traverse the Trie based on its categorical features. At each level, we calculate a weighted distance based on available data, ending in a probability score in each leaf node.\n\n\nWhy This Works\n\nNo Direct Distance Calculation for Categorical Features: The Trie structure implicitly captures the relationships between categorical values.\nLocalized KNN Models: By fitting KNN models at the leaf nodes, we ensure that distance calculations are performed only on relevant numerical features.\nScalability: The Trie structure efficiently handles a large number of categorical features and values."
  },
  {
    "objectID": "posts/10_treeknn/index.html#building-a-trieknn-model",
    "href": "posts/10_treeknn/index.html#building-a-trieknn-model",
    "title": "TrieKNN: Unleashing KNN’s Power on Mixed Data Types",
    "section": "Building a TrieKNN Model",
    "text": "Building a TrieKNN Model\nLet’s dive into the implementation. We’ll start with the TrieNode and Trie classes, then move on to the KNN model and the training/prediction process.\n\nTrie Implementation\n\n\nCode\nimport numpy as np\nfrom collections import Counter\n\nclass TrieNode:\n    def __init__(self):\n        self.children = {}  # Dictionary to store child nodes\n        self.is_end_of_word = False  # True if the node is the end of a word\n        self.count = 0  # Count of how many times a word has been inserted\n        self.class_counts = {}  # Class counts\n        self.class_weights = {}\n        self.model = None  # Model at leaf nodes\n        self.indexes = []  # Store data indexes belonging to this leaf\n        self.labels = []  # Store data indexes belonging to this leaf\n        self.node_weight = None\n\nclass Trie:\n    def __init__(self):\n        self.root = TrieNode()  # Root node of the Trie\n        self.data_index = 0  # Initialize data index\n\n    def insert(self, word_val, model):\n        current_node = self.root\n        word, val = word_val\n        current_node.count += 1\n\n        # Adding class counts\n        if val not in current_node.class_counts:\n            current_node.class_counts[val] = 0\n        current_node.class_counts[val] += 1\n\n        for char in word:\n            # If the character is not in children, add a new TrieNode\n            if char not in current_node.children:\n                current_node.children[char] = TrieNode()\n            current_node = current_node.children[char]\n\n            # Adding count of instances\n            current_node.count += 1\n\n            # adding class counts\n            if val not in current_node.class_counts:\n                current_node.class_counts[val] = 0\n            current_node.class_counts[val] += 1\n\n        # Mark the end of the word and increment count\n        current_node.is_end_of_word = True\n        current_node.indexes.append(self.data_index)  # Store the data index\n        current_node.labels.append(val)\n        current_node.model = model\n        self.data_index += 1  # Increment data index\n\n    def search(self, word):\n        current_node = self.root\n        for char in word:\n            # If the character doesn't exist in the children, the word doesn't exist\n            if char not in current_node.children:\n                return False\n            current_node = current_node.children[char]\n\n        # Return True if it's the end of a word and the word exists\n        return current_node.is_end_of_word\n\n    def count_word(self, word):\n        current_node = self.root\n        for char in word:\n            # If the character doesn't exist, the word doesn't exist\n            if char not in current_node.children:\n                return 0, current_node.class_counts  # Correctly return class_counts\n            current_node = current_node.children[char]\n\n        # Return the count of the word\n        return current_node.count, current_node.class_counts\n\n    def display(self):\n        # Recursively display the tree\n        def _display(node, word):\n            if node.is_end_of_word:\n                print(f\"Data: {word}, Count: {node.count}, Indexes: {len(node.indexes)} Classes :{node.class_counts} weights:{len(node.class_weights)}\")  # Display indexes too\n            for char, child in node.children.items():\n                _display(child, word + char)  # corrected the display\n\n        _display(self.root, \"\")\n\n    def apply(self, func):\n        \"\"\"\n        Applies a function to all models in the leaf nodes.\n        \"\"\"\n        def _apply(node):\n            if node.is_end_of_word and node.model is not None:\n                func(node)\n            for child in node.children.values():\n                _apply(child)\n\n        _apply(self.root)\n\n    def apply_weight_to_indexes(self, weight):\n        \"\"\"\n        Applies a weight to the indexes based on the percentage of data available.\n        \"\"\"\n        def _apply_weight_to_indexes(node):\n            if node.is_end_of_word:\n                total_count = sum(self.root.children[child].count for child in self.root.children)\n                percentage = node.count / total_count if total_count &gt; 0 else 0\n                weighted_indexes = [(index, weight * percentage) for index in node.indexes]\n                node.class_weights = weighted_indexes  # Corrected this line\n            for child in node.children.values():\n                _apply_weight_to_indexes(child)\n\n        _apply_weight_to_indexes(self.root)\n\n\n\n\nKNN Model\n\n\nCode\nclass KNNModel:\n    def __init__(self, k=5):\n        self.data = None\n        self.labels = []\n        self.k = k\n\n    def fit(self, data, indexes, labels):\n        # print(\"Fitting model with indexes:\", len(indexes), \"labels:\", len(labels))\n        self.data = data[indexes].astype(float)\n        self.labels = np.array(labels).astype(float)\n\n    def predict(self, data):\n        # print(\"Predicting with data:\", data)\n        dist_ind = np.sqrt(np.sum((self.data - data) ** 2, axis=1) ** 2)  # euclidean distance\n        main_arr = np.column_stack((self.labels, dist_ind))  # labels with distance\n        main = main_arr[main_arr[:, 1].argsort()]  # sorting based on distance\n        count = Counter(main[0:self.k, 0])  # counting labels\n        sums = np.array(list(count.values()))  # getting counts\n        return sums / np.sum(sums)  # prediction as probability\n\n\n\n\nTraining and Evaluation\nHere’s how we train and evaluate the TrieKNN model:\n\n\nCode\n# Sample data\nn = 10000\ndata = np.array((np.random.choice(['Anything ', 'By ','Chance '], p=[0.6,0.1,0.3],size=n),\n                 np.random.choice(['can', 'go', 'here','lets', 'see', \"it\"], p=[0.1, 0.1, 0.1, 0.2, 0.4, 0.1], size=n),\n                 np.random.normal(3, 1, size=n),\n                 np.random.normal(5, 2, size=n))).T\ny_label = np.random.choice([0,1], p=[0.7, 0.3], size=n)\n\n# Trie training\ntrie = Trie()\nfor X, y in zip(data, y_label):\n    trie.insert((X[:2], y),None)\n\n# Apply weights to indexes\ntrie.apply_weight_to_indexes(0.5)\n\n# Fit models of leaf nodes\ndef add_model(node, data):\n    node.model = KNNModel()\n    node.model.fit(data, node.indexes, node.labels)\n\ndef traverse_and_add_model(node, data):\n    if node.is_end_of_word:\n        add_model(node, data)  # Add model to leaf node\n    for child in node.children.values():\n        traverse_and_add_model(child, data)\n\ntraverse_and_add_model(trie.root, data[:, 2:])\n\n\n\n\nExplanation\n\nWe create sample data with mixed categorical and numerical features.\nWe insert each data point into the Trie, using the categorical features as the path.\nAfter the Trie is built, we traverse it and fit a KNN model to the data points stored at each leaf node.\nFinally, we can predict the class of new data points by traversing the Trie and using the KNN model at the corresponding leaf node."
  },
  {
    "objectID": "posts/10_treeknn/index.html#results-and-discussion",
    "href": "posts/10_treeknn/index.html#results-and-discussion",
    "title": "TrieKNN: Unleashing KNN’s Power on Mixed Data Types",
    "section": "Results and Discussion",
    "text": "Results and Discussion\nLet us display the trie.\n\n\n\n\nCode\ntrie.display()\n\n\nData: Anything see, Count: 2383, Indexes: 2383 Classes :{1: 698, 0: 1685} weights:2383\nData: Anything go, Count: 618, Indexes: 618 Classes :{0: 427, 1: 191} weights:618\nData: Anything lets, Count: 1259, Indexes: 1259 Classes :{1: 365, 0: 894} weights:1259\nData: Anything it, Count: 574, Indexes: 574 Classes :{1: 182, 0: 392} weights:574\nData: Anything can, Count: 592, Indexes: 592 Classes :{0: 411, 1: 181} weights:592\nData: Anything here, Count: 617, Indexes: 617 Classes :{0: 438, 1: 179} weights:617\nData: Chance see, Count: 1217, Indexes: 1217 Classes :{1: 358, 0: 859} weights:1217\nData: Chance it, Count: 300, Indexes: 300 Classes :{0: 214, 1: 86} weights:300\nData: Chance go, Count: 302, Indexes: 302 Classes :{0: 227, 1: 75} weights:302\nData: Chance lets, Count: 549, Indexes: 549 Classes :{1: 165, 0: 384} weights:549\nData: Chance here, Count: 284, Indexes: 284 Classes :{0: 188, 1: 96} weights:284\nData: Chance can, Count: 280, Indexes: 280 Classes :{0: 185, 1: 95} weights:280\nData: By see, Count: 421, Indexes: 421 Classes :{1: 112, 0: 309} weights:421\nData: By go, Count: 81, Indexes: 81 Classes :{0: 64, 1: 17} weights:81\nData: By can, Count: 107, Indexes: 107 Classes :{0: 80, 1: 27} weights:107\nData: By it, Count: 90, Indexes: 90 Classes :{0: 64, 1: 26} weights:90\nData: By here, Count: 107, Indexes: 107 Classes :{0: 75, 1: 32} weights:107\nData: By lets, Count: 219, Indexes: 219 Classes :{0: 149, 1: 70} weights:219\n\n\nThe model predicted the following values:\n\n\n\n\nCode\n# Prediction example\ndef predict_with_model(node):\n    predictions = node.model.predict(np.array([2,5]))\n    print(\"Predictions:\", predictions)\n\ntrie.apply(predict_with_model)\n\n\nPredictions: [0.8 0.2]\nPredictions: [0.6 0.4]\nPredictions: [0.6 0.4]\nPredictions: [0.6 0.4]\nPredictions: [0.4 0.6]\nPredictions: [0.6 0.4]\nPredictions: [0.8 0.2]\nPredictions: [0.4 0.6]\nPredictions: [1.]\nPredictions: [0.4 0.6]\nPredictions: [0.4 0.6]\nPredictions: [1.]\nPredictions: [0.6 0.4]\nPredictions: [0.6 0.4]\nPredictions: [0.4 0.6]\nPredictions: [0.2 0.8]\nPredictions: [0.4 0.6]\nPredictions: [0.4 0.6]\n\n\nThe predictions will vary on each run. From this we can see that we can use KNN on mixed data types."
  },
  {
    "objectID": "posts/10_treeknn/index.html#conclusion-a-promising-path-forward",
    "href": "posts/10_treeknn/index.html#conclusion-a-promising-path-forward",
    "title": "TrieKNN: Unleashing KNN’s Power on Mixed Data Types",
    "section": "Conclusion: A Promising Path Forward",
    "text": "Conclusion: A Promising Path Forward\nTrieKNN presents a compelling solution for extending the applicability of KNN to mixed data types. By leveraging the Trie data structure, it avoids direct distance calculations on categorical features, enabling the use of localized KNN models for numerical data.\nFurther research could explore:\n\nOptimizing the weighting scheme for combining predictions from different Trie levels.\nComparing TrieKNN’s performance against other mixed-data KNN approaches on benchmark datasets.\nExtending TrieKNN to handle missing data and noisy categorical features.\n\nTrieKNN opens up new possibilities for applying KNN in domains where mixed data types are prevalent, such as healthcare, e-commerce, and social science.\nResources and further reads:\n1. Nomclust R package\n2. An Improved kNN Based on Class Contribution and Feature Weighting\n3. An Improved Weighted KNN Algorithm for Imbalanced Data Classification\n4. A weighting approach for KNN classifier\n5. Unsupervised Outlier Detection for Mixed-Valued Dataset Based on the Adaptive k-Nearest Neighbor Global Network\n6. A hybrid approach based on k-nearest neighbors and decision tree for software fault prediction\n7. Analysis of Decision Tree and K-Nearest Neighbor Algorithm in the Classification of Breast Cancer"
  },
  {
    "objectID": "posts/02_hypothesis_test/index.html",
    "href": "posts/02_hypothesis_test/index.html",
    "title": "A flow to Test Your Hypothesis in Python",
    "section": "",
    "text": "Hypothesis testing Photo by Tara Winstead\n\n\n\nOverview\nAll the practitioners of data science always hit one giant thing to do with data and you know it well its EDA -Exploratory Data Analysis. This word EDA1 was coined by Tukey himself in his seminal book published in 1983. But do you think that before that EDA doesn’t existed ?\n1 Emerson, J. D., & Hoaglin, D. C. (1983). Stem-and-leaf displays. In D. C. Hoaglin, F. Mosteller, & J. W. Tukey (Eds.) Understanding Robust and Exploratory Data Analysis, pp. 7–32. New York: Wiley. Book is here.Well glad you thought. Before that all were doing what is called as Hypothesis Testing. Yes, before this the race was majorly to fit the data and make most unbiased and robust estimate. But remember one thing when you talk about Hypothesis Testing it was always and majorly would be related to RCTs (Randomized Controlled Trials) a.k.a Randomized Clinical Trials and is Gold Standard of data.\n\n\n\n\n\n\nTipMore on RCTs and ODs\n\n\n\n\n\nNow let me now not hijack the discussion to what is RCTs and Observational Data (ODs) as it is more of Philosophical Reasoning rather than other quality of data, but essentially what we are trying to find is that can we by, using stats, identify interesting patterns in data.\nThe only thing happens wit RCT data is that we tend to believe these interesting patterns coincide with some sort of ‘Cause-Effect’ kind of relationship. But essentially due to bia nature of ODs, we certainly cant conclude this. And hence, can only find interesting patterns.\n\n\n\nLets move on. The big question is, for whatever reason you are doing HT , you are doing it for finding something intreating. And that something interesting is usually found by using Post-Hoc Tests. Now there are variety of Post-Hocs available but what is more know and hence easily found to be implemented in Tukey’s HSD.\nSo lets directly jump to how to follow this procedure. We’ll be using bioinfokit for this, as it is much simpler wrapper around whats implemented in statsmodels.\n\n\nWhat are the results\nPheww… Thats too much code right. But that would save a lot of your time in real life. So in real life you would write code as 3 steps below:\n\n\nCode\n# import libraries\nimport pandas as pd\n\n# Getting car data from UCI\ndf = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data',\n                 sep='\\s+',header=None,\n                 names=['mpg','cylinders','displacement','horsepower','weight',\n                 'acceleration','model_year','origin','car_name'])\ndf.head()\n\n# Syntax to do anove with validating the assumption, doing test and a post-hoc\nresults = do_anova_test(df=df, res_var='mpg',xfac_var='cylinders', \n                        anova_model='mpg ~ C(cylinders)+C(origin)+C(cylinders):C(origin)',\n                        ss_typ=3, result_full=True)\n\n\nResults form the do_anova_test\nLevens Test Result:\n                 Parameter    Value\n0      Test statistics (W)  14.5856\n1  Degrees of freedom (Df)   4.0000\n2                  p value   0.0000\n\nBartletts Test Result:\n                 Parameter    Value\n0      Test statistics (T)  61.2143\n1  Degrees of freedom (Df)   4.0000\n2                  p value   0.0000\n\nANOVA\\ANCOVA Test Result:\n                           df     sum_sq    mean_sq         F  PR(&gt;F)      n2\nIntercept                 1.0  6195.1701  6195.1701  296.3452  0.0000  0.2727\nC(cylinders)              4.0  7574.5864  1893.6466   90.5824  0.0000  0.3334\nC(origin)                 2.0   241.0703   120.5351    5.7658  0.0034  0.0106\nC(cylinders):C(origin)    8.0   577.4821    72.1853    3.4530  0.0046  0.0254\nResidual                389.0  8132.1404    20.9052       NaN     NaN     NaN\n\nTukey HSD Result:\n   group1  group2     Diff    Lower    Upper  q-value  p-value\n0       8       4  14.3237  12.8090  15.8383  36.6527   0.0010\n1       8       6   5.0226   3.1804   6.8648  10.5671   0.0010\n2       8       3   5.5869  -0.7990  11.9728   3.3909   0.1183\n3       8       5  12.4036   5.0643  19.7428   6.5503   0.0010\n4       4       6   9.3011   7.6765  10.9256  22.1910   0.0010\n5       4       3   8.7368   2.4102  15.0633   5.3524   0.0017\n6       4       5   1.9201  -5.3676   9.2078   1.0212   0.9000\n7       6       3   0.5643  -5.8486   6.9772   0.3410   0.9000\n8       6       5   7.3810   0.0182  14.7437   3.8854   0.0491\n9       3       5   6.8167  -2.7539  16.3873   2.7606   0.2919\nNice!!!\n\nAnd plotting is even easier\n\n\nCode\n# Numbers are clumsy for most. Making more interpretable plot on above results.\nplot_hsd(results.tukeyhsd.sort_values('Diff'), title=\"Tukey HSD resutls Anova of MPG ~ Cylinder\")\n\n\nResults form the plot_hsd\n\n\n\nTukey’s HSD comparison based on Anova Results\n\n\nPlots look good with ‘p-values’.\n\n\nConclusion\nNow since we applied the above to a Non RCT we cannot conclude that Difference in mpg based on cylinder is huge specially as number of cylinders goes up. But this statement might not be as explicit as might be appearing from plot. Unless you have a strong believe that the data follows with rules and assumptions of RCTs, we should be only seeking interesting as in associated results and not cause-effect results.\n\n\nGive me “The Code”\n\nPerforming AnovaPlotting Results\n\n\n\n\nAnova Test anova_test.py\nfrom bioinfokit import analys\n\nimport numpy as np\nfrom scipy import stats\n\nclass KeyResults:\n    \"\"\"\n    A basic class to hold all the results\n    \"\"\"\n    \n    def __init__(self,result_full):\n        self.keys = []\n        self.result_full = result_full\n    \n    def add_result(self,name,result):\n        if name == 'tukeyhsd':\n            self.keys.append(name)\n            setattr(self, name, result)\n        elif self.result_full:\n            self.keys.append(name)\n            setattr(self, name, result)\n\n\n# Anova test code\ndef do_anova_test(df, res_var, xfac_var, anova_model,ss_typ=3,\n                  effectsize='n2',result_full=False,add_res=False):\n    \"\"\"\n    Do all sequential anova tests\n    \n    Step 1) Leven's/ bartellet test for checking weather variance is homogenous or not\n    Step 2) Main ANOVA/ANCOVA test\n    Step 3) Tukey's HSD for individual combinations\n    \n    :param df: Pandas DataFrame holding all the columns\n    :param res_var: Variable for which we are checking ANOVA\n    :param xfac_var: Grouping Variables for which we want to do the comparisons\n    :param anova_model: SM formula for the model. This is life savour to make all things work\n    :param result_full: To provide all the results of intermediate steps\n    \n    \"\"\"\n\n    results = KeyResults(result_full)\n    \n    # initialize stat method\n    res = analys.stat()\n    \n    # doing levens test\n    res.levene(df=df, res_var=res_var,xfac_var=xfac_var)\n    print('\\nLeven\\'s Test Result:')\n    print(res.levene_summary)\n    results.add_result('levene',res.levene_summary)\n\n    # doing bartlett test\n    res.bartlett(df=df, res_var=res_var,xfac_var=xfac_var)\n    print('\\nBartlett\\'s Test Result:')\n    print(res.bartlett_summary)\n    results.add_result('bartlett',res.bartlett_summary)\n    \n    # doing anova / ancova\n    res.anova_stat(df=df, res_var=res_var, anova_model=anova_model,ss_typ=ss_typ)\n    aov_res = res.anova_summary\n    \n    # Add effect sizes\n    if effectsize == \"n2\":\n        all_effsize = (aov_res['sum_sq'] / aov_res['sum_sq'].sum()).to_numpy()\n        all_effsize[-1] = np.nan\n    else:\n        ss_resid = aov_res['sum_sq'].iloc[-1]\n        all_effsize = aov_res['sum_sq'].apply(lambda x: x / (x + ss_resid)).to_numpy()\n        all_effsize[-1] = np.nan\n    aov_res[effectsize] = all_effsize\n    #aov_res['bw_'] = res.anova_model_out.params.iloc[-1]\n    aov_res = aov_res.round(4)\n    \n    # printing results\n    print('\\nANOVA\\ANCOVA Test Result:')\n    print(aov_res)\n    results.add_result('anova',res.anova_summary.round(4))\n    results.add_result('anova_model',res.anova_model_out)\n    \n    # doing tukey's hsd top compare the groups\n    res.tukey_hsd(df=df, res_var=res_var,xfac_var=xfac_var, anova_model=anova_model,ss_typ=ss_typ)\n    print('\\nTukey HSD Result:')\n    print(res.tukey_summary.round(4))\n    results.add_result('tukeyhsd',res.tukey_summary.round(4))\n    \n    # add all result componets again if needed \n    if add_res:\n        results.add_result('allresult',res)\n    \n    return results\n\n\n\n\n\n\nPlotting results plot_hsd.py\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.style.use('seaborn-bright')\n\ndef plot_hsd(hsdres,p_cutoff=0.05,title=None,ax=None,figsize=(10,7)):\n     \"\"\"\n     Do plotting of tukeyhsd results\n    \n  \n    :param hsdres: 'tukeyhsd' result form the do_anova_test function\n    :param p_cutoff: Cutoff at which we get say a combination is significant\n    :param title: Title of the plot\n    :param ax: Define or get the matplotlib axes\n    :param figsize: Mention Figure size to draw\n    \n    \"\"\"\n\n    if ax is None:\n        fig,axp = plt.subplots(figsize=figsize)\n    else:\n        axp = ax\n    \n    # helper func\n    p_ind = lambda x : '' if x &gt; 0.1 else ('+' if x &gt; 0.05 else ('*' if x &gt; 0.01 else ('**' if x &gt;0.001 else '***')))\n    label_gen  = lambda x: f\"${x[0]} - {x[1]}\\ |\\ p:{x[2]:0.2f}{p_ind(x[2]):5s}$\"\n    \n    #setting values\n    mask = hsdres['p-value'] &lt;= p_cutoff\n    yticklabs = hsdres[['group1','group2','p-value']].apply(label_gen,axis=1).values\n    ys = np.arange(len(hsdres))\n    \n    # adding plot to axes\n    axp.errorbar(hsdres[~mask]['Diff'],ys[~mask],xerr=np.abs(hsdres[~mask][['Lower',\"Upper\"]]).values.T,\n                fmt='o', color='black', ecolor='lightgray', elinewidth=2, capsize=0)\n    axp.errorbar(hsdres[mask]['Diff'],ys[mask],xerr=np.abs(hsdres[mask][['Lower',\"Upper\"]]).values.T,\n                fmt='o', color='red', ecolor='pink', elinewidth=2, capsize=5)\n    axp.axvline(x=0,linestyle='--',c='skyblue')\n    axp.set_yticks([])\n    (l,u) = axp.get_xlim()\n    axp.set_xlim(l+1.5*l,u)\n    (l,u) = axp.get_xlim()\n    for idx,labs in enumerate(yticklabs):\n        axp.text(l-0.1*l,ys[idx],labs)\n    axp.set_yticklabels([])\n    \n    # finally doing what is needed\n    if ax is None:\n        plt.title('' if title is None else title,fontsize=14)\n        plt.show()\n    else:\n        return axp\n\n\n\n\n\nHope this give you kickstart to find you intresting patterns. Happy Learning!\n\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "posts/04_crosstab_sparsity_classification/index.html",
    "href": "posts/04_crosstab_sparsity_classification/index.html",
    "title": "CrossTab Sparsity for Classification",
    "section": "",
    "text": "Cross Roads where everyone meets!\n\n\n\nIntroduction: A Journey into Data\nPicture this: you’re standing on the icy shores of Antarctica, the wind whipping around you as you watch a colony of Palmer Penguins waddling about, oblivious to the data detective work you’re about to embark on. As a data science architect, you’re not just an observer; you’re a sleuth armed with algorithms and insights, ready to unravel the mysteries hidden within data. Today, we’ll transform raw numbers into powerful narratives using CrossTab Sparsity as our guiding compass. This blog post will demonstrate how this metric can revolutionize classification tasks, shedding light on many fascinating datasets—the charming Palmer Penguins and the serious Obesity, Credit cards data and many more.\n\n\nThe Power of CrossTab Sparsity\n\nWhat is CrossTab Sparsity?\nCrossTab Sparsity isn’t just a fancy term that sounds good at dinner parties; it’s a statistical measure that helps us peer into the intricate relationships between categorical variables. Imagine it as a magnifying glass that reveals how different categories interact within a contingency table. Understanding these interactions is crucial in classification tasks, where the right features can make or break your model (and your day).\nWhy Does It Matter?\nIn the world of data science, especially in classification, selecting relevant features is like picking the right ingredients for a gourmet meal—get it wrong, and you might end up with something unpalatable. CrossTab Sparsity helps us achieve this by:\n\nHighlighting Relationships: It’s like having a friend who always points out when two people are meant to be together—understanding how features interact with the target variable.\nStreamlining Models: Reducing complexity by focusing on significant features means less time spent untangling spaghetti code.\nEnhancing Interpretability: Making models easier to understand and explain to stakeholders is like translating tech jargon into plain English—everyone appreciates that!\n\n\n\n\nData Overview: Our Data People at work here\n\nThe Datasets\nData 1: Estimation of Obesity Levels Based On Eating Habits and Physical Condition\nLittle bit about the data: This dataset, shared on 8/26/2019, looks at obesity levels in people from Mexico, Peru, and Colombia based on their eating habits and physical health. It includes 2,111 records with 16 features, and classifies individuals into different obesity levels, from insufficient weight to obesity type III. Most of the data (77%) was created using a tool, while the rest (23%) was collected directly from users online.\nData 2: Predict Students’ Dropout and Academic Success\nLittle bit about the data: This dataset, shared on 12/12/2021, looks at factors like students’ backgrounds, academic path, and socio-economic status to predict whether they’ll drop out or succeed in their studies. With 4,424 records across 36 features, it covers students from different undergrad programs. The goal is to use machine learning to spot at-risk students early, so schools can offer support. The data has been cleaned and doesn’t have any missing values. It’s a classification task with three outcomes: dropout, still enrolled, or graduated\nKey Features:\n\nMulticlass: Both data set cater a multi class problems with NObeyesdad and Target columns\nMixed Data Type: A good mix of categorical and continuous variables are available for usage.\nSizeable: More than 2 K rows are available for testing.\n\n\n\n\nExploratory Data Analysis (EDA): Setting the Stage\nBefore we dive into model creation, let’s explore our dataset through some quick EDA. Think of this as getting to know your non-obese friends before inviting them to a party.\n\nEDA for Obesity Data\nHere’s a brief code snippet to perform essential EDA on the Obesity dataset:\n\n\nLoading data and generating basic descriptive\n# Load the Obesity data\nraw_df = pd.read_csv('ObesityDataSet_raw_and_data_sinthetic.csv')\ntarget = 'NObeyesdad'\n\n# Load Students data\n\n# Load Credit data\n# raw_data = sm.datasets.get_rdataset(\"credit_data\",'modeldata')\n# raw_df = raw_data.data\n# target = 'Status'\n\n# # Load Palmer penguins data\n# raw_data = sm.datasets.get_rdataset(\"penguins\",'palmerpenguins')\n# raw_df = raw_data.data\n# target = 'species'\n\n\n# # Load Credit data\n# raw_data = sm.datasets.get_rdataset(\"CreditCard\",'AER')\n# raw_df = raw_data.data\n# target = 'card'\n\n\n# setting things up for aal the next steps\nraw_df[target] = raw_df[target].astype('category') \nprint('No of data points available to work:',raw_df.shape)\ndisplay(raw_df.head())\n\n\n# Summary statistics\ndisplay(raw_df.describe())\n\n\nNo of data points available to work: (2111, 17)\n\n\n\n\n\n\n\n\n\nGender\nAge\nHeight\nWeight\nFamil_Hist_Owt\nFAVC\nFCVC\nNCP\nCAEC\nSMOKE\nCH2O\nSCC\nFAF\nTUE\nCALC\nMTRANS\nNObeyesdad\n\n\n\n\n0\nFemale\n21.0\n1.62\n64.0\nyes\nno\n2.0\n3.0\nSometimes\nno\n2.0\nno\n0.0\n1.0\nno\nPublic_Transportation\nNormal_Weight\n\n\n1\nFemale\n21.0\n1.52\n56.0\nyes\nno\n3.0\n3.0\nSometimes\nyes\n3.0\nyes\n3.0\n0.0\nSometimes\nPublic_Transportation\nNormal_Weight\n\n\n2\nMale\n23.0\n1.80\n77.0\nyes\nno\n2.0\n3.0\nSometimes\nno\n2.0\nno\n2.0\n1.0\nFrequently\nPublic_Transportation\nNormal_Weight\n\n\n3\nMale\n27.0\n1.80\n87.0\nno\nno\n3.0\n3.0\nSometimes\nno\n2.0\nno\n2.0\n0.0\nFrequently\nWalking\nOverweight_Level_I\n\n\n4\nMale\n22.0\n1.78\n89.8\nno\nno\n2.0\n1.0\nSometimes\nno\n2.0\nno\n0.0\n0.0\nSometimes\nPublic_Transportation\nOverweight_Level_II\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAge\nHeight\nWeight\nFCVC\nNCP\nCH2O\nFAF\nTUE\n\n\n\n\ncount\n2111.000000\n2111.000000\n2111.000000\n2111.000000\n2111.000000\n2111.000000\n2111.000000\n2111.000000\n\n\nmean\n24.312600\n1.701677\n86.586058\n2.419043\n2.685628\n2.008011\n1.010298\n0.657866\n\n\nstd\n6.345968\n0.093305\n26.191172\n0.533927\n0.778039\n0.612953\n0.850592\n0.608927\n\n\nmin\n14.000000\n1.450000\n39.000000\n1.000000\n1.000000\n1.000000\n0.000000\n0.000000\n\n\n25%\n19.947192\n1.630000\n65.473343\n2.000000\n2.658738\n1.584812\n0.124505\n0.000000\n\n\n50%\n22.777890\n1.700499\n83.000000\n2.385502\n3.000000\n2.000000\n1.000000\n0.625350\n\n\n75%\n26.000000\n1.768464\n107.430682\n3.000000\n3.000000\n2.477420\n1.666678\n1.000000\n\n\nmax\n61.000000\n1.980000\n173.000000\n3.000000\n4.000000\n3.000000\n3.000000\n2.000000\n\n\n\n\n\n\n\n\n\nTarget distribution\n\n\nTarget and Correlation\n# Visualize target data distribution\nplt.figure(figsize=(4, 3))\nsns.countplot(data=raw_df, x=target, hue=target, palette='Set2',)\nplt.title(f'Distribution of {target} levels')\nplt.xticks(rotation=45)\nplt.show()\n\n# Heatmap to check for correlations between numeric variables\ncorr = raw_df.corr('kendall',numeric_only=True)\nsns.heatmap(corr, annot=True, cmap='coolwarm')\nplt.title('Kendall Correlation Heatmap')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoneSome Mode EDA for the data\n\n\n\n\n\n\n\nEDA code\n# Visualize the distribution of numerical variables\nsns.pairplot(raw_df, hue=target, corner=True)\nplt.show()\n\n\n\n\n# Gettign Categorical data\ncategorical_columns = raw_df.select_dtypes(include='object').columns\n\n# Plot categorical variables with respect to the target variable\nfor col in categorical_columns:\n    plt.figure(figsize=(12, 5))\n    sns.countplot(data=raw_df,x=col, hue=target)\n    plt.title(f\"Countplot of {col} with respect to {target}\")\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Creation: Establishing a Baseline\nWith our exploratory analysis complete, we’re ready to create our baseline model using logistic regression with Statsmodels. This initial model will serve as our reference point—like setting up a benchmark for your favorite video game.\n\n\nSplitting data and training a default Multinomila Logit model on our data\ndata_df = raw_df.dropna().reset_index(drop=True)\ndata_df[target] = data_df[target].cat.codes\n# X = data_df[['bill_length_mm','bill_depth_mm','flipper_length_mm','body_mass_g']] \n\ndata_df_test = data_df.sample(frac=0.1,random_state=3)\ndata_df_train = data_df.drop(data_df_test.index)\n\n# Using MN logistic regression model using formula API\n# This would essentially bold down to pair wise logsitic regression\nlogit_model = sm.MNLogit.from_formula(\n    f\"{target} ~ {' + '.join([col for col in data_df_train.columns if col != target])}\", \n    data=data_df_train\n).fit_regularized()\n\n\nOptimization terminated successfully    (Exit mode 0)\n            Current function value: 0.17113347578942742\n            Iterations: 489\n            Function evaluations: 670\n            Gradient evaluations: 489\n\n\n\n\n\n\n\n\n\n\nNoneBase model summary for geeks\n\n\n\n\n\n\n\nDisplay summary\ndisplay(logit_model.summary())\n\n\n\nMNLogit Regression Results\n\n\nDep. Variable:\nNObeyesdad\nNo. Observations:\n1900\n\n\nModel:\nMNLogit\nDf Residuals:\n1756\n\n\nMethod:\nMLE\nDf Model:\n138\n\n\nDate:\nThu, 27 Feb 2025\nPseudo R-squ.:\n0.9119\n\n\nTime:\n03:18:53\nLog-Likelihood:\n-325.15\n\n\nconverged:\nTrue\nLL-Null:\n-3691.8\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n0.000\n\n\n\n\n\n\n\n\nNObeyesdad=1\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-11.3439\n3.45e+05\n-3.29e-05\n1.000\n-6.76e+05\n6.76e+05\n\n\nGender[T.Male]\n-3.4606\n0.819\n-4.224\n0.000\n-5.066\n-1.855\n\n\nFamil_Hist_Owt[T.yes]\n-0.8874\n0.658\n-1.349\n0.177\n-2.177\n0.402\n\n\nFAVC[T.yes]\n0.2631\n0.782\n0.337\n0.736\n-1.269\n1.795\n\n\nCAEC[T.Frequently]\n-8.2219\n2.342\n-3.511\n0.000\n-12.811\n-3.632\n\n\nCAEC[T.Sometimes]\n-6.2475\n2.265\n-2.758\n0.006\n-10.687\n-1.808\n\n\nCAEC[T.no]\n-8.6341\n2.916\n-2.961\n0.003\n-14.349\n-2.919\n\n\nSMOKE[T.yes]\n4.5048\n3.105\n1.451\n0.147\n-1.582\n10.591\n\n\nSCC[T.yes]\n-0.7063\n1.458\n-0.484\n0.628\n-3.565\n2.152\n\n\nCALC[T.Frequently]\n-12.6173\n3.45e+05\n-3.66e-05\n1.000\n-6.76e+05\n6.76e+05\n\n\nCALC[T.Sometimes]\n-13.3244\n3.45e+05\n-3.86e-05\n1.000\n-6.76e+05\n6.76e+05\n\n\nCALC[T.no]\n-14.1980\n3.45e+05\n-4.12e-05\n1.000\n-6.76e+05\n6.76e+05\n\n\nMTRANS[T.Bike]\n15.8821\n2529.381\n0.006\n0.995\n-4941.614\n4973.378\n\n\nMTRANS[T.Motorbike]\n4.0050\n47.345\n0.085\n0.933\n-88.790\n96.800\n\n\nMTRANS[T.Public_Transportation]\n4.5116\n1.001\n4.505\n0.000\n2.549\n6.474\n\n\nMTRANS[T.Walking]\n4.3989\n1.507\n2.918\n0.004\n1.445\n7.353\n\n\nAge\n0.3779\n0.098\n3.858\n0.000\n0.186\n0.570\n\n\nHeight\n-14.4182\n4.123\n-3.497\n0.000\n-22.499\n-6.338\n\n\nWeight\n1.0784\n0.146\n7.384\n0.000\n0.792\n1.365\n\n\nFCVC\n-0.7676\n0.428\n-1.793\n0.073\n-1.607\n0.072\n\n\nNCP\n-1.7199\n0.489\n-3.516\n0.000\n-2.679\n-0.761\n\n\nCH2O\n-1.7255\n0.578\n-2.985\n0.003\n-2.859\n-0.592\n\n\nFAF\n-0.1753\n0.281\n-0.624\n0.533\n-0.726\n0.375\n\n\nTUE\n-0.9735\n0.458\n-2.124\n0.034\n-1.872\n-0.075\n\n\nNObeyesdad=2\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n17.3832\nnan\nnan\nnan\nnan\nnan\n\n\nGender[T.Male]\n-13.9964\n1.976\n-7.083\n0.000\n-17.869\n-10.123\n\n\nFamil_Hist_Owt[T.yes]\n2.0850\n1.721\n1.212\n0.226\n-1.288\n5.458\n\n\nFAVC[T.yes]\n1.0223\n1.765\n0.579\n0.562\n-2.437\n4.482\n\n\nCAEC[T.Frequently]\n-10.0658\n4.392\n-2.292\n0.022\n-18.674\n-1.458\n\n\nCAEC[T.Sometimes]\n-1.0233\n3.443\n-0.297\n0.766\n-7.771\n5.724\n\n\nCAEC[T.no]\n-0.4821\n977.119\n-0.000\n1.000\n-1915.601\n1914.637\n\n\nSMOKE[T.yes]\n8.1449\n4.011\n2.030\n0.042\n0.283\n16.007\n\n\nSCC[T.yes]\n-7.6939\n155.443\n-0.049\n0.961\n-312.356\n296.968\n\n\nCALC[T.Frequently]\n-2.4712\nnan\nnan\nnan\nnan\nnan\n\n\nCALC[T.Sometimes]\n-7.5357\nnan\nnan\nnan\nnan\nnan\n\n\nCALC[T.no]\n-7.2634\nnan\nnan\nnan\nnan\nnan\n\n\nMTRANS[T.Bike]\n-11.9360\n1.16e+08\n-1.03e-07\n1.000\n-2.27e+08\n2.27e+08\n\n\nMTRANS[T.Motorbike]\n10.9302\n48.258\n0.226\n0.821\n-83.653\n105.513\n\n\nMTRANS[T.Public_Transportation]\n11.2094\n1.756\n6.383\n0.000\n7.767\n14.651\n\n\nMTRANS[T.Walking]\n1.7141\n2.758\n0.622\n0.534\n-3.691\n7.119\n\n\nAge\n0.8105\n0.133\n6.108\n0.000\n0.550\n1.071\n\n\nHeight\n-184.0655\n14.785\n-12.450\n0.000\n-213.043\n-155.088\n\n\nWeight\n3.9430\n0.288\n13.681\n0.000\n3.378\n4.508\n\n\nFCVC\n0.8899\n1.009\n0.882\n0.378\n-1.088\n2.867\n\n\nNCP\n-1.1103\n0.710\n-1.564\n0.118\n-2.502\n0.281\n\n\nCH2O\n-1.5409\n0.877\n-1.757\n0.079\n-3.259\n0.178\n\n\nFAF\n-1.4599\n0.593\n-2.461\n0.014\n-2.622\n-0.297\n\n\nTUE\n-0.5909\n0.840\n-0.704\n0.482\n-2.237\n1.055\n\n\nNObeyesdad=3\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-138.5283\nnan\nnan\nnan\nnan\nnan\n\n\nGender[T.Male]\n-16.6646\n8.382\n-1.988\n0.047\n-33.094\n-0.235\n\n\nFamil_Hist_Owt[T.yes]\n2.3697\n11.592\n0.204\n0.838\n-20.350\n25.090\n\n\nFAVC[T.yes]\n-8.7847\n5.440\n-1.615\n0.106\n-19.447\n1.878\n\n\nCAEC[T.Frequently]\n-71.7139\n2.13e+08\n-3.37e-07\n1.000\n-4.17e+08\n4.17e+08\n\n\nCAEC[T.Sometimes]\n-3.9355\n4.749\n-0.829\n0.407\n-13.244\n5.373\n\n\nCAEC[T.no]\n7.7274\n977.625\n0.008\n0.994\n-1908.382\n1923.836\n\n\nSMOKE[T.yes]\n3.5336\n19.117\n0.185\n0.853\n-33.935\n41.002\n\n\nSCC[T.yes]\n-19.4881\n156.920\n-0.124\n0.901\n-327.046\n288.070\n\n\nCALC[T.Frequently]\n-43.6047\nnan\nnan\nnan\nnan\nnan\n\n\nCALC[T.Sometimes]\n-45.7392\nnan\nnan\nnan\nnan\nnan\n\n\nCALC[T.no]\n-28.2608\nnan\nnan\nnan\nnan\nnan\n\n\nMTRANS[T.Bike]\n0.0374\nnan\nnan\nnan\nnan\nnan\n\n\nMTRANS[T.Motorbike]\n-2.3922\n1.05e+11\n-2.28e-11\n1.000\n-2.05e+11\n2.05e+11\n\n\nMTRANS[T.Public_Transportation]\n22.6192\n6.634\n3.410\n0.001\n9.618\n35.621\n\n\nMTRANS[T.Walking]\n-5.3362\n34.114\n-0.156\n0.876\n-72.198\n61.526\n\n\nAge\n2.5098\n0.960\n2.615\n0.009\n0.629\n4.391\n\n\nHeight\n-278.8861\n44.172\n-6.314\n0.000\n-365.461\n-192.311\n\n\nWeight\n7.1526\n1.391\n5.141\n0.000\n4.426\n9.879\n\n\nFCVC\n4.1479\n3.269\n1.269\n0.204\n-2.258\n10.554\n\n\nNCP\n-1.5833\n2.388\n-0.663\n0.507\n-6.264\n3.098\n\n\nCH2O\n-13.3811\n5.527\n-2.421\n0.015\n-24.213\n-2.549\n\n\nFAF\n-9.8066\n4.355\n-2.252\n0.024\n-18.342\n-1.271\n\n\nTUE\n-5.7061\n3.289\n-1.735\n0.083\n-12.152\n0.739\n\n\nNObeyesdad=4\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-87.3253\n6.43e+07\n-1.36e-06\n1.000\n-1.26e+08\n1.26e+08\n\n\nGender[T.Male]\n-200.2957\n4.25e+07\n-4.71e-06\n1.000\n-8.33e+07\n8.33e+07\n\n\nFamil_Hist_Owt[T.yes]\n-30.9113\nnan\nnan\nnan\nnan\nnan\n\n\nFAVC[T.yes]\n-53.1787\nnan\nnan\nnan\nnan\nnan\n\n\nCAEC[T.Frequently]\n-28.5507\n2.16e+08\n-1.32e-07\n1.000\n-4.23e+08\n4.23e+08\n\n\nCAEC[T.Sometimes]\n-21.5727\n4.19e+07\n-5.15e-07\n1.000\n-8.21e+07\n8.21e+07\n\n\nCAEC[T.no]\n-2.1999\n1.31e+29\n-1.69e-29\n1.000\n-2.56e+29\n2.56e+29\n\n\nSMOKE[T.yes]\n-6.0935\n9.24e+08\n-6.59e-09\n1.000\n-1.81e+09\n1.81e+09\n\n\nSCC[T.yes]\n-12.3062\nnan\nnan\nnan\nnan\nnan\n\n\nCALC[T.Frequently]\n-6.2458\n1.59e+10\n-3.93e-10\n1.000\n-3.12e+10\n3.12e+10\n\n\nCALC[T.Sometimes]\n-37.1969\nnan\nnan\nnan\nnan\nnan\n\n\nCALC[T.no]\n-64.5072\nnan\nnan\nnan\nnan\nnan\n\n\nMTRANS[T.Bike]\n-0.2989\n1.92e+53\n-1.56e-54\n1.000\n-3.76e+53\n3.76e+53\n\n\nMTRANS[T.Motorbike]\n-0.2031\n3.86e+35\n-5.26e-37\n1.000\n-7.57e+35\n7.57e+35\n\n\nMTRANS[T.Public_Transportation]\n-57.6929\n5.78e+07\n-9.98e-07\n1.000\n-1.13e+08\n1.13e+08\n\n\nMTRANS[T.Walking]\n-7.4454\n2.11e+15\n-3.52e-15\n1.000\n-4.14e+15\n4.14e+15\n\n\nAge\n-9.3711\n100.732\n-0.093\n0.926\n-206.803\n188.061\n\n\nHeight\n-174.4791\n585.777\n-0.298\n0.766\n-1322.581\n973.623\n\n\nWeight\n8.7401\n34.352\n0.254\n0.799\n-58.588\n76.068\n\n\nFCVC\n49.0843\n3.05e+04\n0.002\n0.999\n-5.98e+04\n5.99e+04\n\n\nNCP\n2.3456\n4587.346\n0.001\n1.000\n-8988.688\n8993.379\n\n\nCH2O\n-18.5876\n33.678\n-0.552\n0.581\n-84.595\n47.420\n\n\nFAF\n-65.1863\n257.967\n-0.253\n0.801\n-570.792\n440.420\n\n\nTUE\n-44.3687\n279.949\n-0.158\n0.874\n-593.058\n504.321\n\n\nNObeyesdad=5\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-12.5582\n3.45e+05\n-3.64e-05\n1.000\n-6.76e+05\n6.76e+05\n\n\nGender[T.Male]\n-6.8927\n1.091\n-6.319\n0.000\n-9.031\n-4.755\n\n\nFamil_Hist_Owt[T.yes]\n-0.5826\n0.791\n-0.736\n0.462\n-2.134\n0.969\n\n\nFAVC[T.yes]\n2.6029\n0.975\n2.670\n0.008\n0.692\n4.514\n\n\nCAEC[T.Frequently]\n-7.2782\n2.533\n-2.873\n0.004\n-12.243\n-2.314\n\n\nCAEC[T.Sometimes]\n-2.8841\n2.442\n-1.181\n0.238\n-7.671\n1.903\n\n\nCAEC[T.no]\n-3.8084\n3.166\n-1.203\n0.229\n-10.013\n2.397\n\n\nSMOKE[T.yes]\n3.1147\n3.291\n0.947\n0.344\n-3.335\n9.565\n\n\nSCC[T.yes]\n2.1332\n1.626\n1.312\n0.190\n-1.054\n5.320\n\n\nCALC[T.Frequently]\n-9.0218\n3.45e+05\n-2.61e-05\n1.000\n-6.76e+05\n6.76e+05\n\n\nCALC[T.Sometimes]\n-9.1622\n3.45e+05\n-2.66e-05\n1.000\n-6.76e+05\n6.76e+05\n\n\nCALC[T.no]\n-10.7609\n3.45e+05\n-3.12e-05\n1.000\n-6.76e+05\n6.76e+05\n\n\nMTRANS[T.Bike]\n19.0539\n2529.381\n0.008\n0.994\n-4938.442\n4976.550\n\n\nMTRANS[T.Motorbike]\n1.6649\n47.401\n0.035\n0.972\n-91.240\n94.570\n\n\nMTRANS[T.Public_Transportation]\n6.0083\n1.212\n4.956\n0.000\n3.632\n8.385\n\n\nMTRANS[T.Walking]\n4.3751\n1.779\n2.460\n0.014\n0.889\n7.861\n\n\nAge\n0.4896\n0.107\n4.589\n0.000\n0.281\n0.699\n\n\nHeight\n-49.9784\n6.729\n-7.427\n0.000\n-63.167\n-36.790\n\n\nWeight\n1.7920\n0.168\n10.650\n0.000\n1.462\n2.122\n\n\nFCVC\n-0.8144\n0.599\n-1.359\n0.174\n-1.989\n0.360\n\n\nNCP\n-1.4253\n0.552\n-2.580\n0.010\n-2.508\n-0.343\n\n\nCH2O\n-1.8250\n0.678\n-2.690\n0.007\n-3.155\n-0.495\n\n\nFAF\n-0.5296\n0.375\n-1.412\n0.158\n-1.265\n0.206\n\n\nTUE\n-0.8409\n0.557\n-1.510\n0.131\n-1.932\n0.250\n\n\nNObeyesdad=6\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-2.1495\n1.51e+06\n-1.42e-06\n1.000\n-2.96e+06\n2.96e+06\n\n\nGender[T.Male]\n-6.7717\n1.213\n-5.583\n0.000\n-9.149\n-4.395\n\n\nFamil_Hist_Owt[T.yes]\n1.9277\n1.078\n1.789\n0.074\n-0.185\n4.040\n\n\nFAVC[T.yes]\n-0.4390\n1.141\n-0.385\n0.700\n-2.676\n1.798\n\n\nCAEC[T.Frequently]\n-5.4475\n3.295\n-1.653\n0.098\n-11.906\n1.011\n\n\nCAEC[T.Sometimes]\n0.8345\n3.075\n0.271\n0.786\n-5.192\n6.861\n\n\nCAEC[T.no]\n1.6818\n3.972\n0.423\n0.672\n-6.103\n9.466\n\n\nSMOKE[T.yes]\n7.0586\n3.567\n1.979\n0.048\n0.068\n14.049\n\n\nSCC[T.yes]\n1.3350\n2.021\n0.661\n0.509\n-2.625\n5.295\n\n\nCALC[T.Frequently]\n-2.1230\n1.51e+06\n-1.41e-06\n1.000\n-2.96e+06\n2.96e+06\n\n\nCALC[T.Sometimes]\n-4.6506\n1.51e+06\n-3.08e-06\n1.000\n-2.96e+06\n2.96e+06\n\n\nCALC[T.no]\n-4.1703\n1.51e+06\n-2.76e-06\n1.000\n-2.96e+06\n2.96e+06\n\n\nMTRANS[T.Bike]\n-21.8443\n6.4e+09\n-3.41e-09\n1.000\n-1.26e+10\n1.26e+10\n\n\nMTRANS[T.Motorbike]\n3.1683\n47.467\n0.067\n0.947\n-89.865\n96.202\n\n\nMTRANS[T.Public_Transportation]\n8.7749\n1.423\n6.165\n0.000\n5.985\n11.564\n\n\nMTRANS[T.Walking]\n1.2621\n2.258\n0.559\n0.576\n-3.163\n5.687\n\n\nAge\n0.6974\n0.116\n6.002\n0.000\n0.470\n0.925\n\n\nHeight\n-104.7093\n9.038\n-11.585\n0.000\n-122.424\n-86.995\n\n\nWeight\n2.6268\n0.190\n13.821\n0.000\n2.254\n2.999\n\n\nFCVC\n0.2192\n0.764\n0.287\n0.774\n-1.278\n1.716\n\n\nNCP\n-1.8144\n0.606\n-2.992\n0.003\n-3.003\n-0.626\n\n\nCH2O\n-1.9110\n0.757\n-2.525\n0.012\n-3.394\n-0.428\n\n\nFAF\n-0.9928\n0.439\n-2.264\n0.024\n-1.852\n-0.133\n\n\nTUE\n0.0701\n0.671\n0.104\n0.917\n-1.246\n1.386\n\n\n\n\n\n\n\n\n\nEvaluating Model Performance\nTo gauge our models’ effectiveness, we’ll employ various metrics such as accuracy, precision, recall, and F1-score. A confusion matrix will help visualize how well our models perform in classifying outcomes—think of it as a report card for your model!\n\n\nEvaluating the Logit model\n# Predict on test data\nbase_preds = logit_model.predict(data_df_test).idxmax(axis=1)\ny_test = data_df_test[target]\n\n# Evaluate the model\naccuracy_orig = accuracy_score(y_test, base_preds)\nreport_orig = classification_report(y_test, base_preds)\n\nprint(\"Accuracy:\", accuracy_orig)\nprint(\"Classification Report:\")\nprint(report_orig)\n\n\nAccuracy: 0.909952606635071\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.93      0.86      0.89        29\n           1       0.86      0.83      0.84        29\n           2       0.95      0.91      0.93        45\n           3       0.94      0.97      0.95        31\n           4       1.00      0.96      0.98        27\n           5       0.83      0.90      0.86        21\n           6       0.84      0.93      0.89        29\n\n    accuracy                           0.91       211\n   macro avg       0.91      0.91      0.91       211\nweighted avg       0.91      0.91      0.91       211\n\n\n\n\n\n\nLooking for some Improvments!\n\nFeature Selection Using CrossTab Sparsity\nNow comes the exciting part—using CrossTab Sparsity to refine our feature selection process! It’s like cleaning up your closet and only keeping the clothes that spark joy (thank you, Marie Kondo). 1\n1 This is based on work in Unique Metric for Health Analysis with Optimization of Clustering Activity and Cross Comparison of Results from Different Approach. Paper LinkCode is here!\n\n\nStandared Steps for Feature Selection\n\nCalculate CrossTab Sparsity: For each feature against the target variable.\nSelect Features: Based on sparsity scores that indicate significant interactions with the target variable.\nRecreate Models: Train new models using only the selected features—less is often more!\n\nHere we go!!!\n\n\n\nDoing what needs to Done Code ;)\nsns.set_style(\"white\")\nsns.set_context(\"paper\")\n# Calculating Crostab sparsity for each Column\nresults = crosstab_sparsity(data_df_train.iloc[:,:-1],data_df_train[target],numeric_bin='decile')\n\n# presenting results for consumption\ndf_long = pd.melt(results['scores'], id_vars=['Columns'], value_vars=['seggregation', 'explaination', 'metric'],\n                  var_name='Metric', value_name='values')\n\n# Adding jitter: small random noise to 'Columns' (x-axis)\n# df_long['values_jittered'] = df_long['Value'] + np.random.uniform(-0.1, 0.1, size=len(df_long))\n\n# Create a seaborn scatter plot with jitter, more professional color palette, and transparency\nplt.figure(figsize=(12, 5))\nsns.scatterplot(x='Columns', y='values', hue='Metric', style='Metric',\n        data=df_long, s=100, alpha=0.7, palette='deep')\n\n# Title and labels\nplt.title('Metrics by Columns', fontsize=16)\nplt.xticks(rotation=45) \nplt.xlabel('Columns', fontsize=10)\nplt.ylabel('Value', fontsize=10)\n\n# Display legend outside the plot for better readability\nplt.legend(title='Metric', loc='upper right', fancybox=True, framealpha=0.5)\n\n# Show the plot\nplt.tight_layout()\nplt.show()\n\n\nCSP calculated with decile for breaks!\n\nScores for 7 groups(s) is : 140.96057955229762\n\n\n\n\n\n\n\n\n\n\n\n\nAnd Drum Rolls pelase!!!\nUsing just top 5 varaibles we are getting almost similar or better overall accuracy. This amounts to greatly simplifing the models and clearly explain why some variable are not useful for modeling.\n\n\nAnd finally training and evaluating with drum rolls\nlogit_model_rev = sm.MNLogit.from_formula(f\"{target} ~ {' + '.join(results['scores'].loc[:5,'Columns'].values)}\", \n    data=data_df_train\n).fit_regularized()\n\n# Predict on test data\nchallenger_preds = logit_model_rev.predict(data_df_test).idxmax(axis=1)\ny_test = data_df_test[target]\n\n# Evaluate the model\naccuracy_new = accuracy_score(y_test, challenger_preds)\nreport_new = classification_report(y_test, challenger_preds)\n\nprint(\"Accuracy:\", accuracy_new)\nprint(\"Classification Report:\")\nprint(report_new)\n\n\nOptimization terminated successfully    (Exit mode 0)\n            Current function value: 0.174380345428068\n            Iterations: 417\n            Function evaluations: 662\n            Gradient evaluations: 417\nAccuracy: 0.9383886255924171\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.93      0.97      0.95        29\n           1       0.93      0.93      0.93        29\n           2       0.96      1.00      0.98        45\n           3       0.93      0.90      0.92        31\n           4       0.93      0.93      0.93        27\n           5       0.90      0.90      0.90        21\n           6       0.96      0.90      0.93        29\n\n    accuracy                           0.94       211\n   macro avg       0.94      0.93      0.93       211\nweighted avg       0.94      0.94      0.94       211\n\n\n\n\n\n\n\n\n\n\n\nNoneSummary of retrained model\n\n\n\n\n\n\n\nCode\ndisplay(logit_model_rev.summary())\n\n\n\nMNLogit Regression Results\n\n\nDep. Variable:\nNObeyesdad\nNo. Observations:\n1900\n\n\nModel:\nMNLogit\nDf Residuals:\n1858\n\n\nMethod:\nMLE\nDf Model:\n36\n\n\nDate:\nThu, 27 Feb 2025\nPseudo R-squ.:\n0.9103\n\n\nTime:\n03:18:54\nLog-Likelihood:\n-331.32\n\n\nconverged:\nTrue\nLL-Null:\n-3691.8\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n0.000\n\n\n\n\n\n\n\n\nNObeyesdad=1\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n58.1248\n9.361\n6.209\n0.000\n39.778\n76.472\n\n\nTUE\n0.1130\n0.445\n0.254\n0.799\n-0.759\n0.985\n\n\nCH2O\n-0.8634\n0.609\n-1.419\n0.156\n-2.056\n0.329\n\n\nFAF\n0.1425\n0.334\n0.426\n0.670\n-0.513\n0.798\n\n\nAge\n0.0579\n0.077\n0.754\n0.451\n-0.093\n0.208\n\n\nHeight\n-76.5735\n10.536\n-7.268\n0.000\n-97.224\n-55.923\n\n\nWeight\n1.3337\n0.176\n7.566\n0.000\n0.988\n1.679\n\n\nNObeyesdad=2\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n328.4616\n25.112\n13.080\n0.000\n279.242\n377.681\n\n\nTUE\n2.2275\n0.870\n2.560\n0.010\n0.522\n3.933\n\n\nCH2O\n-1.4150\n0.984\n-1.439\n0.150\n-3.343\n0.513\n\n\nFAF\n-1.3585\n0.747\n-1.820\n0.069\n-2.822\n0.105\n\n\nAge\n0.1537\n0.097\n1.591\n0.112\n-0.036\n0.343\n\n\nHeight\n-426.3945\n30.970\n-13.768\n0.000\n-487.095\n-365.694\n\n\nWeight\n5.3584\n0.372\n14.386\n0.000\n4.628\n6.088\n\n\nNObeyesdad=3\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n306.6447\n33.046\n9.279\n0.000\n241.876\n371.413\n\n\nTUE\n-7.8630\n5.691\n-1.382\n0.167\n-19.017\n3.291\n\n\nCH2O\n-21.0118\n11.508\n-1.826\n0.068\n-43.567\n1.543\n\n\nFAF\n-11.3624\n5.759\n-1.973\n0.048\n-22.650\n-0.075\n\n\nAge\n2.4017\n1.260\n1.905\n0.057\n-0.069\n4.872\n\n\nHeight\n-710.3867\n156.303\n-4.545\n0.000\n-1016.734\n-404.039\n\n\nWeight\n10.1072\n2.588\n3.905\n0.000\n5.034\n15.180\n\n\nNObeyesdad=4\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n352.4249\n33.573\n10.497\n0.000\n286.623\n418.227\n\n\nTUE\n-9.2469\n5.711\n-1.619\n0.105\n-20.440\n1.946\n\n\nCH2O\n-20.6780\n11.516\n-1.796\n0.073\n-43.250\n1.894\n\n\nFAF\n-14.7525\n5.794\n-2.546\n0.011\n-26.108\n-3.397\n\n\nAge\n2.1487\n1.262\n1.703\n0.089\n-0.325\n4.622\n\n\nHeight\n-758.2318\n156.401\n-4.848\n0.000\n-1064.772\n-451.692\n\n\nWeight\n10.5011\n2.589\n4.056\n0.000\n5.427\n15.575\n\n\nNObeyesdad=5\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n126.2892\n12.539\n10.072\n0.000\n101.713\n150.865\n\n\nTUE\n0.5832\n0.541\n1.077\n0.281\n-0.478\n1.645\n\n\nCH2O\n-0.8764\n0.706\n-1.242\n0.214\n-2.260\n0.507\n\n\nFAF\n-0.1920\n0.403\n-0.476\n0.634\n-0.983\n0.599\n\n\nAge\n0.0719\n0.082\n0.874\n0.382\n-0.089\n0.233\n\n\nHeight\n-160.2982\n14.026\n-11.429\n0.000\n-187.788\n-132.808\n\n\nWeight\n2.3663\n0.208\n11.397\n0.000\n1.959\n2.773\n\n\nNObeyesdad=6\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n207.3760\n15.374\n13.489\n0.000\n177.244\n237.508\n\n\nTUE\n1.6561\n0.646\n2.564\n0.010\n0.390\n2.922\n\n\nCH2O\n-0.6583\n0.773\n-0.851\n0.395\n-2.174\n0.857\n\n\nFAF\n-0.1243\n0.485\n-0.256\n0.798\n-1.076\n0.827\n\n\nAge\n0.1042\n0.087\n1.197\n0.231\n-0.066\n0.275\n\n\nHeight\n-266.6050\n17.598\n-15.150\n0.000\n-301.097\n-232.113\n\n\nWeight\n3.6160\n0.241\n15.026\n0.000\n3.144\n4.088\n\n\n\n\n\n\n\n\n\n\n\nImpact on Model Accuracy\nAfter applying feature selection based on CrossTab Sparsity, we’ll compare the accuracy of our new models against our baseline models. This comparison will reveal how effectively CrossTab Sparsity enhances classification performance.\n\nResults and Discussion: Unveiling Insights\nModel Comparison Table\nAfter implementing CrossTab Sparsity in our feature selection process, let’s take a look at the results:\n\n\nComparision Code\nmetrics = {\n    \"Metric\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"],\n    \"Baseline Model with all Parameters\": [\n        accuracy_score(y_test, base_preds),\n        precision_score(y_test, base_preds, average='weighted'),\n        recall_score(y_test, base_preds, average='weighted'),\n        f1_score(y_test, base_preds, average='weighted'),\n    ],\n    \"Challenger Model with only 5 Variables\": [\n        accuracy_score(y_test, challenger_preds),\n        precision_score(y_test, challenger_preds, average='weighted'),\n        recall_score(y_test, challenger_preds, average='weighted'),\n        f1_score(y_test, challenger_preds, average='weighted'),\n    ]\n}\ndisplay(pd.DataFrame(metrics).round(4).set_index('Metric').T)\n\n\n\n\n\n\n\n\nMetric\nAccuracy\nPrecision\nRecall\nF1-Score\n\n\n\n\nBaseline Model with all Parameters\n0.9100\n0.9123\n0.9100\n0.9103\n\n\nChallenger Model with only 5 Variables\n0.9384\n0.9384\n0.9384\n0.9381\n\n\n\n\n\n\n\nInsights Gained\nThrough this analysis, several key insights emerge:\n\n\nReduction of similar accuracy from 16 to 5 i.e 68.75% reduction\n\n\n\nFeature Interactions Matter: The selected features based on CrossTab Sparsity significantly improved model accuracy—like finding out which ingredients make your favorite dish even better!\nSimplicity is Key: By focusing on relevant features, we enhance accuracy while simplifying model interpretation—because nobody likes unnecessary complexity.\nReal-World Applications: These findings have practical implications in fields such as environmental science where classification plays a critical role—helping us make better decisions for our planet.\n\n\n\n\nConclusion: The Road Ahead\nIn conclusion, this blog has illustrated how CrossTab Sparsity can be a game-changer in classification tasks using the Obesity dataset. By leveraging this metric for feature selection, we achieved notable improvements in model performance—proof that sometimes less really is more!\nFuture Work: Expanding Horizons\nAs we look ahead, there are exciting avenues to explore:\n\nInvestigating regression problems using CrossTab Sparsity.\nComparing its effectiveness with other feature selection methods such as Recursive Feature Elimination (RFE) or comparision with other feature selection mehtods.\n\nBy continuing this journey into data science, we not only enhance our technical skills but also contribute valuable insights that can drive meaningful change in various industries.\n\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "vault/index.html",
    "href": "vault/index.html",
    "title": "The Vault",
    "section": "",
    "text": "Knowledge Base\n\nThe Vault\nFrameworks, code, and strategy for AI Architects.\n\n\n\nStrategyEngineering\n\n\nFor the C-Suite. ROI-focused frameworks and business transformation insights.\n\n\n\n\nNo matching items\n\n\n\nFor the builders. Code, implementation patterns, and technical deep-dives.\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "consulting.html",
    "href": "consulting.html",
    "title": "Work Directly With Me",
    "section": "",
    "text": "Strategic Consulting\n\n\nLimited 1:1 consulting engagements for Fortune 500 enterprises."
  },
  {
    "objectID": "consulting.html#the-approach",
    "href": "consulting.html#the-approach",
    "title": "Work Directly With Me",
    "section": "The Approach",
    "text": "The Approach\nAfter 14 years architecting AI systems across automotive, retail, insurance, FMCG, and telecommunications, I’ve learned one thing:\nThe best solutions come from looking sideways.\nMost consultants stay within their industry expertise. I bring patterns from completely different domains to solve problems that seem intractable.\nHealthcare insights solving automotive challenges. Retail strategies transforming manufacturing operations. Mathematical frameworks from applied physics applied to business systems."
  },
  {
    "objectID": "consulting.html#what-well-work-on",
    "href": "consulting.html#what-well-work-on",
    "title": "Work Directly With Me",
    "section": "What We’ll Work On",
    "text": "What We’ll Work On\n\n\nAI Strategy & Architecture\nMap your AI maturity. Design scalable systems. Build transformation roadmaps that actually deploy.\n\n\nCross-Industry Pattern Recognition\nApply insights from adjacent industries. Solve problems others can’t see because they’re looking straight ahead.\n\n\nExecutive Team Alignment\nGet your leadership on the same page about AI transformation. No vendor pitches. Just architecture.\n\n\nImplementation Support\nI don’t just hand you a deck and leave. I work alongside your team until the system is live and scaled."
  },
  {
    "objectID": "consulting.html#who-this-is-for",
    "href": "consulting.html#who-this-is-for",
    "title": "Work Directly With Me",
    "section": "Who This Is For",
    "text": "Who This Is For\nFortune 500 enterprises with real transformation budgets.\nLeaders who are tired of prescriptive AI and ready for predictive strategy.\nOrganizations ready to commit to systemic change, not point solutions.\nThis isn’t for everyone. But if you’re the right fit, the ROI is undeniable."
  },
  {
    "objectID": "consulting.html#current-engagement-model",
    "href": "consulting.html#current-engagement-model",
    "title": "Work Directly With Me",
    "section": "Current Engagement Model",
    "text": "Current Engagement Model\nI work with a limited number of clients at any given time, typically serving as Acting CTO or Strategic AI Advisor across multiple accounts.\nTypical engagement: 6-12 months, embedded with your leadership team.\nInvestment: Custom scoped based on organization size and transformation goals."
  },
  {
    "objectID": "consulting.html#apply-for-consulting",
    "href": "consulting.html#apply-for-consulting",
    "title": "Work Directly With Me",
    "section": "Apply for Consulting",
    "text": "Apply for Consulting\nCurrent waitlist: [Q2 2025]\nIf you’re exploring a major AI transformation and want to discuss whether we’re a fit, let’s talk.\nSchedule a Discovery Call\n\nOr email me directly at jitin@jitinkapila.com"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research Work",
    "section": "",
    "text": "Unique Metric for Health Analysis with Optimization of Clustering Activity and Cross Comparison of Results from Different Approach. Paper Link\n\n\n\nA method for estimating clusters holistically. The metric dose not depend on any assumptions about the Approach to Cluster the Data. All that matters is the Data what it fits on and what are the Cluster Segments observed.\n\n\n\n\n\nReinforcement Evolutionary Learning Method for Self-Learning (RELM).Paper Link\n\n\nRELM is a take on using Reinforcement Learning on Quantitative Data where learning happens via Evolutionary Algorithms (GA, ES, NES, NSGA-II, etc). This method takes on the challenge age old problem of Concept Drift.\n\n\nFuturistic Classification with Dynamic Reference Frame Strategy.Paper Link\n\n\nAs a Data science practitioner, one of the classical problem is to understand The Churn. This paper takes a perspective on data that enables to identify what actually distinguishes Churn from Other.\n\n\nPersonalized Influence Estimation Technique(PIE). Paper Link\n\n\nWe have lot of methods to access the global influence of a variable in data for most of ML Algorithms. But to make a justifiable point estimate about a datum might be a key source of information in many cases of Anomaly detection, Churn Analysis or Employee Attrition Reduction, etc.. Individual PIE values (point estimates) can be used to trigger a certain action. In this paper we estimate PIE values for Linear and Non Linear models.\n\n\nDistribution Assertive Regression (DAR). Paper Link and Blog Post\n\n\nDAR is regression analysis in which we understand how a fit of regression varies over it’s value. In this paper we resolve the Regression by Parts and estimate the new data based on heuristic methods like K-nearest Neighbor and Regression Equation."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stop Guessing. Start Architecting.",
    "section": "",
    "text": "Framework for AI Leaders\n\n\nThe operating system for AI-driven enterprises. Transform from ‘Corporate AI Lead’ to ‘Strategic AI Architect’.\n\nStart the Protocol Read the Manifesto"
  },
  {
    "objectID": "index.html#the-ai-art-matrix",
    "href": "index.html#the-ai-art-matrix",
    "title": "Stop Guessing. Start Architecting.",
    "section": "The AI-ART Matrix",
    "text": "The AI-ART Matrix\nWhy 73% of enterprise AI initiatives fail to scale.\n\n\n\nThe Old Way\nThe Architect Way\n\n\n\n\nPrescriptive AI\nPredictive Strategy\n\n\nTool-first thinking\nOutcome-first design\n\n\nSiloed implementations\nSystemic integration\n\n\nVendor-dependent\nArchitecture-independent\n\n\nReactive problem-solving\nProactive pattern recognition\n\n\nDashboard metrics\nBusiness transformation"
  },
  {
    "objectID": "index.html#the-olcd-protocol",
    "href": "index.html#the-olcd-protocol",
    "title": "Stop Guessing. Start Architecting.",
    "section": "The OLCD Protocol",
    "text": "The OLCD Protocol\nObjective → Learner → Controller → Diagnostic\nA battle-tested operating system for enterprise AI transformation.\nAfter 14 years across automotive, retail, insurance, FMCG, and telecommunications, I’ve distilled what separates successful AI implementations from expensive experiments.\nIt’s not about having the best tools. It’s about having the right architecture.\nExplore the Protocol →"
  },
  {
    "objectID": "index.html#what-youll-master",
    "href": "index.html#what-youll-master",
    "title": "Stop Guessing. Start Architecting.",
    "section": "What You’ll Master",
    "text": "What You’ll Master\n\n\nStrategic Architecture\nMap your organization’s AI maturity. Identify gaps between current state and AI-driven future. Design systems that scale.\n\n\nPattern Recognition\nApply cross-industry insights. See solutions others miss. Solve problems sideways.\n\n\nImplementation Frameworks\nOLCD Protocol. AI-ART Matrix. Blueprint methodology. Deployable from day one."
  },
  {
    "objectID": "index.html#ready-to-transform-your-approach",
    "href": "index.html#ready-to-transform-your-approach",
    "title": "Stop Guessing. Start Architecting.",
    "section": "Ready to Transform Your Approach?",
    "text": "Ready to Transform Your Approach?\nThe next cohort of The AI Architect Protocol starts soon.\nJoin the Blueprint →"
  },
  {
    "objectID": "insights.html",
    "href": "insights.html",
    "title": "Insights",
    "section": "",
    "text": "Examples of how cross-industry pattern recognition transforms complex challenges into elegant solutions."
  },
  {
    "objectID": "insights.html#mathematical-thinking-applied-to-business-problems",
    "href": "insights.html#mathematical-thinking-applied-to-business-problems",
    "title": "Insights",
    "section": "",
    "text": "Examples of how cross-industry pattern recognition transforms complex challenges into elegant solutions."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Jitin Kapila",
    "section": "",
    "text": "14 years. 6 industries. One insight that changes everything."
  },
  {
    "objectID": "about.html#the-origin-story",
    "href": "about.html#the-origin-story",
    "title": "Jitin Kapila",
    "section": "The Origin Story",
    "text": "The Origin Story\nI didn’t set out to become “The AI Architect.”\nI started as a mechanical engineer with a background in applied mathematics. Early in my career, I was hired to solve automotive inventory problems that traditional approaches couldn’t crack.\nThe patterns were too complex. The variables too numerous. The standard playbooks weren’t working.\nThen I had a realization:\nWhat if I stopped thinking about this like an automotive problem and started thinking about it like a healthcare system managing patient flow?\nSuddenly, the solution became obvious. We eliminated 45,000 excess units.\nNot because I knew more about automotive than the automotive experts.\nBecause I was looking at it sideways."
  },
  {
    "objectID": "about.html#the-pattern-emerges",
    "href": "about.html#the-pattern-emerges",
    "title": "Jitin Kapila",
    "section": "The Pattern Emerges",
    "text": "The Pattern Emerges\nOver 14 years across automotive, retail, insurance, FMCG, and telecommunications, I kept seeing the same thing:\nThe breakthrough comes from applying insights from completely different domains.\nHealthcare solving automotive. Retail transforming manufacturing. Mathematical frameworks from physics applied to business systems.\nMost experts stay within their industry expertise. They know their domain deeply, but they’re looking straight ahead.\nI learned to look sideways."
  },
  {
    "objectID": "about.html#the-mathematical-mindset",
    "href": "about.html#the-mathematical-mindset",
    "title": "Jitin Kapila",
    "section": "The Mathematical Mindset",
    "text": "The Mathematical Mindset\nIn mathematics, there’s never just one solution to a problem. Some roots are simply more beautiful than others.\nThis philosophy drives how I approach enterprise AI:\n\nPattern recognition across domains instead of single-industry expertise\nRoot cause analysis instead of surface-level fixes\nElegant solutions over brute force approaches\nSystemic thinking instead of point solutions\n\nWhen you’ve solved similar problems in healthcare, the path through retail becomes visible. When you understand the mathematics of one optimization challenge, you start seeing the same structures everywhere."
  },
  {
    "objectID": "about.html#current-work",
    "href": "about.html#current-work",
    "title": "Jitin Kapila",
    "section": "Current Work",
    "text": "Current Work\nI serve as Acting CTO across multiple Fortune 500 accounts while mentoring a team of 25+ professionals.\nThe work spans:\n\nPredictive analytics and AI strategy architecture\nCross-industry system optimization\nExecutive team transformation and strategic planning\nImplementation frameworks that actually scale\n\nI also run The AI Architect Protocol — a 3-day intensive that teaches enterprise leaders to think like architects instead of tool users."
  },
  {
    "objectID": "about.html#the-framework",
    "href": "about.html#the-framework",
    "title": "Jitin Kapila",
    "section": "The Framework",
    "text": "The Framework\nAfter years of cross-industry work, I’ve distilled the approach into repeatable systems:\nThe OLCD Protocol: Observe → Learn → Create → Deploy\nThe AI-ART Matrix: A diagnostic framework that reveals why 73% of enterprise AI initiatives fail to scale\nThe Blueprint Methodology: Architecture-first thinking for AI transformation\nThese aren’t theoretical models. They’re battle-tested frameworks deployed across six industries."
  },
  {
    "objectID": "about.html#beyond-the-work",
    "href": "about.html#beyond-the-work",
    "title": "Jitin Kapila",
    "section": "Beyond the Work",
    "text": "Beyond the Work\nWhen I’m not architecting AI systems, you’ll find me:\n\nPlaying guitar (badly but enthusiastically)\nPlanning the next off-the-beaten-path adventure\nReading mathematical proofs for fun (yes, really)\n\nGetting lost in unfamiliar places keeps the mind flexible. Some of my best insights come from being completely outside my comfort zone."
  },
  {
    "objectID": "about.html#want-to-work-together",
    "href": "about.html#want-to-work-together",
    "title": "Jitin Kapila",
    "section": "Want to Work Together?",
    "text": "Want to Work Together?\nI take on a limited number of consulting engagements and run cohort-based training for enterprise AI leaders.\nIf you’re exploring a major AI transformation, let’s talk.\nExplore Consulting Join The Protocol\n\nOr just email me to discuss your challenge."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "📧 Mail | 🔗 LinkedIn | 📍 Bengaluru, India\n\n\n\nCross-industry problem solver with 14+ years applying mathematical thinking to business challenges across automotive, retail, FMCG, telecommunications, and insurance. Currently serving as Acting CTO for multiple Fortune 500 accounts while developing pattern recognition methodologies that transfer insights between industries.\n\n\n\n\n\n\nMulti-Account Strategic Leadership - Simultaneously managed technical strategy across 4+ Fortune 500 clients - Developed cross-industry pattern recognition framework for solution transfer - Built and mentored cross-functional teams of 15+ professionals - Currently mentoring 25+ professionals across multiple accounts\nBusiness Impact - Automotive: Reduced inventory excess by 45,000 units through predictive analytics - Retail: Optimized marketing allocation resulting in $500K annual budget efficiency - Insurance: Implemented claims analytics reducing costs by ¥20M - FMCG: Post-pandemic inventory strategy eliminating $850K in holding costs\n\n\n\nEnterprise AI Strategy & Implementation - Led AI transformation initiatives for Fortune 500 companies - Applied machine learning solutions across automotive, FMCG, telecommunications - Consistently delivered 3-7x return on investments within 2-3 year timeframes - Pioneered cross-industry solution adaptation methodology\n\n\n\nFoundation Building & Technical Excellence - Built expertise in advanced analytics, machine learning, and statistical modeling - Gained experience across 6+ industries, developing pattern recognition abilities - Designed scalable AI systems for enterprise-level implementations\n\n\n\n\n\nStrategic Leadership - Cross-Industry Pattern Recognition & Innovation - Team Building & Talent Development - Executive Stakeholder Management\nTechnical Excellence - Advanced Analytics & Machine Learning Architecture - Predictive Modeling & Statistical Analysis - Enterprise AI System Design & Implementation\nIndustry Expertise - Automotive: Inventory optimization, supply chain analytics - Retail/FMCG: Customer analytics, marketing optimization, promotional planning - Insurance: Claims analytics, risk modeling, fraud detection - Telecommunications: Customer lifecycle management, network optimization\n\n\n\n\nMaster of Technolog | Data Sceince & Engineering | BITS Pilani\nBachelor of Engineering | Mechanical Engineering | BIT, Durg\n\n\n\n\nFractional Client AI-CTO across multiple Fortune 500 accounts, developing frameworks for cross-industry insight transfer while building the next generation of analytical leaders.\n\n\nAvailable for senior strategic roles and consulting engagements\nDownload PDF Resume | Get in Touch"
  },
  {
    "objectID": "resume.html#professional-summary",
    "href": "resume.html#professional-summary",
    "title": "Resume",
    "section": "",
    "text": "Cross-industry problem solver with 14+ years applying mathematical thinking to business challenges across automotive, retail, FMCG, telecommunications, and insurance. Currently serving as Acting CTO for multiple Fortune 500 accounts while developing pattern recognition methodologies that transfer insights between industries."
  },
  {
    "objectID": "resume.html#professional-experience",
    "href": "resume.html#professional-experience",
    "title": "Resume",
    "section": "",
    "text": "Multi-Account Strategic Leadership - Simultaneously managed technical strategy across 4+ Fortune 500 clients - Developed cross-industry pattern recognition framework for solution transfer - Built and mentored cross-functional teams of 15+ professionals - Currently mentoring 25+ professionals across multiple accounts\nBusiness Impact - Automotive: Reduced inventory excess by 45,000 units through predictive analytics - Retail: Optimized marketing allocation resulting in $500K annual budget efficiency - Insurance: Implemented claims analytics reducing costs by ¥20M - FMCG: Post-pandemic inventory strategy eliminating $850K in holding costs\n\n\n\nEnterprise AI Strategy & Implementation - Led AI transformation initiatives for Fortune 500 companies - Applied machine learning solutions across automotive, FMCG, telecommunications - Consistently delivered 3-7x return on investments within 2-3 year timeframes - Pioneered cross-industry solution adaptation methodology\n\n\n\nFoundation Building & Technical Excellence - Built expertise in advanced analytics, machine learning, and statistical modeling - Gained experience across 6+ industries, developing pattern recognition abilities - Designed scalable AI systems for enterprise-level implementations"
  },
  {
    "objectID": "resume.html#core-competencies",
    "href": "resume.html#core-competencies",
    "title": "Resume",
    "section": "",
    "text": "Strategic Leadership - Cross-Industry Pattern Recognition & Innovation - Team Building & Talent Development - Executive Stakeholder Management\nTechnical Excellence - Advanced Analytics & Machine Learning Architecture - Predictive Modeling & Statistical Analysis - Enterprise AI System Design & Implementation\nIndustry Expertise - Automotive: Inventory optimization, supply chain analytics - Retail/FMCG: Customer analytics, marketing optimization, promotional planning - Insurance: Claims analytics, risk modeling, fraud detection - Telecommunications: Customer lifecycle management, network optimization"
  },
  {
    "objectID": "resume.html#education",
    "href": "resume.html#education",
    "title": "Resume",
    "section": "",
    "text": "Master of Technolog | Data Sceince & Engineering | BITS Pilani\nBachelor of Engineering | Mechanical Engineering | BIT, Durg"
  },
  {
    "objectID": "resume.html#current-focus",
    "href": "resume.html#current-focus",
    "title": "Resume",
    "section": "",
    "text": "Fractional Client AI-CTO across multiple Fortune 500 accounts, developing frameworks for cross-industry insight transfer while building the next generation of analytical leaders.\n\n\nAvailable for senior strategic roles and consulting engagements\nDownload PDF Resume | Get in Touch"
  },
  {
    "objectID": "bootcamp.html",
    "href": "bootcamp.html",
    "title": "The AI Architect Protocol",
    "section": "",
    "text": "Live Cohort Training\n\n\nDon’t just learn AI. Install the Operating System.\nA 3-day intensive that transforms how you think about enterprise AI strategy."
  },
  {
    "objectID": "bootcamp.html#the-problem",
    "href": "bootcamp.html#the-problem",
    "title": "The AI Architect Protocol",
    "section": "The Problem",
    "text": "The Problem\nYou’ve been to the conferences. Read the whitepapers. Attended the vendor demos.\nBut when it’s time to actually implement AI at scale, the frameworks fall apart.\nWhy?\nBecause most AI training teaches you to think like a tool user. Not like an architect.\nThe AI Architect Protocol rewires your approach from the ground up."
  },
  {
    "objectID": "bootcamp.html#the-3-day-intensive",
    "href": "bootcamp.html#the-3-day-intensive",
    "title": "The AI Architect Protocol",
    "section": "The 3-Day Intensive",
    "text": "The 3-Day Intensive\n\n\nDay 1: Observe\nMap your organization’s AI maturity. Identify the gaps between where you are and where AI can take you. Learn the diagnostic frameworks that reveal hidden bottlenecks.\n\n\nDay 2: Learn\nMaster the frameworks. OLCD Protocol, AI-ART Matrix, and the Architecture Blueprint. Understand how to think systematically about AI transformation instead of project-by-project.\n\n\nDay 3: Create\nBuild your 90-day AI transformation roadmap. Leave with a deployable strategy, not theoretical concepts. Apply pattern recognition across industries to solve your specific challenges."
  },
  {
    "objectID": "bootcamp.html#what-you-get",
    "href": "bootcamp.html#what-you-get",
    "title": "The AI Architect Protocol",
    "section": "What You Get",
    "text": "What You Get\n\n\nLive Interactive Sessions\n3 full days of intensive training with direct access to 14+ years of cross-industry AI implementation experience.\n\n\nThe Framework Library\nOLCD Protocol. AI-ART Matrix. Blueprint Methodology. All templates, worksheets, and diagnostic tools.\n\n\nYour 90-Day Roadmap\nWalk away with a customized transformation plan for your organization. Not theory. Deployable strategy.\n\n\nLifetime Access\nRecording access, framework updates, and ongoing community support with other AI Architects."
  },
  {
    "objectID": "bootcamp.html#who-this-is-for",
    "href": "bootcamp.html#who-this-is-for",
    "title": "The AI Architect Protocol",
    "section": "Who This Is For",
    "text": "Who This Is For\nYou’re a director or above at an enterprise organization tasked with AI strategy.\nYou’re tired of vendor pitches and want to understand the underlying architecture.\nYou need results, not another certification to put on LinkedIn.\nThis isn’t for beginners. This is for leaders who need to architect systems that scale."
  },
  {
    "objectID": "bootcamp.html#ready-to-architect",
    "href": "bootcamp.html#ready-to-architect",
    "title": "The AI Architect Protocol",
    "section": "Ready to Architect?",
    "text": "Ready to Architect?\nNext cohort starts [INSERT DATE]. Limited to 20 seats.\nInvestment: $4,997\nSecure Your Seat →\n\nQuestions? Email me directly"
  },
  {
    "objectID": "posts/01_adaptive_regression/index.html",
    "href": "posts/01_adaptive_regression/index.html",
    "title": "Adaptive Regression",
    "section": "",
    "text": "Adapting path through mountains! Photo by Zülfü Demir📸\n\n\n\n\nIntroduction\nHere I am trying to express our logic to find such Observation. Lets dive in.\nThere are different value estimation technique like regression analysis and time-series analysis. Everyone of us has experimented on regression using OLS ,MLE, Ridge, LASSO, Robust etc., and also might have evaluated them using RMSE (Root Mean/Median Square Error), MAD (Mean/Median Absolute Deviation), MAE (Mean / Median Absolute Error) and MAPE (Mean/Median Absolute Percentage Error), etc…\nBut all of these gives a single point estimate that what is the overall error looks like. Just a different thought!! can we be sure that this single value of MAPE or MAE? How easy it is to infer that our trained model has fitted well across the distribution of dependent variable?\n\n\n\n\n\nPlot of Anscombe’s Quartet\n\n\n\n\n\nSome Descriptive Stats for Anscombe’s Quartet\n\n\nLet me give you a pretty small data-set to play with “The Anscombe’s quartet”. This is a very famous data-set by Francis Anscombe. Please refer the plots below to understand the distribution of y1, y2, y3, y4. Isn’t it different?\nWould the measure of central tendency and disportion be same for this data? I am sure none of us would believe but to our utter surprise we see all the descriptive stats are kind of same. Don’t believe me !!! Please see the results below ( Source: Wikipedia ):\n\n\nSo what we do Now!\nAstonished !!! Don’t be. This is what has been hiding behind those numbers. And this is why we really won’t be able to cross certain performance level. Unless you change some features or even do a lot of hyper parameter tuning, your results won’t vary much.\nIf you look at the average value of MAPE in each decile you would see an interesting pattern. Let us show you what we see that pattern. One day while working on a business problem where I was using regression on a discussion with Kumarjit, we deviced a different way of model diagnosis. We worked together to give this a shape and build on it.\n\nAs you can see it is absolutely evident that either of the side in the distribution of MAPE values is going wild!!!!!!! Still overall MAPE is good (18%).\n\n\n\nSeeking Scope of Improvement\nWe worked together to build a different framework to address such issues on the go and reduce the MAPE deterioration on the edge of the distribution.\nThis problems gives rise to a concept we named as Distribution Assertive Regression (DAR).\nDAR is a framework that is based on cancelling the weakness of one point summaries by using the classical concepts of Reliability Engineering : The Bath Tub Curve.\n\n\n\n\n\nPlot for Classical Bath Tub Curve using a Hazard Function\n\n\nThe Specialty of this curve is that it gives you the likelihood which areas one tends to have high failure rates. In our experiments when we replace failure with MAPE value and the Time with sorted (ascending) value of target / dependent variable, we observe the same phenomenon. This is likely to happen because most of regression techniques assumes Normal (Gaussian) Distribution of data and fits itself towards the central tendency of this distribution.\nBecause of this tendency, any regression methods tends to learn less about data which are away from the central tendency of the target.\nLets look at BostonHousing data from “mlbench” package in R.\n\n\n\nPlot for MAPE Bath Tub Curve for Decile Split “mdev” from Data\n\n\nHere the MAPE is calculated for each decile split of ordered target variable. As you can observe it is following the bath tub curve. Hence the validates our hypothesis that the regression method is not able to understand much about the data at the either ends of the distribution.\n\n\nFinal Analysis\nNow the DAR framework essentially fixes this weakness of regression method and understands the behavior of data which is stable and can be tweak in a fashion that can be use in general practice.\nPlot of MAPE Bath Tub Curve after applying DAR Framework for Decile Split “mdev” from Data\n\nHow this framework with same method reduced MAPEs so much and made model much more stable…?? Well here it is:\nThe DAR framework splits the data at either ends of the order target variable and performs regression on these “split” data individually. This inherently reduces the so called “noise” part of the data and treat it as an individual data.\n\n\n\nScoring on New Data\nNow you might be thinking while applying regression this sounds good but how will one score this on new data. Well to answer that we used our most simple yet very effective friend “KNN” (Though any multiclass Classifier can be used here). So ideally scoring involves two step method :\n\nScore new value against each KNN / Multiclass Classifier model of the data\nBased on closeness we score it with the regression method used for that part of data.\n\nSo now we know how we can improve the prediction power of data for regression.\n\n\n\nCode and Flowchart\nIf things are simple lets keep it simple. Refer flowchart and code below for implementation of this framework. Paper here!\n\nR codePython codeHere is the Flow Chart\n\n\n\n\n\n\n\n\nTipClick to Expand\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipClick to Expand\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngraph TB\n    \n    subgraph Testing\n        p1(Finding bucket of model to choose)\n        p1 --&gt; p2([Making predictions &lt;br&gt; based on selected model for inference])\n        p2 --&gt; p3(Consolidate final score of prediction)\n    end\n\n    subgraph Training\n        md([Fitting a &lt;br&gt;Regression model])==&gt; di\n        di{Binning Data via &lt;br/&gt; evaluating Distribution &lt;br/&gt; MAPE values }\n        di --&gt; md2([Fitting a Buckteing model &lt;br/&gt; to Binned MAPE Buckets])\n        md2 --&gt; md3([Fitting Regression &lt;br&gt; Models on Binned Data])\n        md == Keeping main&lt;br/&gt;model ==&gt; ro        \n        md3 ==&gt; ro(Final Models &lt;br&gt; Binning Data Models + &lt;br&gt; Set of Regressoin Models)\n    end\n\n    \n    od([Data Input]) -- Training&lt;br&gt; Data--&gt; md\n    od -- Testing&lt;br&gt; Data--&gt; p1\n    ro -.-&gt; p1\n    ro -.-&gt; p2\n\n    classDef green fill:#9f6,stroke:#333,stroke-width:2px;\n    classDef yellow fill:#ff6,stroke:#333,stroke-width:2px;\n    classDef blue fill:#00f,stroke:#333,stroke-width:2px,color:#fff;\n    classDef orange fill:#f96,stroke:#333,stroke-width:4px;\n    class md,md2,md3 green\n    class di orange\n    class p1,p2 yellow\n    class ro,p3 blue\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "posts/03_crosstab_sparsity/index.html",
    "href": "posts/03_crosstab_sparsity/index.html",
    "title": "CrossTab Sparsity",
    "section": "",
    "text": "Cluster analysis has always fascinated me as a window into the hidden structures of data. During my collaboration with Kumarjit Pathak, we grappled with a persistent challenge in unsupervised learning: how to objectively evaluate clustering quality across different algorithms. Traditional metrics like the Silhouette Index or Bayesian Information Criterion felt restrictive—they were siloed within specific methodologies, making cross-algorithm comparisons unreliable.\nThis frustration led us to develop a universal cluster evaluation metric, detailed in our paper “Cross Comparison of Results from Different Clustering Approaches”. Our goal was to create a framework that transcends algorithmic biases, enabling:\n- Direct comparison of K-Means vs GMM vs DBSCAN vs PAM vs SOM vs Anything results\n- Identification of variables muddying cluster separation\n- Automated determination of optimal cluster counts\nIn this blog, I’ll walk you through our journey—from conceptualization to real-world validation—and share insights that didn’t make it into the final paper."
  },
  {
    "objectID": "posts/03_crosstab_sparsity/index.html#our-aha-moment---crosstab-sparsity",
    "href": "posts/03_crosstab_sparsity/index.html#our-aha-moment---crosstab-sparsity",
    "title": "CrossTab Sparsity",
    "section": "Our “Aha!” Moment - Crosstab Sparsity",
    "text": "Our “Aha!” Moment - Crosstab Sparsity\n\n\n\n\nBest Cluster for K-means Using Crosstab sparsity\n\n\n\nWhile analyzing cross-tab matrices of variable distributions across clusters, we noticed a pattern: well-segregated clusters consistently showed higher frequencies along matrix diagonals. This inspired our two-part metric:\n\n\n\nSegregation Factor:\n# Simplified calculation from our codebase  \nmedian = np.median(cross_tab)  \nN_vk = np.sum(cross_tab &gt; median)  # Count \"well-segregated\" instances  \nExplanation Factor:\nexplanation = np.log(len(data) / (bins * clusters))  \n\n\nSegregation Factor: Measures how distinctly clusters separate data points. We used the median (not mean) to avoid skew from outlier-dominated matrices.\nExplanation Factor: Quantifies how well clusters capture data variability. The logarithmic term penalizes overfitting—a critical insight from debugging early over-segmented clusters.\n\nAnd the Final Formula:\nFor variable \\(v\\) with \\(k\\) clusters:\n\\[\nS_v^k = \\underbrace{\\frac{N_v^k}{\\max(l, k)}}_{\\text{Segregation}} \\times \\underbrace{\\ln\\left(\\frac{N_d}{l \\times k}\\right)}_{\\text{Explanation}}\n\\]\nwhere:\n- \\(N_v^k\\): Segregated instances (values above cross-tab matrix median)\n- \\(l\\): Number of value intervals for variable \\(v\\)\n- \\(N_d\\): Total observations\nThis formulation ensures algorithmic invariance, allowing comparison across methods like K-Means (distance-based) and GMM (probability-based). Also, now you can see from the formula two scenarios happens: 1. If each variable crosstab is too dense then their is no separation between classes 2. If each variable crosstab is too sparse then we loose on explanation.\nHence the curve reaches a maximum and then falls down giving use the separability that the cluster can produce:"
  },
  {
    "objectID": "extras/BRAND_MEMORY.html",
    "href": "extras/BRAND_MEMORY.html",
    "title": "AI Profit OS — Brand Memory Document",
    "section": "",
    "text": "Last Updated: December 6, 2025 Purpose: Persistent memory for AI assistants and personal reference\n\n\n\n\n\n\n\nExperience: 14+ years in AI/ML implementation\nIndustries: Automotive, Retail, Insurance, FMCG, Telecommunications\nNotable Achievement: Eliminated 45,000 excess automotive inventory units using cross-industry AI insights (healthcare → automotive)\nCurrent Role: Acting CTO across multiple accounts (Fortune 500)\nMentoring: 25+ AI/data science professionals\nEducation: Mechanical Engineering + Applied Mathematics background\nSuperpower: Cross-industry pattern recognition — applying solutions from one domain to another\n\n\n\n\n\nBrand Name: The AI Architect\nTagline: “Stop Guessing. Start Architecting.”\nCore Promise: Transform from “Corporate AI Lead” to “Strategic AI Architect”\nDifferentiation: Not another prompt engineering course — this is the operating system for enterprise AI leadership\n\n\n\n\n\nWebsite: jitinkapila.com\nLinkedIn: linkedin.com/in/jitinkapila\nGitHub: github.com/jkapila\nInstagram: @jitinkapila\nCompany Brand: Kriyalytics (www.kriyalytics.com)\n\n\n\n\n\n\n\n\nPrice: ₹19,999 (Founding Cohort) → ₹34,999 (Regular)\nThe Three Pillars:\n\n\n\nPillar\nFramework\nPurpose\n\n\n\n\nStrategy\nAI-ART Matrix\nDiagnose why AI initiatives fail\n\n\nExecution\nOLCD Framework\nSystematic implementation approach\n\n\nROI\nROIfication Grid\nMeasure and communicate value\n\n\n\nOLCD Framework Breakdown: - Observe → Map current state without judgment - Learn → Identify patterns connecting silos - Create → Design systems, not solutions - Deploy → Ship incrementally, measure relentlessly\nSession Structure (Each Day): | Block | Duration | Format | |——-|———-|——–| | Live Teaching | 1 hour | Lecture + frameworks | | Group Assignment | 1 hour | Breakout rooms | | Q&A / Clarity | 1 hour | Open discussion |\nDeliverables for Participants: - 90-Day AI Transformation Plan - Personal Workflow Documentation - Prompt Library for their use case - Continuous example they can reference\nFirst Cohort Dates: December 19, 20, 21, 2025\n\n\n\n\nPurpose: Lead generation funnel into paid workshop Structure: - Teaches initial concepts - Uses the SAME running example that continues into the workshop - Ends with workshop pitch\n\n\n\n\nPrice: ₹10,00,000+ ($10,000+) Focus: Enterprise AI Strategy Status: Future phase (after course proves concept)\n\n\n\n\n\n\n\nPRIMARY\n- Navy Deep:     #0C2B4E  (Primary buttons, headers)\n- Navy Royal:    #213555  (Hover states, secondary)\n- Slate Blue:    #3E5879  (Accent text)\n\nACCENT\n- Cream Gold:    #D8C4B6  (Dividers, highlights, premium feel)\n- Gold Hover:    #C4B09E  (Hover for gold elements)\n\nBACKGROUNDS\n- Light:         #FAFBFC  (Default page background)\n- Warm:          #F8F6F3  (Alternative sections)\n- Grid Lines:    rgba(33, 53, 85, 0.03)\n\nTEXT\n- Primary:       #0C2B4E  (Headlines)\n- Body:          #5A6A7A  (Paragraphs)\n- Muted:         #8896A6  (Meta info, timestamps)\n\n\n\n\n\n\nUse\nFont\nStyle\n\n\n\n\nHeadlines\nBebas Neue\nUppercase, tight tracking\n\n\nBody\nIBM Plex Sans\nClean, 400/500/600 weights\n\n\nCanva Alternative\nLato\nFor body (Bebas available in Canva)\n\n\n\n\n\n\n\nEngineering graph paper grid background\nSharp corners (border-radius: 0) on everything\nGeometric accents (corner brackets, divider lines)\nNo gradients, no blobs, no rounded edges\nIndustrial, architectural feel\n\n\n\n\n\nOption A — Minimal Blueprint: Light, professional, LinkedIn-optimized\nOption B — Dark Authority: Premium, high-contrast, Instagram carousels\nOption C — Editorial: Magazine-style with sidebar, educational content\n\n\n\n\n\n\n\n\n\nPlatform: Quarto (Static Site Generator)\nStyling: Custom CSS (no Bootstrap/Tailwind)\nHosting: Current hosting maintained\nAnalytics: Google Analytics 4\nSEO: Schema.org JSON-LD (ProfessionalService + Course)\n\n\n\n\n\nFunnel/Courses: Systeme.io\nPayments (India): Razorpay\nEmail Sequences: Systeme.io (built-in)\n\n\n\n\n\nBlog Integration: Substack embeds possible\nSocial: LinkedIn (primary), Instagram (secondary)\n\n\n\n\n\n\njitinkapila.com/\n├── index.qmd          # Homepage — Hero + Problem/Solution\n├── about.qmd          # Authority building\n├── vault/             # Blog (\"The Vault\")\n│   ├── index.qmd      # Tabbed listing (Strategy | Engineering)\n│   └── posts/         # Individual articles\n├── bootcamp.qmd       # AI Profit OS sales page\n├── consulting.qmd     # Waitlist for high-ticket\n├── _quarto.yml        # Configuration\n├── custom.css         # Blueprint design system\n└── schema.html        # SEO structured data\n\n\n\nLeft: “JITIN KAPILA” (logo/home)\nRight: The Protocol | The Vault | Consulting | About | [Join the Blueprint →]\n\n\n\n\n\n\n\n\n\n“Stop Guessing. Start Architecting.”\n“Don’t just learn AI. Install the Operating System.”\n“The framework Fortune 500 leaders use to move from reactive AI to strategic advantage.”\n“Why 73% of Enterprise AI Projects Fail (And How to Fix It)”\n\n\n\n\nMost organizations are stuck in “Prescriptive AI” — tool-first, siloed, vendor-dependent, measuring activity instead of impact.\n\n\n\nAI Profit OS installs a complete operating system: Strategy (AI-ART Matrix) + Execution (OLCD) + ROI (ROIfication Grid) = Predictable AI transformation.\n\n\n\n\n14 years Fortune 500 implementation\nCross-industry pattern recognition methodology\n45,000 unit automotive inventory case study\n25+ professionals mentored\n\n\n\n\n\n\n\n\n\nTitle: Senior Manager / Director / VP level\nFunction: AI/ML Lead, Data Science Head, Digital Transformation\nCompany: Enterprise / Fortune 500\nPain: AI initiatives that don’t scale, can’t prove ROI\nBudget: Can expense ₹20,000-50,000 for professional development\n\n\n\n\n\nTitle: Mid-level professionals aspiring to leadership\nGoal: Move from “implementer” to “strategist”\nBudget: Personal investment, price-sensitive\n\n\n\n\n\nStudents / Early career (&lt; 5 years)\nPeople looking for coding bootcamps\nThose wanting “prompt engineering tricks”\n\n\n\n\n\n\n\n\n\n\n\nSegment\nPrice Range\nYour Position\n\n\n\n\nMass market (Udemy)\n₹500-3,000\nAbove this\n\n\nCreator courses (Graphy)\n₹2,000-15,000\nAbove this\n\n\nPremium cohorts (GrowthX)\n₹30,000-60,000\nIn this tier\n\n\nExecutive ed (IIMs)\n₹75,000-2,00,000\nBelow this\n\n\n\n\n\n\n\nvs. Udemy/Coursera: Live, interactive, accountability\nvs. IIM certificates: Practical, faster, practitioner-led\nvs. Other creators: Enterprise focus, proprietary frameworks, real F500 experience\n\n\n\n\n\n\nAWARENESS                    CONSIDERATION                 DECISION\n    │                              │                           │\n    ▼                              ▼                           ▼\n┌─────────┐                 ┌─────────────┐             ┌──────────┐\n│LinkedIn │                 │Free Webinar │             │AI Profit │\n│ Posts   │────────────────►│  (2 hours)  │────────────►│   OS     │\n│Instagram│                 │             │             │ ₹19,999  │\n└─────────┘                 └─────────────┘             └──────────┘\n                                   │                          │\n                                   ▼                          ▼\n                            ┌─────────────┐            ┌───────────┐\n                            │ Email List  │            │Consulting │\n                            │   Nurture   │            │  Upsell   │\n                            └─────────────┘            └───────────┘\n\n\n\n\n\n\n\nWeek 1 (Dec 6-12): LinkedIn warmup + backend setup\nDec 13: Announcement post\nDec 14-18: Promotion + DM outreach\nDec 19-21: First cohort delivery\n\n\n\n\n\nCollect testimonials\nRefine curriculum\nRaise price to ₹34,999\nRun Cohort 2\n\n\n\n\n\nAdd evergreen webinar funnel\nIntroduce consulting offers\nConsider team/enterprise pricing\n\n\n\n\n\n\nWhen helping Jitin with content or strategy:\n\nMaintain brand voice: Confident, direct, no fluff. “I’ve done this” energy.\nUse the frameworks: Reference AI-ART Matrix, OLCD, ROIfication by name.\nDesign adherence: Navy + cream/gold palette, sharp corners, blueprint aesthetic.\nAudience awareness: Senior professionals, not beginners. Don’t dumb down.\nPlatform context: LinkedIn = professional insight, Instagram = visual frameworks.\n\n\nThis document should be loaded at the start of any conversation about Jitin’s business, content, or brand."
  },
  {
    "objectID": "extras/BRAND_MEMORY.html#complete-reference-for-jitin-kapilas-coaching-business",
    "href": "extras/BRAND_MEMORY.html#complete-reference-for-jitin-kapilas-coaching-business",
    "title": "AI Profit OS — Brand Memory Document",
    "section": "",
    "text": "Last Updated: December 6, 2025 Purpose: Persistent memory for AI assistants and personal reference"
  },
  {
    "objectID": "extras/BRAND_MEMORY.html#section-1-founder-profile",
    "href": "extras/BRAND_MEMORY.html#section-1-founder-profile",
    "title": "AI Profit OS — Brand Memory Document",
    "section": "",
    "text": "Experience: 14+ years in AI/ML implementation\nIndustries: Automotive, Retail, Insurance, FMCG, Telecommunications\nNotable Achievement: Eliminated 45,000 excess automotive inventory units using cross-industry AI insights (healthcare → automotive)\nCurrent Role: Acting CTO across multiple accounts (Fortune 500)\nMentoring: 25+ AI/data science professionals\nEducation: Mechanical Engineering + Applied Mathematics background\nSuperpower: Cross-industry pattern recognition — applying solutions from one domain to another\n\n\n\n\n\nBrand Name: The AI Architect\nTagline: “Stop Guessing. Start Architecting.”\nCore Promise: Transform from “Corporate AI Lead” to “Strategic AI Architect”\nDifferentiation: Not another prompt engineering course — this is the operating system for enterprise AI leadership\n\n\n\n\n\nWebsite: jitinkapila.com\nLinkedIn: linkedin.com/in/jitinkapila\nGitHub: github.com/jkapila\nInstagram: @jitinkapila\nCompany Brand: Kriyalytics (www.kriyalytics.com)"
  },
  {
    "objectID": "extras/BRAND_MEMORY.html#section-2-product-suite",
    "href": "extras/BRAND_MEMORY.html#section-2-product-suite",
    "title": "AI Profit OS — Brand Memory Document",
    "section": "",
    "text": "Price: ₹19,999 (Founding Cohort) → ₹34,999 (Regular)\nThe Three Pillars:\n\n\n\nPillar\nFramework\nPurpose\n\n\n\n\nStrategy\nAI-ART Matrix\nDiagnose why AI initiatives fail\n\n\nExecution\nOLCD Framework\nSystematic implementation approach\n\n\nROI\nROIfication Grid\nMeasure and communicate value\n\n\n\nOLCD Framework Breakdown: - Observe → Map current state without judgment - Learn → Identify patterns connecting silos - Create → Design systems, not solutions - Deploy → Ship incrementally, measure relentlessly\nSession Structure (Each Day): | Block | Duration | Format | |——-|———-|——–| | Live Teaching | 1 hour | Lecture + frameworks | | Group Assignment | 1 hour | Breakout rooms | | Q&A / Clarity | 1 hour | Open discussion |\nDeliverables for Participants: - 90-Day AI Transformation Plan - Personal Workflow Documentation - Prompt Library for their use case - Continuous example they can reference\nFirst Cohort Dates: December 19, 20, 21, 2025\n\n\n\n\nPurpose: Lead generation funnel into paid workshop Structure: - Teaches initial concepts - Uses the SAME running example that continues into the workshop - Ends with workshop pitch\n\n\n\n\nPrice: ₹10,00,000+ ($10,000+) Focus: Enterprise AI Strategy Status: Future phase (after course proves concept)"
  },
  {
    "objectID": "extras/BRAND_MEMORY.html#section-3-design-system",
    "href": "extras/BRAND_MEMORY.html#section-3-design-system",
    "title": "AI Profit OS — Brand Memory Document",
    "section": "",
    "text": "PRIMARY\n- Navy Deep:     #0C2B4E  (Primary buttons, headers)\n- Navy Royal:    #213555  (Hover states, secondary)\n- Slate Blue:    #3E5879  (Accent text)\n\nACCENT\n- Cream Gold:    #D8C4B6  (Dividers, highlights, premium feel)\n- Gold Hover:    #C4B09E  (Hover for gold elements)\n\nBACKGROUNDS\n- Light:         #FAFBFC  (Default page background)\n- Warm:          #F8F6F3  (Alternative sections)\n- Grid Lines:    rgba(33, 53, 85, 0.03)\n\nTEXT\n- Primary:       #0C2B4E  (Headlines)\n- Body:          #5A6A7A  (Paragraphs)\n- Muted:         #8896A6  (Meta info, timestamps)\n\n\n\n\n\n\nUse\nFont\nStyle\n\n\n\n\nHeadlines\nBebas Neue\nUppercase, tight tracking\n\n\nBody\nIBM Plex Sans\nClean, 400/500/600 weights\n\n\nCanva Alternative\nLato\nFor body (Bebas available in Canva)\n\n\n\n\n\n\n\nEngineering graph paper grid background\nSharp corners (border-radius: 0) on everything\nGeometric accents (corner brackets, divider lines)\nNo gradients, no blobs, no rounded edges\nIndustrial, architectural feel\n\n\n\n\n\nOption A — Minimal Blueprint: Light, professional, LinkedIn-optimized\nOption B — Dark Authority: Premium, high-contrast, Instagram carousels\nOption C — Editorial: Magazine-style with sidebar, educational content"
  },
  {
    "objectID": "extras/BRAND_MEMORY.html#section-4-tech-stack",
    "href": "extras/BRAND_MEMORY.html#section-4-tech-stack",
    "title": "AI Profit OS — Brand Memory Document",
    "section": "",
    "text": "Platform: Quarto (Static Site Generator)\nStyling: Custom CSS (no Bootstrap/Tailwind)\nHosting: Current hosting maintained\nAnalytics: Google Analytics 4\nSEO: Schema.org JSON-LD (ProfessionalService + Course)\n\n\n\n\n\nFunnel/Courses: Systeme.io\nPayments (India): Razorpay\nEmail Sequences: Systeme.io (built-in)\n\n\n\n\n\nBlog Integration: Substack embeds possible\nSocial: LinkedIn (primary), Instagram (secondary)"
  },
  {
    "objectID": "extras/BRAND_MEMORY.html#section-5-website-structure",
    "href": "extras/BRAND_MEMORY.html#section-5-website-structure",
    "title": "AI Profit OS — Brand Memory Document",
    "section": "",
    "text": "jitinkapila.com/\n├── index.qmd          # Homepage — Hero + Problem/Solution\n├── about.qmd          # Authority building\n├── vault/             # Blog (\"The Vault\")\n│   ├── index.qmd      # Tabbed listing (Strategy | Engineering)\n│   └── posts/         # Individual articles\n├── bootcamp.qmd       # AI Profit OS sales page\n├── consulting.qmd     # Waitlist for high-ticket\n├── _quarto.yml        # Configuration\n├── custom.css         # Blueprint design system\n└── schema.html        # SEO structured data\n\n\n\nLeft: “JITIN KAPILA” (logo/home)\nRight: The Protocol | The Vault | Consulting | About | [Join the Blueprint →]"
  },
  {
    "objectID": "extras/BRAND_MEMORY.html#section-6-key-messaging",
    "href": "extras/BRAND_MEMORY.html#section-6-key-messaging",
    "title": "AI Profit OS — Brand Memory Document",
    "section": "",
    "text": "“Stop Guessing. Start Architecting.”\n“Don’t just learn AI. Install the Operating System.”\n“The framework Fortune 500 leaders use to move from reactive AI to strategic advantage.”\n“Why 73% of Enterprise AI Projects Fail (And How to Fix It)”\n\n\n\n\nMost organizations are stuck in “Prescriptive AI” — tool-first, siloed, vendor-dependent, measuring activity instead of impact.\n\n\n\nAI Profit OS installs a complete operating system: Strategy (AI-ART Matrix) + Execution (OLCD) + ROI (ROIfication Grid) = Predictable AI transformation.\n\n\n\n\n14 years Fortune 500 implementation\nCross-industry pattern recognition methodology\n45,000 unit automotive inventory case study\n25+ professionals mentored"
  },
  {
    "objectID": "extras/BRAND_MEMORY.html#section-7-target-audience",
    "href": "extras/BRAND_MEMORY.html#section-7-target-audience",
    "title": "AI Profit OS — Brand Memory Document",
    "section": "",
    "text": "Title: Senior Manager / Director / VP level\nFunction: AI/ML Lead, Data Science Head, Digital Transformation\nCompany: Enterprise / Fortune 500\nPain: AI initiatives that don’t scale, can’t prove ROI\nBudget: Can expense ₹20,000-50,000 for professional development\n\n\n\n\n\nTitle: Mid-level professionals aspiring to leadership\nGoal: Move from “implementer” to “strategist”\nBudget: Personal investment, price-sensitive\n\n\n\n\n\nStudents / Early career (&lt; 5 years)\nPeople looking for coding bootcamps\nThose wanting “prompt engineering tricks”"
  },
  {
    "objectID": "extras/BRAND_MEMORY.html#section-8-competitive-positioning",
    "href": "extras/BRAND_MEMORY.html#section-8-competitive-positioning",
    "title": "AI Profit OS — Brand Memory Document",
    "section": "",
    "text": "Segment\nPrice Range\nYour Position\n\n\n\n\nMass market (Udemy)\n₹500-3,000\nAbove this\n\n\nCreator courses (Graphy)\n₹2,000-15,000\nAbove this\n\n\nPremium cohorts (GrowthX)\n₹30,000-60,000\nIn this tier\n\n\nExecutive ed (IIMs)\n₹75,000-2,00,000\nBelow this\n\n\n\n\n\n\n\nvs. Udemy/Coursera: Live, interactive, accountability\nvs. IIM certificates: Practical, faster, practitioner-led\nvs. Other creators: Enterprise focus, proprietary frameworks, real F500 experience"
  },
  {
    "objectID": "extras/BRAND_MEMORY.html#section-9-funnel-strategy",
    "href": "extras/BRAND_MEMORY.html#section-9-funnel-strategy",
    "title": "AI Profit OS — Brand Memory Document",
    "section": "",
    "text": "AWARENESS                    CONSIDERATION                 DECISION\n    │                              │                           │\n    ▼                              ▼                           ▼\n┌─────────┐                 ┌─────────────┐             ┌──────────┐\n│LinkedIn │                 │Free Webinar │             │AI Profit │\n│ Posts   │────────────────►│  (2 hours)  │────────────►│   OS     │\n│Instagram│                 │             │             │ ₹19,999  │\n└─────────┘                 └─────────────┘             └──────────┘\n                                   │                          │\n                                   ▼                          ▼\n                            ┌─────────────┐            ┌───────────┐\n                            │ Email List  │            │Consulting │\n                            │   Nurture   │            │  Upsell   │\n                            └─────────────┘            └───────────┘"
  },
  {
    "objectID": "extras/BRAND_MEMORY.html#section-10-timeline-milestones",
    "href": "extras/BRAND_MEMORY.html#section-10-timeline-milestones",
    "title": "AI Profit OS — Brand Memory Document",
    "section": "",
    "text": "Week 1 (Dec 6-12): LinkedIn warmup + backend setup\nDec 13: Announcement post\nDec 14-18: Promotion + DM outreach\nDec 19-21: First cohort delivery\n\n\n\n\n\nCollect testimonials\nRefine curriculum\nRaise price to ₹34,999\nRun Cohort 2\n\n\n\n\n\nAdd evergreen webinar funnel\nIntroduce consulting offers\nConsider team/enterprise pricing"
  },
  {
    "objectID": "extras/BRAND_MEMORY.html#usage-instructions-for-ai-assistants",
    "href": "extras/BRAND_MEMORY.html#usage-instructions-for-ai-assistants",
    "title": "AI Profit OS — Brand Memory Document",
    "section": "",
    "text": "When helping Jitin with content or strategy:\n\nMaintain brand voice: Confident, direct, no fluff. “I’ve done this” energy.\nUse the frameworks: Reference AI-ART Matrix, OLCD, ROIfication by name.\nDesign adherence: Navy + cream/gold palette, sharp corners, blueprint aesthetic.\nAudience awareness: Senior professionals, not beginners. Don’t dumb down.\nPlatform context: LinkedIn = professional insight, Instagram = visual frameworks.\n\n\nThis document should be loaded at the start of any conversation about Jitin’s business, content, or brand."
  },
  {
    "objectID": "extras/sample-blog-post.html",
    "href": "extras/sample-blog-post.html",
    "title": "Why 73% of Enterprise AI Projects Fail (And How to Fix It)",
    "section": "",
    "text": "Strategy Framework\n\n\nAnd the architectural blind spot that’s costing you millions."
  },
  {
    "objectID": "extras/sample-blog-post.html#the-pattern-i-keep-seeing",
    "href": "extras/sample-blog-post.html#the-pattern-i-keep-seeing",
    "title": "Why 73% of Enterprise AI Projects Fail (And How to Fix It)",
    "section": "The Pattern I Keep Seeing",
    "text": "The Pattern I Keep Seeing\nAfter 14 years of implementing AI across Fortune 500 companies, I’ve noticed something troubling. The same failure pattern emerges regardless of industry, budget, or team size.\nIt’s not a technology problem. It’s not a talent problem.\nIt’s an architecture problem."
  },
  {
    "objectID": "extras/sample-blog-post.html#the-ai-art-matrix",
    "href": "extras/sample-blog-post.html#the-ai-art-matrix",
    "title": "Why 73% of Enterprise AI Projects Fail (And How to Fix It)",
    "section": "The AI-ART Matrix",
    "text": "The AI-ART Matrix\nHere’s the framework I use to diagnose why AI initiatives stall:\n\n\n\nDimension\nPrescriptive (Failing)\nPredictive (Winning)\n\n\n\n\nApproach\nTool-first\nOutcome-first\n\n\nIntegration\nSiloed\nSystemic\n\n\nDependency\nVendor-locked\nArchitecture-independent\n\n\nMeasurement\nActivity metrics\nImpact metrics\n\n\n\nMost organizations operate in the left column. They buy tools, run pilots, and wonder why nothing scales."
  },
  {
    "objectID": "extras/sample-blog-post.html#the-fix-olcd-protocol",
    "href": "extras/sample-blog-post.html#the-fix-olcd-protocol",
    "title": "Why 73% of Enterprise AI Projects Fail (And How to Fix It)",
    "section": "The Fix: OLCD Protocol",
    "text": "The Fix: OLCD Protocol\nThe solution isn’t more AI—it’s better architecture.\nObserve → Map your current state without judgment.\nLearn → Identify the patterns that connect silos.\nCreate → Design systems, not solutions.\nDeploy → Ship incrementally, measure relentlessly."
  },
  {
    "objectID": "extras/sample-blog-post.html#your-next-step",
    "href": "extras/sample-blog-post.html#your-next-step",
    "title": "Why 73% of Enterprise AI Projects Fail (And How to Fix It)",
    "section": "Your Next Step",
    "text": "Your Next Step\nIf this resonates, you have two options:\n\nRead the full framework → The AI-ART Matrix Deep Dive\nJoin the next cohort → The AI Architect Protocol\n\n\n\nHave questions? Connect with me on LinkedIn."
  },
  {
    "objectID": "CLAUDE.html",
    "href": "CLAUDE.html",
    "title": "CLAUDE.md",
    "section": "",
    "text": "This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\n\n\nThis is “The AI Architect” brand website for Jitin Kapila, built with Quarto. The site transforms from a portfolio into a premium coaching/consulting platform featuring:\n\nHomepage with hero + frameworks (AI-ART Matrix, OLCD Protocol)\nCourse sales page (AI Profit OS - ₹19,999/₹34,999)\nConsulting waitlist\nBlog (“The Vault”) with Strategy/Engineering tabs\nAbout page (authority building)\n\nLive URLs: - Primary: https://jitinkapila.com - Netlify: https://percieveit.com\n\n\n\n\nStatic Site Generator: Quarto (1.4+)\nStyling Approach: SCSS + CSS (hybrid system - see below)\nCurrent Theme: Brite (Bootstrap-based)\nAnalytics: Umami (includes/umami.html)\nSEO: Schema.org JSON-LD (includes/schema.html)\nComments: Utterances (GitHub-based, on blog posts)\nPublishing: Netlify\n\n\n\n\n\n\nThe project currently uses two styling systems which can cause conflicts:\n\ncustom.css - Direct CSS overrides (current implementation)\nstyles.scss - SCSS with Bootstrap variables (backup/original)\n\n\n\n\nBased on Quarto best practices and the Lumo theme example in extras/, here’s why SCSS is superior:\n\n\n\nNative Quarto Integration: Quarto processes SCSS at build time with Bootstrap\nVariable System: Can override Bootstrap variables ($primary, $font-family-sans-serif)\nTheme Inheritance: Extend themes properly instead of fighting them\nCompile-Time Safety: SCSS errors caught during build, not runtime\nSource Order Control: SCSS compiles BEFORE custom CSS in cascade\n\n\n\n\n\nOverride Battles: Must use !important everywhere to fight Bootstrap\nSpecificity Wars: Constantly battling theme defaults\nNo Variables: Can’t modify Bootstrap’s design tokens cleanly\nRuntime Conflicts: Themes like Cosmo/Brite have their own opinions\n\n\n\n\n\n/*-- scss:defaults --*/\n// Override Bootstrap variables BEFORE they compile\n@import url('https://fonts.googleapis.com/css2?family=Bebas+Neue&family=IBM+Plex+Sans:wght@400;500;600&display=swap');\n\n// Typography Variables\n$font-family-sans-serif: 'IBM Plex Sans', sans-serif;\n$headings-font-family: 'Bebas Neue', sans-serif;\n$headings-font-weight: 400;\n$headings-text-transform: uppercase;\n$headings-letter-spacing: 1px;\n\n// Color Variables\n$primary: #0C2B4E;      // Navy Deep\n$secondary: #213555;    // Navy Royal\n$body-color: #5A6A7A;\n$link-color: #0C2B4E;\n\n// Spacing\n$spacer: 1rem;\n\n// Border Radius (Blueprint aesthetic - no curves)\n$border-radius: 0;\n$border-radius-sm: 0;\n$border-radius-lg: 0;\n\n/*-- scss:rules --*/\n// Custom rules that extend the compiled theme\nbody {\n  background-image:\n    linear-gradient(rgba(33, 53, 85, 0.03) 1px, transparent 1px),\n    linear-gradient(90deg, rgba(33, 53, 85, 0.03) 1px, transparent 1px);\n  background-size: 20px 20px;\n}\n\nh1, h2, h3, h4, h5, h6 {\n  font-family: $headings-font-family;\n  text-transform: $headings-text-transform;\n  letter-spacing: $headings-letter-spacing;\n}\n\n\n\n\nVariables in /*-- scss:defaults --*/ override Bootstrap BEFORE compilation\nRules in /*-- scss:rules --*/ extend the theme cleanly\nNo !important needed - you’re modifying the source, not fighting it\nTheme changes are easy - just change theme name in _quarto.yml\n\n\n\n\n\n# Preview with live reload\nquarto preview\n\n# Production build\nquarto render\n\n# Publish to Netlify\nquarto publish netlify\nNote: Pre-render hook cleans _site/* automatically (see _quarto.yml)\n\n\n\n/\n├── index.qmd              # Homepage (Hero + AI-ART Matrix)\n├── about.qmd              # Authority building\n├── bootcamp.qmd           # AI Profit OS sales page\n├── consulting.qmd         # Consulting waitlist\n├── vault/\n│   ├── index.qmd          # Blog listing with tabs\n│   └── (links to ../posts/)\n├── posts/                 # Blog posts\n│   ├── _metadata.yml      # Default settings for all posts\n│   └── [post_name]/index.qmd\n├── includes/\n│   ├── umami.html         # Analytics\n│   └── schema.html        # SEO structured data\n├── extras/                # Reference materials\n│   ├── BRAND_MEMORY.md    # Complete brand guide\n│   ├── design-tokens.json # Design system tokens\n│   └── lumo theme/        # Example Quarto extension\n├── _quarto.yml            # Site configuration\n├── custom.css             # Current CSS (to migrate to SCSS)\n├── styles.scss            # Backup SCSS system\n└── backup/                # Original files backup\n\n\n\n\n\n\nBrand Name: The AI Architect\nTagline: “Stop Guessing. Start Architecting.”\nVisual Motif: Engineering blueprint aesthetic\nCore Principle: Sharp corners, no gradients, geometric precision\n\n\n\n\n:root {\n  /* Primary */\n  --navy-deep: #0C2B4E;      /* Headers, buttons, primary text */\n  --navy-royal: #213555;     /* Hover states, secondary */\n  --slate-blue: #3E5879;     /* Accent text */\n\n  /* Accent */\n  --cream-gold: #D8C4B6;     /* Dividers, highlights */\n  --gold-hover: #C4B09E;     /* Gold hover state */\n\n  /* Backgrounds */\n  --bg-light: #FAFBFC;       /* Default background */\n  --bg-warm: #F8F6F3;        /* Alternate sections */\n  --bg-grid: rgba(33, 53, 85, 0.03);  /* Blueprint grid */\n\n  /* Text */\n  --text-primary: #0C2B4E;   /* Headlines */\n  --text-body: #5A6A7A;      /* Paragraphs */\n  --text-muted: #8896A6;     /* Meta info */\n}\n\n\n\n/* Headers: Bebas Neue - Bold, Uppercase, Tight */\nh1, h2, h3 {\n  font-family: 'Bebas Neue', sans-serif;\n  text-transform: uppercase;\n  letter-spacing: 1px;\n}\n\n/* Body: IBM Plex Sans - Clean, Professional */\nbody {\n  font-family: 'IBM Plex Sans', sans-serif;\n  font-weight: 400;\n  line-height: 1.7;\n}\n\n/* Responsive Sizing */\nh1 { font-size: clamp(48px, 8vw, 80px); line-height: 0.95; }\nh2 { font-size: clamp(32px, 5vw, 48px); line-height: 1.1; }\nh3 { font-size: clamp(24px, 3vw, 32px); line-height: 1.2; }\n\n/* Eyebrow Text */\n.eyebrow {\n  font-size: 11px;\n  font-weight: 600;\n  letter-spacing: 3px;\n  text-transform: uppercase;\n  color: var(--text-muted);\n}\n\n\n\nAll components follow border-radius: 0 (no rounded corners):\n\nButtons: Sharp, uppercase, slide-in hover effect\nCards: 1-2px border, subtle shadow on hover\nDividers: 60px horizontal lines in cream-gold\nGrid Background: 20px × 20px blueprint grid at 3% opacity\n\n\n\n\n\n\n\nproject:\n  type: website\n  output-dir: _site\n  pre-render: [\"rm -rf _site/*\"]  # Clean build\n  resources:\n    - CNAME\n    - favicon-32x32.png\n    - favicon.ico\n    - apple-touch-icon.png\n\nwebsite:\n  title: \"Jitin Kapila | The AI Architect\"\n  site-url: https://jitinkapila.com\n\n  navbar:\n    background: transparent  # Let CSS control this\n    left:\n      - text: \" \"  # Blank for logo/brand\n        href: index.qmd\n    right:\n      - text: \"The Protocol\"\n        href: bootcamp.qmd\n      # ... other nav items\n\n  page-footer:\n    left: \"© 2025 Jitin Kapila | The AI Architect\"\n    right:\n      - icon: linkedin\n        href: https://linkedin.com/in/jitinkapila\n      # ... other social icons\n\nformat:\n  html:\n    theme: brite  # Can also use: cosmo, flatly, litera\n    css: custom.css  # Or use SCSS with theme: [custom.scss]\n    include-in-header:\n      - includes/umami.html\n      - includes/schema.html\n    toc: false  # Disable by default, enable per-page\n    code-fold: true\n    highlight-style: github\n    smooth-scroll: true\n\nexecute:\n  freeze: auto  # Cache computational output\n  cache: true\n\ngoogle-analytics: \"G-XXXXXXXXXX\"  # Replace with actual ID\n\n\n\n\nTheme Selection: Use Bootstrap-based themes (cosmo, brite, flatly, litera)\nCSS Loading Order: Theme → CSS → Page-specific styles\nFreeze Strategy: freeze: auto prevents re-running code unnecessarily\nResources: Always include CNAME, favicons for deployment\n\n\n\n\n\n\n\n---\ntitle: \"Post Title\"\ndescription: \"Brief description for listings\"\ndate: \"YYYY-MM-DD\"\ncategories: [strategy, code]  # For tab filtering\ntags: [ai, ml, enterprise]\nimage: \"img/feature.jpg\"\nimage-alt: \"Alt text\"\ndraft: false  # Set true to hide\nauthor: \"Jitin Kapila\"  # Inherited from posts/_metadata.yml\n---\n\n\n\nposts/_metadata.yml sets defaults for ALL posts:\nfreeze: true\nauthor: 'Jitin Kapila'\ntoc: true\ntoc-depth: 3\ntoc-location: left\npage-layout: article\nreference-location: margin  # Tufte-style margin notes\ncitation-location: margin\nlightbox: true\nlicense: \"CC BY\"\ncode-line-numbers: true\ncode-fold: true\ncomments:\n  utterances:\n    repo: jkapila/perceptions\n    theme: github-light\n\n\n\nThe Vault uses categories to separate posts:\n\nStrategy Tab: categories: [strategy, business, roi]\nEngineering Tab: categories: [code, llm, agents]\n\nNote: Posts can appear in multiple tabs if they have multiple categories.\n\n\n\n\n\n\nQuarto uses fenced divs for layout control:\n::: {.hero-section}\nContent here\n:::\n\n::: {.column-body-outset}\nWide content that extends beyond normal margins\n:::\n\n::: {.panel-tabset}\n## Tab 1\nContent\n## Tab 2\nMore content\n:::\n\n\n\n\n.column-body - Normal width (default)\n.column-body-outset - Slightly wider\n.column-page - Page width\n.column-screen - Full screen width\n.column-margin - Margin notes (Tufte style)\n\n\n\n\nFor deep customization, use template partials (see extras/lumo theme/):\nformat:\n  html:\n    template-partials:\n      - title-block.html  # Custom title block\n      - footer.html       # Custom footer\n\n\n\n\nQuarto 1.4+ supports brand.yml for centralized design tokens. This is the FUTURE direction:\n\n\nmeta:\n  name: \"The AI Architect\"\n  link: \"https://jitinkapila.com\"\n\ncolor:\n  palette:\n    navy-deep: \"#0C2B4E\"\n    navy-royal: \"#213555\"\n    cream-gold: \"#D8C4B6\"\n  background: \"#FAFBFC\"\n  foreground: \"#0C2B4E\"\n  primary: \"#0C2B4E\"\n  secondary: \"#213555\"\n\ntypography:\n  fonts:\n    - family: \"Bebas Neue\"\n      source: google\n    - family: \"IBM Plex Sans\"\n      source: google\n      weight: [400, 500, 600]\n  base-size: \"16px\"\n  headings:\n    family: \"Bebas Neue\"\n    weight: 400\n  body:\n    family: \"IBM Plex Sans\"\n    weight: 400\n    line-height: 1.7\n\n\n\n\nSingle Source of Truth: All design tokens in one file\nCross-Format: Works across HTML, PDF, DOCX outputs\nTooling Integration: IDEs can autocomplete brand values\nTeam Collaboration: Designers can edit without touching code\n\nStatus: Not yet implemented. Consider for Phase 2 refactor.\n\n\n\n\n\n\nLocated in includes/schema.html:\n&lt;script type=\"application/ld+json\"&gt;\n{\n  \"@context\": \"https://schema.org\",\n  \"@graph\": [\n    {\n      \"@type\": \"ProfessionalService\",\n      \"name\": \"Jitin Kapila - The AI Architect\",\n      \"founder\": {\n        \"@type\": \"Person\",\n        \"name\": \"Jitin Kapila\",\n        \"jobTitle\": \"AI Strategy Consultant\"\n      }\n    },\n    {\n      \"@type\": \"Course\",\n      \"name\": \"The AI Architect Protocol\",\n      \"provider\": { \"@type\": \"Person\", \"name\": \"Jitin Kapila\" }\n    }\n  ]\n}\n&lt;/script&gt;\n\n\n\nLocated in includes/umami.html - lightweight, privacy-focused alternative to Google Analytics.\n\n\n\nConfigured in _quarto.yml:\nwebsite:\n  open-graph: true\n  twitter-card:\n    creator: \"@jitinkapila\"\n    card-style: summary_large_image\n\n\n\n\n\n\nexecute:\n  freeze: auto  # Only re-run when source changes\n  cache: true   # Cache results\nWhen to Clear Cache:\nrm -rf _freeze/\nquarto render\n\n\n\n\nUse thumbnails for listings (-thumb.jpg suffix)\nCompress images before adding to repo\nEnable lightbox for blog images (lightbox: true)\n\n\n\n\n\nUse highlight-style: github for consistency\nEnable code-fold: true for long code blocks\nAdd code-line-numbers: true for readability\n\n\n\n\n\n\n\nEdit _quarto.yml → website.navbar.right:\nnavbar:\n  right:\n    - text: \"New Page\"\n      href: newpage.qmd\n\n\n\nEdit _quarto.yml → page-footer.right:\npage-footer:\n  right:\n    - icon: youtube\n      href: https://youtube.com/@channel\n\n\n\n# Create file\ntouch newpage.qmd\n\n# Add frontmatter\n---\ntitle: \"Page Title\"\npage-layout: full  # or article, custom\n---\n\n\n\nEdit styles.scss → /*-- scss:defaults --*/:\n$primary: #0C2B4E;\n$secondary: #213555;\nThen in _quarto.yml:\nformat:\n  html:\n    theme: [cosmo, styles.scss]  # Theme + custom SCSS\n\n\n\n\n\n\nCause: Using theme: none removes Bootstrap navbar structure. Fix: Use a Bootstrap theme (cosmo, brite, flatly) + CSS overrides.\n\n\n\nCheck Order: 1. Is SCSS in /*-- scss:defaults --*/ section? 2. Is CSS loaded AFTER theme in _quarto.yml? 3. Are you using !important (symptom of wrong approach)?\nDebug:\nquarto preview --log-level debug\n\n\n\nSymptom: Old content showing despite changes. Fix:\nrm -rf _freeze/\nquarto render\n\n\n\nFix:\n# Kill all Quarto processes\npkill -f quarto\n\n# Restart preview\nquarto preview\n\n\n\n\n\n\n\nUse SCSS variables instead of CSS custom properties for Bootstrap integration\nPrefer theme extension over complete override\nAvoid !important - it indicates fighting the cascade\nTest mobile-first - use responsive breakpoints\nKeep specificity low - let cascade work naturally\n\n\n\n\n\nUse divs for layout, not CSS classes in markdown\nEnable freeze for computational posts - save build time\nAdd alt text to all images - SEO + accessibility\nUse relative links for internal navigation\nTest locally before pushing - quarto preview\n\n\n\n\n\nOptimize images before committing (use WebP when possible)\nUse code-fold for long code blocks\nEnable caching for expensive computations\nMinimize custom fonts (currently 2 families - good)\nLazy load images in listings\n\n\n\n\n\n\n\n\nMigrate to Bootstrap theme + CSS overrides\nFix navbar visibility\nConsolidate to SCSS-only approach\nRemove !important from CSS\n\n\n\n\n\nImplement brand.yml for design tokens\nCreate Quarto extension for Blueprint theme\nAdd custom template partials\nImplement custom listing layouts\n\n\n\n\n\nAdd service worker for offline\nImplement lazy loading for images\nOptimize build time (currently ~20s)\nAdd progressive enhancement features\n\n\n\n\n\n\n\n\nQuarto Websites\nHTML Themes\nHTML Theme Customization\nBrand Guide\nHTML Basics\n\n\n\n\n\nextras/BRAND_MEMORY.md - Complete brand guide\nextras/design-tokens.json - Design system reference\nextras/lumo theme/ - Example custom Quarto extension\nbackup/ - Original files before rebuild\n\n\n\n\n\nQuarto Discussions\nAwesome Quarto\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFile\nPurpose\nEdit Frequency\n\n\n\n\n_quarto.yml\nSite config, nav, theme\nOccasionally\n\n\ncustom.css\nStyle overrides\nOften (to migrate to SCSS)\n\n\nstyles.scss\nSCSS variables/rules\nRecommended approach\n\n\nincludes/schema.html\nSEO structured data\nRarely\n\n\nincludes/umami.html\nAnalytics script\nNever (unless changing provider)\n\n\nposts/_metadata.yml\nBlog post defaults\nRarely\n\n\nindex.qmd\nHomepage\nOccasionally\n\n\nvault/index.qmd\nBlog listing\nRarely\n\n\n\n\n\n\n---\ntitle: \"Page Title\"\npagetitle: \"Browser Tab Title\"  # Overrides title for &lt;title&gt; tag\npage-layout: full  # full, article, custom\ntoc: true  # Enable table of contents\ntoc-location: left  # left, right, body\ndescription: \"Meta description for SEO\"\nimage: \"path/to/social-share.jpg\"\ndraft: false  # Hide page if true\n---\n\nThis document should be referenced when making ANY changes to the site architecture, styling, or content structure."
  },
  {
    "objectID": "CLAUDE.html#project-overview",
    "href": "CLAUDE.html#project-overview",
    "title": "CLAUDE.md",
    "section": "",
    "text": "This is “The AI Architect” brand website for Jitin Kapila, built with Quarto. The site transforms from a portfolio into a premium coaching/consulting platform featuring:\n\nHomepage with hero + frameworks (AI-ART Matrix, OLCD Protocol)\nCourse sales page (AI Profit OS - ₹19,999/₹34,999)\nConsulting waitlist\nBlog (“The Vault”) with Strategy/Engineering tabs\nAbout page (authority building)\n\nLive URLs: - Primary: https://jitinkapila.com - Netlify: https://percieveit.com"
  },
  {
    "objectID": "CLAUDE.html#technology-stack",
    "href": "CLAUDE.html#technology-stack",
    "title": "CLAUDE.md",
    "section": "",
    "text": "Static Site Generator: Quarto (1.4+)\nStyling Approach: SCSS + CSS (hybrid system - see below)\nCurrent Theme: Brite (Bootstrap-based)\nAnalytics: Umami (includes/umami.html)\nSEO: Schema.org JSON-LD (includes/schema.html)\nComments: Utterances (GitHub-based, on blog posts)\nPublishing: Netlify"
  },
  {
    "objectID": "CLAUDE.html#architecture-decision-why-scss-css-hybrid",
    "href": "CLAUDE.html#architecture-decision-why-scss-css-hybrid",
    "title": "CLAUDE.md",
    "section": "",
    "text": "The project currently uses two styling systems which can cause conflicts:\n\ncustom.css - Direct CSS overrides (current implementation)\nstyles.scss - SCSS with Bootstrap variables (backup/original)\n\n\n\n\nBased on Quarto best practices and the Lumo theme example in extras/, here’s why SCSS is superior:\n\n\n\nNative Quarto Integration: Quarto processes SCSS at build time with Bootstrap\nVariable System: Can override Bootstrap variables ($primary, $font-family-sans-serif)\nTheme Inheritance: Extend themes properly instead of fighting them\nCompile-Time Safety: SCSS errors caught during build, not runtime\nSource Order Control: SCSS compiles BEFORE custom CSS in cascade\n\n\n\n\n\nOverride Battles: Must use !important everywhere to fight Bootstrap\nSpecificity Wars: Constantly battling theme defaults\nNo Variables: Can’t modify Bootstrap’s design tokens cleanly\nRuntime Conflicts: Themes like Cosmo/Brite have their own opinions\n\n\n\n\n\n/*-- scss:defaults --*/\n// Override Bootstrap variables BEFORE they compile\n@import url('https://fonts.googleapis.com/css2?family=Bebas+Neue&family=IBM+Plex+Sans:wght@400;500;600&display=swap');\n\n// Typography Variables\n$font-family-sans-serif: 'IBM Plex Sans', sans-serif;\n$headings-font-family: 'Bebas Neue', sans-serif;\n$headings-font-weight: 400;\n$headings-text-transform: uppercase;\n$headings-letter-spacing: 1px;\n\n// Color Variables\n$primary: #0C2B4E;      // Navy Deep\n$secondary: #213555;    // Navy Royal\n$body-color: #5A6A7A;\n$link-color: #0C2B4E;\n\n// Spacing\n$spacer: 1rem;\n\n// Border Radius (Blueprint aesthetic - no curves)\n$border-radius: 0;\n$border-radius-sm: 0;\n$border-radius-lg: 0;\n\n/*-- scss:rules --*/\n// Custom rules that extend the compiled theme\nbody {\n  background-image:\n    linear-gradient(rgba(33, 53, 85, 0.03) 1px, transparent 1px),\n    linear-gradient(90deg, rgba(33, 53, 85, 0.03) 1px, transparent 1px);\n  background-size: 20px 20px;\n}\n\nh1, h2, h3, h4, h5, h6 {\n  font-family: $headings-font-family;\n  text-transform: $headings-text-transform;\n  letter-spacing: $headings-letter-spacing;\n}\n\n\n\n\nVariables in /*-- scss:defaults --*/ override Bootstrap BEFORE compilation\nRules in /*-- scss:rules --*/ extend the theme cleanly\nNo !important needed - you’re modifying the source, not fighting it\nTheme changes are easy - just change theme name in _quarto.yml"
  },
  {
    "objectID": "CLAUDE.html#current-build-commands",
    "href": "CLAUDE.html#current-build-commands",
    "title": "CLAUDE.md",
    "section": "",
    "text": "# Preview with live reload\nquarto preview\n\n# Production build\nquarto render\n\n# Publish to Netlify\nquarto publish netlify\nNote: Pre-render hook cleans _site/* automatically (see _quarto.yml)"
  },
  {
    "objectID": "CLAUDE.html#site-structure",
    "href": "CLAUDE.html#site-structure",
    "title": "CLAUDE.md",
    "section": "",
    "text": "/\n├── index.qmd              # Homepage (Hero + AI-ART Matrix)\n├── about.qmd              # Authority building\n├── bootcamp.qmd           # AI Profit OS sales page\n├── consulting.qmd         # Consulting waitlist\n├── vault/\n│   ├── index.qmd          # Blog listing with tabs\n│   └── (links to ../posts/)\n├── posts/                 # Blog posts\n│   ├── _metadata.yml      # Default settings for all posts\n│   └── [post_name]/index.qmd\n├── includes/\n│   ├── umami.html         # Analytics\n│   └── schema.html        # SEO structured data\n├── extras/                # Reference materials\n│   ├── BRAND_MEMORY.md    # Complete brand guide\n│   ├── design-tokens.json # Design system tokens\n│   └── lumo theme/        # Example Quarto extension\n├── _quarto.yml            # Site configuration\n├── custom.css             # Current CSS (to migrate to SCSS)\n├── styles.scss            # Backup SCSS system\n└── backup/                # Original files backup"
  },
  {
    "objectID": "CLAUDE.html#design-system-the-blueprint",
    "href": "CLAUDE.html#design-system-the-blueprint",
    "title": "CLAUDE.md",
    "section": "",
    "text": "Brand Name: The AI Architect\nTagline: “Stop Guessing. Start Architecting.”\nVisual Motif: Engineering blueprint aesthetic\nCore Principle: Sharp corners, no gradients, geometric precision\n\n\n\n\n:root {\n  /* Primary */\n  --navy-deep: #0C2B4E;      /* Headers, buttons, primary text */\n  --navy-royal: #213555;     /* Hover states, secondary */\n  --slate-blue: #3E5879;     /* Accent text */\n\n  /* Accent */\n  --cream-gold: #D8C4B6;     /* Dividers, highlights */\n  --gold-hover: #C4B09E;     /* Gold hover state */\n\n  /* Backgrounds */\n  --bg-light: #FAFBFC;       /* Default background */\n  --bg-warm: #F8F6F3;        /* Alternate sections */\n  --bg-grid: rgba(33, 53, 85, 0.03);  /* Blueprint grid */\n\n  /* Text */\n  --text-primary: #0C2B4E;   /* Headlines */\n  --text-body: #5A6A7A;      /* Paragraphs */\n  --text-muted: #8896A6;     /* Meta info */\n}\n\n\n\n/* Headers: Bebas Neue - Bold, Uppercase, Tight */\nh1, h2, h3 {\n  font-family: 'Bebas Neue', sans-serif;\n  text-transform: uppercase;\n  letter-spacing: 1px;\n}\n\n/* Body: IBM Plex Sans - Clean, Professional */\nbody {\n  font-family: 'IBM Plex Sans', sans-serif;\n  font-weight: 400;\n  line-height: 1.7;\n}\n\n/* Responsive Sizing */\nh1 { font-size: clamp(48px, 8vw, 80px); line-height: 0.95; }\nh2 { font-size: clamp(32px, 5vw, 48px); line-height: 1.1; }\nh3 { font-size: clamp(24px, 3vw, 32px); line-height: 1.2; }\n\n/* Eyebrow Text */\n.eyebrow {\n  font-size: 11px;\n  font-weight: 600;\n  letter-spacing: 3px;\n  text-transform: uppercase;\n  color: var(--text-muted);\n}\n\n\n\nAll components follow border-radius: 0 (no rounded corners):\n\nButtons: Sharp, uppercase, slide-in hover effect\nCards: 1-2px border, subtle shadow on hover\nDividers: 60px horizontal lines in cream-gold\nGrid Background: 20px × 20px blueprint grid at 3% opacity"
  },
  {
    "objectID": "CLAUDE.html#configuration-guide",
    "href": "CLAUDE.html#configuration-guide",
    "title": "CLAUDE.md",
    "section": "",
    "text": "project:\n  type: website\n  output-dir: _site\n  pre-render: [\"rm -rf _site/*\"]  # Clean build\n  resources:\n    - CNAME\n    - favicon-32x32.png\n    - favicon.ico\n    - apple-touch-icon.png\n\nwebsite:\n  title: \"Jitin Kapila | The AI Architect\"\n  site-url: https://jitinkapila.com\n\n  navbar:\n    background: transparent  # Let CSS control this\n    left:\n      - text: \" \"  # Blank for logo/brand\n        href: index.qmd\n    right:\n      - text: \"The Protocol\"\n        href: bootcamp.qmd\n      # ... other nav items\n\n  page-footer:\n    left: \"© 2025 Jitin Kapila | The AI Architect\"\n    right:\n      - icon: linkedin\n        href: https://linkedin.com/in/jitinkapila\n      # ... other social icons\n\nformat:\n  html:\n    theme: brite  # Can also use: cosmo, flatly, litera\n    css: custom.css  # Or use SCSS with theme: [custom.scss]\n    include-in-header:\n      - includes/umami.html\n      - includes/schema.html\n    toc: false  # Disable by default, enable per-page\n    code-fold: true\n    highlight-style: github\n    smooth-scroll: true\n\nexecute:\n  freeze: auto  # Cache computational output\n  cache: true\n\ngoogle-analytics: \"G-XXXXXXXXXX\"  # Replace with actual ID\n\n\n\n\nTheme Selection: Use Bootstrap-based themes (cosmo, brite, flatly, litera)\nCSS Loading Order: Theme → CSS → Page-specific styles\nFreeze Strategy: freeze: auto prevents re-running code unnecessarily\nResources: Always include CNAME, favicons for deployment"
  },
  {
    "objectID": "CLAUDE.html#working-with-blog-posts",
    "href": "CLAUDE.html#working-with-blog-posts",
    "title": "CLAUDE.md",
    "section": "",
    "text": "---\ntitle: \"Post Title\"\ndescription: \"Brief description for listings\"\ndate: \"YYYY-MM-DD\"\ncategories: [strategy, code]  # For tab filtering\ntags: [ai, ml, enterprise]\nimage: \"img/feature.jpg\"\nimage-alt: \"Alt text\"\ndraft: false  # Set true to hide\nauthor: \"Jitin Kapila\"  # Inherited from posts/_metadata.yml\n---\n\n\n\nposts/_metadata.yml sets defaults for ALL posts:\nfreeze: true\nauthor: 'Jitin Kapila'\ntoc: true\ntoc-depth: 3\ntoc-location: left\npage-layout: article\nreference-location: margin  # Tufte-style margin notes\ncitation-location: margin\nlightbox: true\nlicense: \"CC BY\"\ncode-line-numbers: true\ncode-fold: true\ncomments:\n  utterances:\n    repo: jkapila/perceptions\n    theme: github-light\n\n\n\nThe Vault uses categories to separate posts:\n\nStrategy Tab: categories: [strategy, business, roi]\nEngineering Tab: categories: [code, llm, agents]\n\nNote: Posts can appear in multiple tabs if they have multiple categories."
  },
  {
    "objectID": "CLAUDE.html#advanced-quarto-features",
    "href": "CLAUDE.html#advanced-quarto-features",
    "title": "CLAUDE.md",
    "section": "",
    "text": "Quarto uses fenced divs for layout control:\n::: {.hero-section}\nContent here\n:::\n\n::: {.column-body-outset}\nWide content that extends beyond normal margins\n:::\n\n::: {.panel-tabset}\n## Tab 1\nContent\n## Tab 2\nMore content\n:::\n\n\n\n\n.column-body - Normal width (default)\n.column-body-outset - Slightly wider\n.column-page - Page width\n.column-screen - Full screen width\n.column-margin - Margin notes (Tufte style)\n\n\n\n\nFor deep customization, use template partials (see extras/lumo theme/):\nformat:\n  html:\n    template-partials:\n      - title-block.html  # Custom title block\n      - footer.html       # Custom footer"
  },
  {
    "objectID": "CLAUDE.html#brand.yml-implementation-recommended-future",
    "href": "CLAUDE.html#brand.yml-implementation-recommended-future",
    "title": "CLAUDE.md",
    "section": "",
    "text": "Quarto 1.4+ supports brand.yml for centralized design tokens. This is the FUTURE direction:\n\n\nmeta:\n  name: \"The AI Architect\"\n  link: \"https://jitinkapila.com\"\n\ncolor:\n  palette:\n    navy-deep: \"#0C2B4E\"\n    navy-royal: \"#213555\"\n    cream-gold: \"#D8C4B6\"\n  background: \"#FAFBFC\"\n  foreground: \"#0C2B4E\"\n  primary: \"#0C2B4E\"\n  secondary: \"#213555\"\n\ntypography:\n  fonts:\n    - family: \"Bebas Neue\"\n      source: google\n    - family: \"IBM Plex Sans\"\n      source: google\n      weight: [400, 500, 600]\n  base-size: \"16px\"\n  headings:\n    family: \"Bebas Neue\"\n    weight: 400\n  body:\n    family: \"IBM Plex Sans\"\n    weight: 400\n    line-height: 1.7\n\n\n\n\nSingle Source of Truth: All design tokens in one file\nCross-Format: Works across HTML, PDF, DOCX outputs\nTooling Integration: IDEs can autocomplete brand values\nTeam Collaboration: Designers can edit without touching code\n\nStatus: Not yet implemented. Consider for Phase 2 refactor."
  },
  {
    "objectID": "CLAUDE.html#seo-analytics",
    "href": "CLAUDE.html#seo-analytics",
    "title": "CLAUDE.md",
    "section": "",
    "text": "Located in includes/schema.html:\n&lt;script type=\"application/ld+json\"&gt;\n{\n  \"@context\": \"https://schema.org\",\n  \"@graph\": [\n    {\n      \"@type\": \"ProfessionalService\",\n      \"name\": \"Jitin Kapila - The AI Architect\",\n      \"founder\": {\n        \"@type\": \"Person\",\n        \"name\": \"Jitin Kapila\",\n        \"jobTitle\": \"AI Strategy Consultant\"\n      }\n    },\n    {\n      \"@type\": \"Course\",\n      \"name\": \"The AI Architect Protocol\",\n      \"provider\": { \"@type\": \"Person\", \"name\": \"Jitin Kapila\" }\n    }\n  ]\n}\n&lt;/script&gt;\n\n\n\nLocated in includes/umami.html - lightweight, privacy-focused alternative to Google Analytics.\n\n\n\nConfigured in _quarto.yml:\nwebsite:\n  open-graph: true\n  twitter-card:\n    creator: \"@jitinkapila\"\n    card-style: summary_large_image"
  },
  {
    "objectID": "CLAUDE.html#performance-optimizations",
    "href": "CLAUDE.html#performance-optimizations",
    "title": "CLAUDE.md",
    "section": "",
    "text": "execute:\n  freeze: auto  # Only re-run when source changes\n  cache: true   # Cache results\nWhen to Clear Cache:\nrm -rf _freeze/\nquarto render\n\n\n\n\nUse thumbnails for listings (-thumb.jpg suffix)\nCompress images before adding to repo\nEnable lightbox for blog images (lightbox: true)\n\n\n\n\n\nUse highlight-style: github for consistency\nEnable code-fold: true for long code blocks\nAdd code-line-numbers: true for readability"
  },
  {
    "objectID": "CLAUDE.html#common-tasks",
    "href": "CLAUDE.html#common-tasks",
    "title": "CLAUDE.md",
    "section": "",
    "text": "Edit _quarto.yml → website.navbar.right:\nnavbar:\n  right:\n    - text: \"New Page\"\n      href: newpage.qmd\n\n\n\nEdit _quarto.yml → page-footer.right:\npage-footer:\n  right:\n    - icon: youtube\n      href: https://youtube.com/@channel\n\n\n\n# Create file\ntouch newpage.qmd\n\n# Add frontmatter\n---\ntitle: \"Page Title\"\npage-layout: full  # or article, custom\n---\n\n\n\nEdit styles.scss → /*-- scss:defaults --*/:\n$primary: #0C2B4E;\n$secondary: #213555;\nThen in _quarto.yml:\nformat:\n  html:\n    theme: [cosmo, styles.scss]  # Theme + custom SCSS"
  },
  {
    "objectID": "CLAUDE.html#troubleshooting",
    "href": "CLAUDE.html#troubleshooting",
    "title": "CLAUDE.md",
    "section": "",
    "text": "Cause: Using theme: none removes Bootstrap navbar structure. Fix: Use a Bootstrap theme (cosmo, brite, flatly) + CSS overrides.\n\n\n\nCheck Order: 1. Is SCSS in /*-- scss:defaults --*/ section? 2. Is CSS loaded AFTER theme in _quarto.yml? 3. Are you using !important (symptom of wrong approach)?\nDebug:\nquarto preview --log-level debug\n\n\n\nSymptom: Old content showing despite changes. Fix:\nrm -rf _freeze/\nquarto render\n\n\n\nFix:\n# Kill all Quarto processes\npkill -f quarto\n\n# Restart preview\nquarto preview"
  },
  {
    "objectID": "CLAUDE.html#best-practices",
    "href": "CLAUDE.html#best-practices",
    "title": "CLAUDE.md",
    "section": "",
    "text": "Use SCSS variables instead of CSS custom properties for Bootstrap integration\nPrefer theme extension over complete override\nAvoid !important - it indicates fighting the cascade\nTest mobile-first - use responsive breakpoints\nKeep specificity low - let cascade work naturally\n\n\n\n\n\nUse divs for layout, not CSS classes in markdown\nEnable freeze for computational posts - save build time\nAdd alt text to all images - SEO + accessibility\nUse relative links for internal navigation\nTest locally before pushing - quarto preview\n\n\n\n\n\nOptimize images before committing (use WebP when possible)\nUse code-fold for long code blocks\nEnable caching for expensive computations\nMinimize custom fonts (currently 2 families - good)\nLazy load images in listings"
  },
  {
    "objectID": "CLAUDE.html#future-roadmap",
    "href": "CLAUDE.html#future-roadmap",
    "title": "CLAUDE.md",
    "section": "",
    "text": "Migrate to Bootstrap theme + CSS overrides\nFix navbar visibility\nConsolidate to SCSS-only approach\nRemove !important from CSS\n\n\n\n\n\nImplement brand.yml for design tokens\nCreate Quarto extension for Blueprint theme\nAdd custom template partials\nImplement custom listing layouts\n\n\n\n\n\nAdd service worker for offline\nImplement lazy loading for images\nOptimize build time (currently ~20s)\nAdd progressive enhancement features"
  },
  {
    "objectID": "CLAUDE.html#resources",
    "href": "CLAUDE.html#resources",
    "title": "CLAUDE.md",
    "section": "",
    "text": "Quarto Websites\nHTML Themes\nHTML Theme Customization\nBrand Guide\nHTML Basics\n\n\n\n\n\nextras/BRAND_MEMORY.md - Complete brand guide\nextras/design-tokens.json - Design system reference\nextras/lumo theme/ - Example custom Quarto extension\nbackup/ - Original files before rebuild\n\n\n\n\n\nQuarto Discussions\nAwesome Quarto"
  },
  {
    "objectID": "CLAUDE.html#quick-reference",
    "href": "CLAUDE.html#quick-reference",
    "title": "CLAUDE.md",
    "section": "",
    "text": "File\nPurpose\nEdit Frequency\n\n\n\n\n_quarto.yml\nSite config, nav, theme\nOccasionally\n\n\ncustom.css\nStyle overrides\nOften (to migrate to SCSS)\n\n\nstyles.scss\nSCSS variables/rules\nRecommended approach\n\n\nincludes/schema.html\nSEO structured data\nRarely\n\n\nincludes/umami.html\nAnalytics script\nNever (unless changing provider)\n\n\nposts/_metadata.yml\nBlog post defaults\nRarely\n\n\nindex.qmd\nHomepage\nOccasionally\n\n\nvault/index.qmd\nBlog listing\nRarely\n\n\n\n\n\n\n---\ntitle: \"Page Title\"\npagetitle: \"Browser Tab Title\"  # Overrides title for &lt;title&gt; tag\npage-layout: full  # full, article, custom\ntoc: true  # Enable table of contents\ntoc-location: left  # left, right, body\ndescription: \"Meta description for SEO\"\nimage: \"path/to/social-share.jpg\"\ndraft: false  # Hide page if true\n---\n\nThis document should be referenced when making ANY changes to the site architecture, styling, or content structure."
  },
  {
    "objectID": "posts/engineering/03_crosstab_sparsity/index.html",
    "href": "posts/engineering/03_crosstab_sparsity/index.html",
    "title": "CrossTab Sparsity",
    "section": "",
    "text": "Cluster analysis has always fascinated me as a window into the hidden structures of data. During my collaboration with Kumarjit Pathak, we grappled with a persistent challenge in unsupervised learning: how to objectively evaluate clustering quality across different algorithms. Traditional metrics like the Silhouette Index or Bayesian Information Criterion felt restrictive—they were siloed within specific methodologies, making cross-algorithm comparisons unreliable.\nThis frustration led us to develop a universal cluster evaluation metric, detailed in our paper “Cross Comparison of Results from Different Clustering Approaches”. Our goal was to create a framework that transcends algorithmic biases, enabling:\n- Direct comparison of K-Means vs GMM vs DBSCAN vs PAM vs SOM vs Anything results\n- Identification of variables muddying cluster separation\n- Automated determination of optimal cluster counts\nIn this blog, I’ll walk you through our journey—from conceptualization to real-world validation—and share insights that didn’t make it into the final paper."
  },
  {
    "objectID": "posts/engineering/03_crosstab_sparsity/index.html#our-aha-moment---crosstab-sparsity",
    "href": "posts/engineering/03_crosstab_sparsity/index.html#our-aha-moment---crosstab-sparsity",
    "title": "CrossTab Sparsity",
    "section": "Our “Aha!” Moment - Crosstab Sparsity",
    "text": "Our “Aha!” Moment - Crosstab Sparsity\n\n\n\n\nBest Cluster for K-means Using Crosstab sparsity\n\n\n\nWhile analyzing cross-tab matrices of variable distributions across clusters, we noticed a pattern: well-segregated clusters consistently showed higher frequencies along matrix diagonals. This inspired our two-part metric:\n\n\n\nSegregation Factor:\n# Simplified calculation from our codebase  \nmedian = np.median(cross_tab)  \nN_vk = np.sum(cross_tab &gt; median)  # Count \"well-segregated\" instances  \nExplanation Factor:\nexplanation = np.log(len(data) / (bins * clusters))  \n\n\nSegregation Factor: Measures how distinctly clusters separate data points. We used the median (not mean) to avoid skew from outlier-dominated matrices.\nExplanation Factor: Quantifies how well clusters capture data variability. The logarithmic term penalizes overfitting—a critical insight from debugging early over-segmented clusters.\n\nAnd the Final Formula:\nFor variable \\(v\\) with \\(k\\) clusters:\n\\[\nS_v^k = \\underbrace{\\frac{N_v^k}{\\max(l, k)}}_{\\text{Segregation}} \\times \\underbrace{\\ln\\left(\\frac{N_d}{l \\times k}\\right)}_{\\text{Explanation}}\n\\]\nwhere:\n- \\(N_v^k\\): Segregated instances (values above cross-tab matrix median)\n- \\(l\\): Number of value intervals for variable \\(v\\)\n- \\(N_d\\): Total observations\nThis formulation ensures algorithmic invariance, allowing comparison across methods like K-Means (distance-based) and GMM (probability-based). Also, now you can see from the formula two scenarios happens: 1. If each variable crosstab is too dense then their is no separation between classes 2. If each variable crosstab is too sparse then we loose on explanation.\nHence the curve reaches a maximum and then falls down giving use the separability that the cluster can produce:"
  },
  {
    "objectID": "posts/engineering/02_hypothesis_test/index.html",
    "href": "posts/engineering/02_hypothesis_test/index.html",
    "title": "A flow to Test Your Hypothesis in Python",
    "section": "",
    "text": "Hypothesis testing Photo by Tara Winstead\n\n\n\nOverview\nAll the practitioners of data science always hit one giant thing to do with data and you know it well its EDA -Exploratory Data Analysis. This word EDA1 was coined by Tukey himself in his seminal book published in 1983. But do you think that before that EDA doesn’t existed ?\n1 Emerson, J. D., & Hoaglin, D. C. (1983). Stem-and-leaf displays. In D. C. Hoaglin, F. Mosteller, & J. W. Tukey (Eds.) Understanding Robust and Exploratory Data Analysis, pp. 7–32. New York: Wiley. Book is here.Well glad you thought. Before that all were doing what is called as Hypothesis Testing. Yes, before this the race was majorly to fit the data and make most unbiased and robust estimate. But remember one thing when you talk about Hypothesis Testing it was always and majorly would be related to RCTs (Randomized Controlled Trials) a.k.a Randomized Clinical Trials and is Gold Standard of data.\n\n\n\n\n\n\nTipMore on RCTs and ODs\n\n\n\n\n\nNow let me now not hijack the discussion to what is RCTs and Observational Data (ODs) as it is more of Philosophical Reasoning rather than other quality of data, but essentially what we are trying to find is that can we by, using stats, identify interesting patterns in data.\nThe only thing happens wit RCT data is that we tend to believe these interesting patterns coincide with some sort of ‘Cause-Effect’ kind of relationship. But essentially due to bia nature of ODs, we certainly cant conclude this. And hence, can only find interesting patterns.\n\n\n\nLets move on. The big question is, for whatever reason you are doing HT , you are doing it for finding something intreating. And that something interesting is usually found by using Post-Hoc Tests. Now there are variety of Post-Hocs available but what is more know and hence easily found to be implemented in Tukey’s HSD.\nSo lets directly jump to how to follow this procedure. We’ll be using bioinfokit for this, as it is much simpler wrapper around whats implemented in statsmodels.\n\n\nWhat are the results\nPheww… Thats too much code right. But that would save a lot of your time in real life. So in real life you would write code as 3 steps below:\n\n\nCode\n# import libraries\nimport pandas as pd\n\n# Getting car data from UCI\ndf = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data',\n                 sep='\\s+',header=None,\n                 names=['mpg','cylinders','displacement','horsepower','weight',\n                 'acceleration','model_year','origin','car_name'])\ndf.head()\n\n# Syntax to do anove with validating the assumption, doing test and a post-hoc\nresults = do_anova_test(df=df, res_var='mpg',xfac_var='cylinders', \n                        anova_model='mpg ~ C(cylinders)+C(origin)+C(cylinders):C(origin)',\n                        ss_typ=3, result_full=True)\n\n\nResults form the do_anova_test\nLevens Test Result:\n                 Parameter    Value\n0      Test statistics (W)  14.5856\n1  Degrees of freedom (Df)   4.0000\n2                  p value   0.0000\n\nBartletts Test Result:\n                 Parameter    Value\n0      Test statistics (T)  61.2143\n1  Degrees of freedom (Df)   4.0000\n2                  p value   0.0000\n\nANOVA\\ANCOVA Test Result:\n                           df     sum_sq    mean_sq         F  PR(&gt;F)      n2\nIntercept                 1.0  6195.1701  6195.1701  296.3452  0.0000  0.2727\nC(cylinders)              4.0  7574.5864  1893.6466   90.5824  0.0000  0.3334\nC(origin)                 2.0   241.0703   120.5351    5.7658  0.0034  0.0106\nC(cylinders):C(origin)    8.0   577.4821    72.1853    3.4530  0.0046  0.0254\nResidual                389.0  8132.1404    20.9052       NaN     NaN     NaN\n\nTukey HSD Result:\n   group1  group2     Diff    Lower    Upper  q-value  p-value\n0       8       4  14.3237  12.8090  15.8383  36.6527   0.0010\n1       8       6   5.0226   3.1804   6.8648  10.5671   0.0010\n2       8       3   5.5869  -0.7990  11.9728   3.3909   0.1183\n3       8       5  12.4036   5.0643  19.7428   6.5503   0.0010\n4       4       6   9.3011   7.6765  10.9256  22.1910   0.0010\n5       4       3   8.7368   2.4102  15.0633   5.3524   0.0017\n6       4       5   1.9201  -5.3676   9.2078   1.0212   0.9000\n7       6       3   0.5643  -5.8486   6.9772   0.3410   0.9000\n8       6       5   7.3810   0.0182  14.7437   3.8854   0.0491\n9       3       5   6.8167  -2.7539  16.3873   2.7606   0.2919\nNice!!!\n\nAnd plotting is even easier\n\n\nCode\n# Numbers are clumsy for most. Making more interpretable plot on above results.\nplot_hsd(results.tukeyhsd.sort_values('Diff'), title=\"Tukey HSD resutls Anova of MPG ~ Cylinder\")\n\n\nResults form the plot_hsd\n\n\n\nTukey’s HSD comparison based on Anova Results\n\n\nPlots look good with ‘p-values’.\n\n\nConclusion\nNow since we applied the above to a Non RCT we cannot conclude that Difference in mpg based on cylinder is huge specially as number of cylinders goes up. But this statement might not be as explicit as might be appearing from plot. Unless you have a strong believe that the data follows with rules and assumptions of RCTs, we should be only seeking interesting as in associated results and not cause-effect results.\n\n\nGive me “The Code”\n\nPerforming AnovaPlotting Results\n\n\n\n\nAnova Test anova_test.py\nfrom bioinfokit import analys\n\nimport numpy as np\nfrom scipy import stats\n\nclass KeyResults:\n    \"\"\"\n    A basic class to hold all the results\n    \"\"\"\n    \n    def __init__(self,result_full):\n        self.keys = []\n        self.result_full = result_full\n    \n    def add_result(self,name,result):\n        if name == 'tukeyhsd':\n            self.keys.append(name)\n            setattr(self, name, result)\n        elif self.result_full:\n            self.keys.append(name)\n            setattr(self, name, result)\n\n\n# Anova test code\ndef do_anova_test(df, res_var, xfac_var, anova_model,ss_typ=3,\n                  effectsize='n2',result_full=False,add_res=False):\n    \"\"\"\n    Do all sequential anova tests\n    \n    Step 1) Leven's/ bartellet test for checking weather variance is homogenous or not\n    Step 2) Main ANOVA/ANCOVA test\n    Step 3) Tukey's HSD for individual combinations\n    \n    :param df: Pandas DataFrame holding all the columns\n    :param res_var: Variable for which we are checking ANOVA\n    :param xfac_var: Grouping Variables for which we want to do the comparisons\n    :param anova_model: SM formula for the model. This is life savour to make all things work\n    :param result_full: To provide all the results of intermediate steps\n    \n    \"\"\"\n\n    results = KeyResults(result_full)\n    \n    # initialize stat method\n    res = analys.stat()\n    \n    # doing levens test\n    res.levene(df=df, res_var=res_var,xfac_var=xfac_var)\n    print('\\nLeven\\'s Test Result:')\n    print(res.levene_summary)\n    results.add_result('levene',res.levene_summary)\n\n    # doing bartlett test\n    res.bartlett(df=df, res_var=res_var,xfac_var=xfac_var)\n    print('\\nBartlett\\'s Test Result:')\n    print(res.bartlett_summary)\n    results.add_result('bartlett',res.bartlett_summary)\n    \n    # doing anova / ancova\n    res.anova_stat(df=df, res_var=res_var, anova_model=anova_model,ss_typ=ss_typ)\n    aov_res = res.anova_summary\n    \n    # Add effect sizes\n    if effectsize == \"n2\":\n        all_effsize = (aov_res['sum_sq'] / aov_res['sum_sq'].sum()).to_numpy()\n        all_effsize[-1] = np.nan\n    else:\n        ss_resid = aov_res['sum_sq'].iloc[-1]\n        all_effsize = aov_res['sum_sq'].apply(lambda x: x / (x + ss_resid)).to_numpy()\n        all_effsize[-1] = np.nan\n    aov_res[effectsize] = all_effsize\n    #aov_res['bw_'] = res.anova_model_out.params.iloc[-1]\n    aov_res = aov_res.round(4)\n    \n    # printing results\n    print('\\nANOVA\\ANCOVA Test Result:')\n    print(aov_res)\n    results.add_result('anova',res.anova_summary.round(4))\n    results.add_result('anova_model',res.anova_model_out)\n    \n    # doing tukey's hsd top compare the groups\n    res.tukey_hsd(df=df, res_var=res_var,xfac_var=xfac_var, anova_model=anova_model,ss_typ=ss_typ)\n    print('\\nTukey HSD Result:')\n    print(res.tukey_summary.round(4))\n    results.add_result('tukeyhsd',res.tukey_summary.round(4))\n    \n    # add all result componets again if needed \n    if add_res:\n        results.add_result('allresult',res)\n    \n    return results\n\n\n\n\n\n\nPlotting results plot_hsd.py\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.style.use('seaborn-bright')\n\ndef plot_hsd(hsdres,p_cutoff=0.05,title=None,ax=None,figsize=(10,7)):\n     \"\"\"\n     Do plotting of tukeyhsd results\n    \n  \n    :param hsdres: 'tukeyhsd' result form the do_anova_test function\n    :param p_cutoff: Cutoff at which we get say a combination is significant\n    :param title: Title of the plot\n    :param ax: Define or get the matplotlib axes\n    :param figsize: Mention Figure size to draw\n    \n    \"\"\"\n\n    if ax is None:\n        fig,axp = plt.subplots(figsize=figsize)\n    else:\n        axp = ax\n    \n    # helper func\n    p_ind = lambda x : '' if x &gt; 0.1 else ('+' if x &gt; 0.05 else ('*' if x &gt; 0.01 else ('**' if x &gt;0.001 else '***')))\n    label_gen  = lambda x: f\"${x[0]} - {x[1]}\\ |\\ p:{x[2]:0.2f}{p_ind(x[2]):5s}$\"\n    \n    #setting values\n    mask = hsdres['p-value'] &lt;= p_cutoff\n    yticklabs = hsdres[['group1','group2','p-value']].apply(label_gen,axis=1).values\n    ys = np.arange(len(hsdres))\n    \n    # adding plot to axes\n    axp.errorbar(hsdres[~mask]['Diff'],ys[~mask],xerr=np.abs(hsdres[~mask][['Lower',\"Upper\"]]).values.T,\n                fmt='o', color='black', ecolor='lightgray', elinewidth=2, capsize=0)\n    axp.errorbar(hsdres[mask]['Diff'],ys[mask],xerr=np.abs(hsdres[mask][['Lower',\"Upper\"]]).values.T,\n                fmt='o', color='red', ecolor='pink', elinewidth=2, capsize=5)\n    axp.axvline(x=0,linestyle='--',c='skyblue')\n    axp.set_yticks([])\n    (l,u) = axp.get_xlim()\n    axp.set_xlim(l+1.5*l,u)\n    (l,u) = axp.get_xlim()\n    for idx,labs in enumerate(yticklabs):\n        axp.text(l-0.1*l,ys[idx],labs)\n    axp.set_yticklabels([])\n    \n    # finally doing what is needed\n    if ax is None:\n        plt.title('' if title is None else title,fontsize=14)\n        plt.show()\n    else:\n        return axp\n\n\n\n\n\nHope this give you kickstart to find you intresting patterns. Happy Learning!\n\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "posts/engineering/10_treeknn/index.html",
    "href": "posts/engineering/10_treeknn/index.html",
    "title": "TrieKNN: Unleashing KNN’s Power on Mixed Data Types",
    "section": "",
    "text": "Photo by Gelgas Airlangga"
  },
  {
    "objectID": "posts/engineering/10_treeknn/index.html#the-allure-and-limitation-of-knn",
    "href": "posts/engineering/10_treeknn/index.html#the-allure-and-limitation-of-knn",
    "title": "TrieKNN: Unleashing KNN’s Power on Mixed Data Types",
    "section": "The Allure and Limitation of KNN",
    "text": "The Allure and Limitation of KNN\nIn the realm of machine learning, the K-Nearest Neighbors (KNN) algorithm stands out for its intuitive nature and ease of implementation. Its principle is simple: classify a data point based on the majority class among its ‘k’ nearest neighbors in the feature space. This non-parametric approach makes no assumptions about the underlying data distribution, rendering it versatile for various applications. KNN is very popular, but it comes with some limitations.\nHowever, KNN’s Achilles’ heel lies in its reliance on distance metrics, which are inherently designed for numerical data. Real-world datasets often contain a mix of numerical and categorical features, posing a significant challenge for KNN. How do you measure the distance between ‘red’ and ‘blue,’ or ‘large’ and ‘small’?\n\nPrior Art\nSeveral strategies have been proposed to adapt KNN for mixed data:\n\nOne-Hot Encoding: Converts categorical features into numerical vectors, but can lead to high dimensionality.\nDistance Functions for Mixed Data: Develops and apply custom distance metrics that can handle both numerical and categorical features such as HEOM and many others.\nUsing mean/mode values: Replace the missing values with mean/mode.\n\nThese methods often involve compromises, either distorting the data’s inherent structure or adding computational overhead."
  },
  {
    "objectID": "posts/engineering/10_treeknn/index.html#enter-trieknn-a-novel-approach",
    "href": "posts/engineering/10_treeknn/index.html#enter-trieknn-a-novel-approach",
    "title": "TrieKNN: Unleashing KNN’s Power on Mixed Data Types",
    "section": "Enter TrieKNN: A Novel Approach",
    "text": "Enter TrieKNN: A Novel Approach\nWhat if we could cleverly sidestep the distance calculation problem for categorical features, while still leveraging KNN’s power? TrieKNN offers just that—a way to perform KNN on any mixed data!\nTrieKNN combines the strengths of Trie data structures and KNN to handle mixed data types gracefully. Here’s the core idea:\n\nTrie-Based Categorical Encoding: A Trie is used to store the categorical features of the data. Each node in the Trie represents a category.\nLeaf-Node KNN Models: At the leaf nodes of the Trie, where specific combinations of categorical features are found, we fit individual KNN models using only the numerical features.\nWeighted Prediction: To classify a new data point, we traverse the Trie based on its categorical features. At each level, we calculate a weighted distance based on available data, ending in a probability score in each leaf node.\n\n\nWhy This Works\n\nNo Direct Distance Calculation for Categorical Features: The Trie structure implicitly captures the relationships between categorical values.\nLocalized KNN Models: By fitting KNN models at the leaf nodes, we ensure that distance calculations are performed only on relevant numerical features.\nScalability: The Trie structure efficiently handles a large number of categorical features and values."
  },
  {
    "objectID": "posts/engineering/10_treeknn/index.html#building-a-trieknn-model",
    "href": "posts/engineering/10_treeknn/index.html#building-a-trieknn-model",
    "title": "TrieKNN: Unleashing KNN’s Power on Mixed Data Types",
    "section": "Building a TrieKNN Model",
    "text": "Building a TrieKNN Model\nLet’s dive into the implementation. We’ll start with the TrieNode and Trie classes, then move on to the KNN model and the training/prediction process.\n\nTrie Implementation\n\n\nCode\nimport numpy as np\nfrom collections import Counter\n\nclass TrieNode:\n    def __init__(self):\n        self.children = {}  # Dictionary to store child nodes\n        self.is_end_of_word = False  # True if the node is the end of a word\n        self.count = 0  # Count of how many times a word has been inserted\n        self.class_counts = {}  # Class counts\n        self.class_weights = {}\n        self.model = None  # Model at leaf nodes\n        self.indexes = []  # Store data indexes belonging to this leaf\n        self.labels = []  # Store data indexes belonging to this leaf\n        self.node_weight = None\n\nclass Trie:\n    def __init__(self):\n        self.root = TrieNode()  # Root node of the Trie\n        self.data_index = 0  # Initialize data index\n\n    def insert(self, word_val, model):\n        current_node = self.root\n        word, val = word_val\n        current_node.count += 1\n\n        # Adding class counts\n        if val not in current_node.class_counts:\n            current_node.class_counts[val] = 0\n        current_node.class_counts[val] += 1\n\n        for char in word:\n            # If the character is not in children, add a new TrieNode\n            if char not in current_node.children:\n                current_node.children[char] = TrieNode()\n            current_node = current_node.children[char]\n\n            # Adding count of instances\n            current_node.count += 1\n\n            # adding class counts\n            if val not in current_node.class_counts:\n                current_node.class_counts[val] = 0\n            current_node.class_counts[val] += 1\n\n        # Mark the end of the word and increment count\n        current_node.is_end_of_word = True\n        current_node.indexes.append(self.data_index)  # Store the data index\n        current_node.labels.append(val)\n        current_node.model = model\n        self.data_index += 1  # Increment data index\n\n    def search(self, word):\n        current_node = self.root\n        for char in word:\n            # If the character doesn't exist in the children, the word doesn't exist\n            if char not in current_node.children:\n                return False\n            current_node = current_node.children[char]\n\n        # Return True if it's the end of a word and the word exists\n        return current_node.is_end_of_word\n\n    def count_word(self, word):\n        current_node = self.root\n        for char in word:\n            # If the character doesn't exist, the word doesn't exist\n            if char not in current_node.children:\n                return 0, current_node.class_counts  # Correctly return class_counts\n            current_node = current_node.children[char]\n\n        # Return the count of the word\n        return current_node.count, current_node.class_counts\n\n    def display(self):\n        # Recursively display the tree\n        def _display(node, word):\n            if node.is_end_of_word:\n                print(f\"Data: {word}, Count: {node.count}, Indexes: {len(node.indexes)} Classes :{node.class_counts} weights:{len(node.class_weights)}\")  # Display indexes too\n            for char, child in node.children.items():\n                _display(child, word + char)  # corrected the display\n\n        _display(self.root, \"\")\n\n    def apply(self, func):\n        \"\"\"\n        Applies a function to all models in the leaf nodes.\n        \"\"\"\n        def _apply(node):\n            if node.is_end_of_word and node.model is not None:\n                func(node)\n            for child in node.children.values():\n                _apply(child)\n\n        _apply(self.root)\n\n    def apply_weight_to_indexes(self, weight):\n        \"\"\"\n        Applies a weight to the indexes based on the percentage of data available.\n        \"\"\"\n        def _apply_weight_to_indexes(node):\n            if node.is_end_of_word:\n                total_count = sum(self.root.children[child].count for child in self.root.children)\n                percentage = node.count / total_count if total_count &gt; 0 else 0\n                weighted_indexes = [(index, weight * percentage) for index in node.indexes]\n                node.class_weights = weighted_indexes  # Corrected this line\n            for child in node.children.values():\n                _apply_weight_to_indexes(child)\n\n        _apply_weight_to_indexes(self.root)\n\n\n\n\nKNN Model\n\n\nCode\nclass KNNModel:\n    def __init__(self, k=5):\n        self.data = None\n        self.labels = []\n        self.k = k\n\n    def fit(self, data, indexes, labels):\n        # print(\"Fitting model with indexes:\", len(indexes), \"labels:\", len(labels))\n        self.data = data[indexes].astype(float)\n        self.labels = np.array(labels).astype(float)\n\n    def predict(self, data):\n        # print(\"Predicting with data:\", data)\n        dist_ind = np.sqrt(np.sum((self.data - data) ** 2, axis=1) ** 2)  # euclidean distance\n        main_arr = np.column_stack((self.labels, dist_ind))  # labels with distance\n        main = main_arr[main_arr[:, 1].argsort()]  # sorting based on distance\n        count = Counter(main[0:self.k, 0])  # counting labels\n        sums = np.array(list(count.values()))  # getting counts\n        return sums / np.sum(sums)  # prediction as probability\n\n\n\n\nTraining and Evaluation\nHere’s how we train and evaluate the TrieKNN model:\n\n\nCode\n# Sample data\nn = 10000\ndata = np.array((np.random.choice(['Anything ', 'By ','Chance '], p=[0.6,0.1,0.3],size=n),\n                 np.random.choice(['can', 'go', 'here','lets', 'see', \"it\"], p=[0.1, 0.1, 0.1, 0.2, 0.4, 0.1], size=n),\n                 np.random.normal(3, 1, size=n),\n                 np.random.normal(5, 2, size=n))).T\ny_label = np.random.choice([0,1], p=[0.7, 0.3], size=n)\n\n# Trie training\ntrie = Trie()\nfor X, y in zip(data, y_label):\n    trie.insert((X[:2], y),None)\n\n# Apply weights to indexes\ntrie.apply_weight_to_indexes(0.5)\n\n# Fit models of leaf nodes\ndef add_model(node, data):\n    node.model = KNNModel()\n    node.model.fit(data, node.indexes, node.labels)\n\ndef traverse_and_add_model(node, data):\n    if node.is_end_of_word:\n        add_model(node, data)  # Add model to leaf node\n    for child in node.children.values():\n        traverse_and_add_model(child, data)\n\ntraverse_and_add_model(trie.root, data[:, 2:])\n\n\n\n\nExplanation\n\nWe create sample data with mixed categorical and numerical features.\nWe insert each data point into the Trie, using the categorical features as the path.\nAfter the Trie is built, we traverse it and fit a KNN model to the data points stored at each leaf node.\nFinally, we can predict the class of new data points by traversing the Trie and using the KNN model at the corresponding leaf node."
  },
  {
    "objectID": "posts/engineering/10_treeknn/index.html#results-and-discussion",
    "href": "posts/engineering/10_treeknn/index.html#results-and-discussion",
    "title": "TrieKNN: Unleashing KNN’s Power on Mixed Data Types",
    "section": "Results and Discussion",
    "text": "Results and Discussion\nLet us display the trie.\n\n\n\n\nCode\ntrie.display()\n\n\nData: Anything see, Count: 2429, Indexes: 2429 Classes :{np.int64(0): 1690, np.int64(1): 739} weights:2429\nData: Anything here, Count: 576, Indexes: 576 Classes :{np.int64(0): 407, np.int64(1): 169} weights:576\nData: Anything lets, Count: 1166, Indexes: 1166 Classes :{np.int64(1): 335, np.int64(0): 831} weights:1166\nData: Anything go, Count: 599, Indexes: 599 Classes :{np.int64(1): 172, np.int64(0): 427} weights:599\nData: Anything can, Count: 623, Indexes: 623 Classes :{np.int64(0): 439, np.int64(1): 184} weights:623\nData: Anything it, Count: 614, Indexes: 614 Classes :{np.int64(0): 432, np.int64(1): 182} weights:614\nData: Chance can, Count: 300, Indexes: 300 Classes :{np.int64(1): 84, np.int64(0): 216} weights:300\nData: Chance see, Count: 1247, Indexes: 1247 Classes :{np.int64(1): 403, np.int64(0): 844} weights:1247\nData: Chance it, Count: 295, Indexes: 295 Classes :{np.int64(1): 101, np.int64(0): 194} weights:295\nData: Chance lets, Count: 602, Indexes: 602 Classes :{np.int64(1): 182, np.int64(0): 420} weights:602\nData: Chance here, Count: 301, Indexes: 301 Classes :{np.int64(0): 209, np.int64(1): 92} weights:301\nData: Chance go, Count: 314, Indexes: 314 Classes :{np.int64(0): 228, np.int64(1): 86} weights:314\nData: By lets, Count: 177, Indexes: 177 Classes :{np.int64(0): 120, np.int64(1): 57} weights:177\nData: By see, Count: 386, Indexes: 386 Classes :{np.int64(0): 279, np.int64(1): 107} weights:386\nData: By here, Count: 91, Indexes: 91 Classes :{np.int64(0): 74, np.int64(1): 17} weights:91\nData: By go, Count: 83, Indexes: 83 Classes :{np.int64(0): 54, np.int64(1): 29} weights:83\nData: By can, Count: 102, Indexes: 102 Classes :{np.int64(0): 68, np.int64(1): 34} weights:102\nData: By it, Count: 95, Indexes: 95 Classes :{np.int64(1): 24, np.int64(0): 71} weights:95\n\n\nThe model predicted the following values:\n\n\n\n\nCode\n# Prediction example\ndef predict_with_model(node):\n    predictions = node.model.predict(np.array([2,5]))\n    print(\"Predictions:\", predictions)\n\ntrie.apply(predict_with_model)\n\n\nPredictions: [0.6 0.4]\nPredictions: [0.4 0.6]\nPredictions: [0.4 0.6]\nPredictions: [0.6 0.4]\nPredictions: [1.]\nPredictions: [0.8 0.2]\nPredictions: [0.8 0.2]\nPredictions: [1.]\nPredictions: [0.4 0.6]\nPredictions: [0.8 0.2]\nPredictions: [0.8 0.2]\nPredictions: [0.4 0.6]\nPredictions: [0.6 0.4]\nPredictions: [0.8 0.2]\nPredictions: [0.8 0.2]\nPredictions: [0.6 0.4]\nPredictions: [0.8 0.2]\nPredictions: [0.2 0.8]\n\n\nThe predictions will vary on each run. From this we can see that we can use KNN on mixed data types."
  },
  {
    "objectID": "posts/engineering/10_treeknn/index.html#conclusion-a-promising-path-forward",
    "href": "posts/engineering/10_treeknn/index.html#conclusion-a-promising-path-forward",
    "title": "TrieKNN: Unleashing KNN’s Power on Mixed Data Types",
    "section": "Conclusion: A Promising Path Forward",
    "text": "Conclusion: A Promising Path Forward\nTrieKNN presents a compelling solution for extending the applicability of KNN to mixed data types. By leveraging the Trie data structure, it avoids direct distance calculations on categorical features, enabling the use of localized KNN models for numerical data.\nFurther research could explore:\n\nOptimizing the weighting scheme for combining predictions from different Trie levels.\nComparing TrieKNN’s performance against other mixed-data KNN approaches on benchmark datasets.\nExtending TrieKNN to handle missing data and noisy categorical features.\n\nTrieKNN opens up new possibilities for applying KNN in domains where mixed data types are prevalent, such as healthcare, e-commerce, and social science.\nResources and further reads:\n1. Nomclust R package\n2. An Improved kNN Based on Class Contribution and Feature Weighting\n3. An Improved Weighted KNN Algorithm for Imbalanced Data Classification\n4. A weighting approach for KNN classifier\n5. Unsupervised Outlier Detection for Mixed-Valued Dataset Based on the Adaptive k-Nearest Neighbor Global Network\n6. A hybrid approach based on k-nearest neighbors and decision tree for software fault prediction\n7. Analysis of Decision Tree and K-Nearest Neighbor Algorithm in the Classification of Breast Cancer"
  },
  {
    "objectID": "vault.html",
    "href": "vault.html",
    "title": "The Vault",
    "section": "",
    "text": "Knowledge Base\n\nThe Vault\nFrameworks, code, and strategy for AI Architects.\n\n\n\nStrategyEngineering\n\n\nFor the C-Suite. ROI-focused frameworks and business transformation insights.\n\n\n\n\n\n\n\nDecision-First AI: Why Data Should Follow, Not Lead\n\n\n\nstrategy\n\nroi\n\nbusiness\n\nai\n\n\n\nLet’s stop the habit of blaming data and AI and start with what decisions we want !!\n\n\n\nOct 9, 2025\n\n\n\n\n\n\nNo matching items\n\n\n\nFor the builders. Code, implementation patterns, and technical deep-dives.\n\n\n\n\n\n\n\nTrieKNN: Unleashing KNN’s Power on Mixed Data Types\n\n\n\nknn\n\nml\n\nmixed-data\n\n\n\nDiscover TrieKNN, a novel approach to extend the K-Nearest Neighbors algorithm to datasets with both categorical and numerical features. Learn how it works, see it in…\n\n\n\nFeb 26, 2025\n\n\n\n\n\n\n\n\n\n\nCrossTab Sparsity for Classification\n\n\n\nclassification\n\nmetric\n\nfeature selection\n\n\n\nCan our metric help us in making a classification problem work better ?\n\n\n\nJan 3, 2023\n\n\n\n\n\n\n\n\n\n\nCrossTab Sparsity\n\n\n\nclustering\n\nanalysis\n\n\n\nA label and data type agnostic metric for evaluating clustering performance!\n\n\n\nMay 3, 2022\n\n\n\n\n\n\n\n\n\n\nA flow to Test Your Hypothesis in Python\n\n\n\neda\n\nhypothesis\n\nanalysis\n\npython\n\n\n\nMaking life easy to do some serious hypothesis testing in python.\n\n\n\nAug 10, 2021\n\n\n\n\n\n\n\n\n\n\nAdaptive Regression\n\n\n\nstrategy\n\n\n\nWe recently put through our observation on Regression Problem in our research. This post is a nonformal attempt to explain it.\n\n\n\nMay 1, 2018\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/strategy/decision-first-ai/index.html",
    "href": "posts/strategy/decision-first-ai/index.html",
    "title": "Decision-First AI: Why Data Should Follow, Not Lead",
    "section": "",
    "text": "Start here: don’t open a notebook until you know the decision you want to change.\nSounds obvious. But most AI projects don’t start there. They start with a dataset, or with a “let’s try this model,” or with a platform demo that looks great in the cloud. And then weeks later the obvious question appears: “Okay — what decision does this support?” People shrug. The project stalls. The models are good. The business impact is vague.\nThis is the dataset-first trap. It wastes time, money, and faith. It also gives AI a bad name.\nI’ve seen the opposite work — a lot. Start with the decision. Map the decision. Then pick the simplest data and model that make the decision better. The result? Faster pilots, clearer ROI, and systems that actually get used.\nThat approach is not just a management neat-idea. There’s a real body of research showing that aligning models to downstream decision goals yields better decisions than optimizing prediction accuracy alone. And there are real, practical wins — from telecom fault detection to inventory systems — when you flip the order. arXiv+1"
  },
  {
    "objectID": "posts/strategy/decision-first-ai/index.html#the-dataset-first-trap-what-it-looks-like-and-why-it-hurts",
    "href": "posts/strategy/decision-first-ai/index.html#the-dataset-first-trap-what-it-looks-like-and-why-it-hurts",
    "title": "Decision-First AI: Why Data Should Follow, Not Lead",
    "section": "The “dataset-first” trap — what it looks like, and why it hurts",
    "text": "The “dataset-first” trap — what it looks like, and why it hurts\nHere’s the typical playbook I see in companies:\n\nSomeone discovers a new dataset.\nThey build dashboards, then a model, then a fine model, then a fancier model.\nThey show a demo. The demo gets applause. Then the work hits integration, governance, and the messy reality of people who must make decisions every day. The model’s outputs don’t map to a decision process. So adoption fails.\n\nWhy? Because the project optimized the wrong thing. It optimized prediction metrics — error, F1, AUC, MAPE. And those are useful. But they’re not the measure of business impact. A model with better accuracy can still be useless if it doesn’t change what someone does.\nHarvard Business Review captured this idea well: decisions don’t start with data. They start with a problem, a role, a process, and a behavior. If your analytics don’t connect to that reality, you get slides and disappointment. Harvard Business Review\nThere are more subtle costs too. Dataset-first projects often create models that are brittle in production: they overfit to historical quirks, they require constant data wrangling, and they produce numbers no one trusts. That kills scale."
  },
  {
    "objectID": "posts/strategy/decision-first-ai/index.html#decision-first-in-research-not-new-but-finally-practical",
    "href": "posts/strategy/decision-first-ai/index.html#decision-first-in-research-not-new-but-finally-practical",
    "title": "Decision-First AI: Why Data Should Follow, Not Lead",
    "section": "Decision-first in research: not new, but finally practical",
    "text": "Decision-first in research: not new, but finally practical\nThere’s academic grounding for starting with decisions. In the machine-learning community this shows up as “decision-focused learning” or “smart predict-then-optimize.” The idea: train predictive models not for pure accuracy, but to minimize the loss that matters to the downstream optimization or decision task. When you optimize directly for the decision loss, you often get better business outcomes — even with “worse” prediction metrics. arXiv+1\nRecent papers and reviews show both the theory and practical methods: surrogate losses that reflect decision outcomes, techniques to differentiate through optimization, and heuristics for discrete problems. The takeaway: the math supports the intuition. If you want a model to help choose inventory levels, price points, or routing, train it with that decision in mind — not just with RMSE. Optimization Online+1\nThat doesn’t mean every model must be complex. Often the opposite. Framing the decision reduces model complexity because you only model what matters. This is your Occam’s Razor"
  },
  {
    "objectID": "posts/strategy/decision-first-ai/index.html#the-decision-map-simple-framework-you-can-use-today",
    "href": "posts/strategy/decision-first-ai/index.html#the-decision-map-simple-framework-you-can-use-today",
    "title": "Decision-First AI: Why Data Should Follow, Not Lead",
    "section": "The Decision Map: simple framework you can use today",
    "text": "The Decision Map: simple framework you can use today\nIf you want to flip to decision-first, start with a small, disciplined tool I call a Decision Map. It’s a one-page artefact. Build it before you touch data.\nHere’s the Decision Map — six steps. Do them in order.\n\nName the decision. Who decides, how often, and what options do they choose? Example: “Field-ops decides whether to dispatch a technician to a suspected DSL fault.” Be specific. Frequency matters — hourly, daily, weekly change what you can do.\nDefine the decision metric(s).What counts as success? Lower cost? Faster response time? Increase in net revenue? Pick one primary metric and one secondary. If you can’t name it in a single measurable sentence, you don’t have a decision.\nMap the current process. Where is the decision made today? Which people and tools are involved? Where does data enter? Where do delays happen? This step exposes the friction you must remove.\nIdentify the minimal action the model must trigger. The model doesn’t need to be perfect. It needs to change behavior. If the model’s output is a probability, what threshold triggers action? Who gets the alert? What’s the handoff?\nList the minimal signals (data) needed. Only include data that directly reduces uncertainty for the decision. You’ll be surprised how small this list often is. Think: signal → action. Not “all the data.”\nPlan the feedback loop. How will you measure the decision metric after deployment? How will you collect labels and iterate? Decide that upfront.\n\nIf you complete this map, you’ll have done 80% of the work most teams skip. It forces alignment, and it reveals whether the project is worth doing."
  },
  {
    "objectID": "posts/strategy/decision-first-ai/index.html#a-telecom-example-mapped-end-to-end-real-story",
    "href": "posts/strategy/decision-first-ai/index.html#a-telecom-example-mapped-end-to-end-real-story",
    "title": "Decision-First AI: Why Data Should Follow, Not Lead",
    "section": "A telecom example, mapped end-to-end (real story)",
    "text": "A telecom example, mapped end-to-end (real story)\nA brief, real example matters. I built a real-time anomaly detection system for a European telecom client early in my career. The project didn’t begin with “we have logs.” It began with this decision map:\n\nDecision: Should operations dispatch a field technician proactively for a suspected DSL fault?\nMetric: Reduce customer reported faults and improve Net Promoter Score (NPS) by reducing time-to-detect. Also: monthly cost savings from fewer reactive truck rolls and more planned truck rolls.\nProcess: Operations received the customer call, created a ticket, and dispatched if needed — often hours or days later. That was slow and expensive.\n\nAction: If the system flags an anomaly with high confidence\nRed : Very high probability in next 48 hours,\nAmber: probability in next 2-15 days\nGreen: No visibility of error in next 15 days This creates a high-priority ticket and dispatch a remote check or technician and was planned in region wise manner.\n\nSignals: DSL line metrics, error rates, device telemetry, event logs — a handful of streams, not every log.\nFeedback: Compare flagged incidents to customer complaints and adjust thresholds.\n\nBecause the decision was so clear, we could measure value before full scale. The pilot cut detection time from days to hours, raised customer satisfaction significantly ( We sent messages to possible signal disruption early), and saved ~£80K per month ( because reducing complete breakdown to DSL by heat or so and by planning the route than going everywhere) . It wasn’t an exotic model (or may be it was, tech details some other day); it was a tightly scoped system that informed a clear action.\nNotice how this maps to the Decision-First steps. The model existed to change a single operational choice. That focus made deployment possible and measurable fast."
  },
  {
    "objectID": "posts/strategy/decision-first-ai/index.html#another-quick-example-inventory-decisions",
    "href": "posts/strategy/decision-first-ai/index.html#another-quick-example-inventory-decisions",
    "title": "Decision-First AI: Why Data Should Follow, Not Lead",
    "section": "Another quick example: inventory decisions",
    "text": "Another quick example: inventory decisions\nInventory forecasting is a classic area where decision-first matters. You can chase lower MAPE and never change stocking policy. Or you can ask: what decision do merchandisers make with this forecast? When you frame it as “which SKUs do we order for next week, and what reorder points trigger expedited shipments,” you design the forecast differently: shorter horizons, bias for understock on fast movers, and direct constraints on reorder costs.\nI’ve led projects that delivered $11M in inventory optimization by building forecasts and decision rules that match merchant behavior and supply constraints. The trick was not better models — it was framing forecasts so the merchandisers could act with confidence."
  },
  {
    "objectID": "posts/strategy/decision-first-ai/index.html#practical-tips-for-teams-do-this-in-week-one",
    "href": "posts/strategy/decision-first-ai/index.html#practical-tips-for-teams-do-this-in-week-one",
    "title": "Decision-First AI: Why Data Should Follow, Not Lead",
    "section": "Practical tips for teams (do this in week one)",
    "text": "Practical tips for teams (do this in week one)\n\nRun a one-hour Decision Map workshop. Invite the decision owner, one operator, one engineer, and one product owner. Build the one-page map. If the owner can’t commit to a metric, pause the project.\nStart with a simple rule baseline. Before modeling, define a rule that will be your baseline (e.g., “if X &gt; T, create ticket”). If the model can’t beat that rule in decision impact, scrap it.\nMeasure decision impact, not model accuracy. Your dashboard should show business metric delta — not just RMSE. If you show the board a change in cost or conversion, you’ll get attention.\nPrioritize deployment constraints. Decide telemetry, latency, and handoff requirements first. Models that can’t meet latency or trust constraints are useless no matter how accurate.\nIterate with real feedback. Don’t wait for “perfect.” Ship an MVP that can be measured, then refine. Real decisions provide labels and operational learning."
  },
  {
    "objectID": "posts/strategy/decision-first-ai/index.html#common-objections-and-how-to-handle-them",
    "href": "posts/strategy/decision-first-ai/index.html#common-objections-and-how-to-handle-them",
    "title": "Decision-First AI: Why Data Should Follow, Not Lead",
    "section": "Common objections and how to handle them",
    "text": "Common objections and how to handle them\n“But we don’t have a clear decision owner.” Then don’t build a model. Decisions live in roles. Pull the right owner in early, or you’ll build for nobody.\n“Our data is messy.” Fine. If you can define the minimal signals, you can often create a proxy or start with manual labels. Messy data is easier to handle when you only need a few signals for a specific decision.\n“We need predictions for many uses.” Build a simple decision-first pilot first. Use its success to fund broader platform work. Pilots create proof that unlocks investment.\n“Decision-focused methods are academic — too hard.” There’s truth and myth here. The academic techniques show big wins when decision loss can be written down. But you don’t need complicated differentiable optimization to start. Use a decision map, simple thresholds, A/B tests, and iterative measurement. Graduate to decision-focused training once you have a stable objective. The research just tells us — unsurprisingly — that when you train with the decision in mind, outcomes improve. Optimization Online+1"
  },
  {
    "objectID": "posts/strategy/decision-first-ai/index.html#one-page-checklist-copy-this",
    "href": "posts/strategy/decision-first-ai/index.html#one-page-checklist-copy-this",
    "title": "Decision-First AI: Why Data Should Follow, Not Lead",
    "section": "One-page checklist (copy this)",
    "text": "One-page checklist (copy this)\n\nDecision name (The GOTO problem statement): ____________________\nPrimary / Secondary metric (which can accounted for ROI calculation later ): _____\nDecision owner (I don’t want to debate on this): ________________________\nFrequency of predictions: real-time / hourly / daily / weekly / monthly\nAction / interventions which can be triggered by model: ____________________\nMinimal signals required( The core Data to begin with): ______________________\nBaseline rule (your fail-safe if everything goes wrong): ____________________\nFeedback source ( might be tricky, but you should have one): __________________\n\nIf you can fill this in, you’re set for a pilot."
  },
  {
    "objectID": "posts/strategy/decision-first-ai/index.html#final-note-start-small-measure-fast-then-scale-with-discipline",
    "href": "posts/strategy/decision-first-ai/index.html#final-note-start-small-measure-fast-then-scale-with-discipline",
    "title": "Decision-First AI: Why Data Should Follow, Not Lead",
    "section": "Final note — start small, measure fast, then scale with discipline",
    "text": "Final note — start small, measure fast, then scale with discipline\nThe decision-first approach is simple because business problems are simple when stated well. The hard part is discipline: saying no to shiny demos and yes to measurable change. Start with one decision that matters. Map it. Ship a small system that changes behavior. Measure the business metric. Iterate.\nResearch supports this: models trained with the decision in mind perform better on the actual outcomes you care about. And in practice, teams that flip the order — decision first, data second — get to value faster. arXiv+1\nIf you want help mapping a decision in your company, send me one line describing the decision and the current process. I’ll reply with the Decision Map template you can use in a one-hour workshop. Or if you want we can connect too.\nIf this post help you or think help someone in need, please do share it. Thanks!!!\n##Key sources & further reading Elmachtoub, A.N., & Grigas, P. — Smart “Predict, then Optimize” (foundational paper on decision-focused loss and SPO). arXiv Reviews and recent work on decision-focused learning (predict-and-optimize / decision-focused methods). arXiv+1 Harvard Business Review — Decisions Don’t Start with Data — on why framing the decision matters. Harvard Business Review (And — the telecom and inventory examples referenced above are from projects on my profile/resume: Ask, if you want more details !! )"
  },
  {
    "objectID": "posts/engineering/01_adaptive_regression/index.html",
    "href": "posts/engineering/01_adaptive_regression/index.html",
    "title": "Adaptive Regression",
    "section": "",
    "text": "Adapting path through mountains! Photo by Zülfü Demir📸"
  },
  {
    "objectID": "posts/engineering/01_adaptive_regression/index.html#introduction",
    "href": "posts/engineering/01_adaptive_regression/index.html#introduction",
    "title": "Adaptive Regression",
    "section": "Introduction",
    "text": "Introduction\nHere I am trying to express our logic to find such Observation. Lets dive in.\nThere are different value estimation technique like regression analysis and time-series analysis. Everyone of us has experimented on regression using OLS ,MLE, Ridge, LASSO, Robust etc., and also might have evaluated them using RMSE (Root Mean/Median Square Error), MAD (Mean/Median Absolute Deviation), MAE (Mean / Median Absolute Error) and MAPE (Mean/Median Absolute Percentage Error), etc…\nBut all of these gives a single point estimate that what is the overall error looks like. Just a different thought!! can we be sure that this single value of MAPE or MAE? How easy it is to infer that our trained model has fitted well across the distribution of dependent variable?\n\n\n\n\n\nPlot of Anscombe’s Quartet\n\n\n\n\n\nSome Descriptive Stats for Anscombe’s Quartet\n\n\nLet me give you a pretty small data-set to play with “The Anscombe’s quartet”. This is a very famous data-set by Francis Anscombe. Please refer the plots below to understand the distribution of y1, y2, y3, y4. Isn’t it different?\nWould the measure of central tendency and disportion be same for this data? I am sure none of us would believe but to our utter surprise we see all the descriptive stats are kind of same. Don’t believe me !!! Please see the results below ( Source: Wikipedia ):"
  },
  {
    "objectID": "posts/engineering/01_adaptive_regression/index.html#so-what-we-do-now",
    "href": "posts/engineering/01_adaptive_regression/index.html#so-what-we-do-now",
    "title": "Adaptive Regression",
    "section": "So what we do Now!",
    "text": "So what we do Now!\nAstonished !!! Don’t be. This is what has been hiding behind those numbers. And this is why we really won’t be able to cross certain performance level. Unless you change some features or even do a lot of hyper parameter tuning, your results won’t vary much.\nIf you look at the average value of MAPE in each decile you would see an interesting pattern. Let us show you what we see that pattern. One day while working on a business problem where I was using regression on a discussion with Kumarjit, we deviced a different way of model diagnosis. We worked together to give this a shape and build on it.\n\nAs you can see it is absolutely evident that either of the side in the distribution of MAPE values is going wild!!!!!!! Still overall MAPE is good (18%)."
  },
  {
    "objectID": "posts/engineering/01_adaptive_regression/index.html#seeking-scope-of-improvement",
    "href": "posts/engineering/01_adaptive_regression/index.html#seeking-scope-of-improvement",
    "title": "Adaptive Regression",
    "section": "Seeking Scope of Improvement",
    "text": "Seeking Scope of Improvement\nWe worked together to build a different framework to address such issues on the go and reduce the MAPE deterioration on the edge of the distribution.\nThis problems gives rise to a concept we named as Distribution Assertive Regression (DAR).\nDAR is a framework that is based on cancelling the weakness of one point summaries by using the classical concepts of Reliability Engineering : The Bath Tub Curve.\n\n\n\n\n\nPlot for Classical Bath Tub Curve using a Hazard Function\n\n\nThe Specialty of this curve is that it gives you the likelihood which areas one tends to have high failure rates. In our experiments when we replace failure with MAPE value and the Time with sorted (ascending) value of target / dependent variable, we observe the same phenomenon. This is likely to happen because most of regression techniques assumes Normal (Gaussian) Distribution of data and fits itself towards the central tendency of this distribution.\nBecause of this tendency, any regression methods tends to learn less about data which are away from the central tendency of the target.\nLets look at BostonHousing data from “mlbench” package in R.\n\n\n\nPlot for MAPE Bath Tub Curve for Decile Split “mdev” from Data\n\n\nHere the MAPE is calculated for each decile split of ordered target variable. As you can observe it is following the bath tub curve. Hence the validates our hypothesis that the regression method is not able to understand much about the data at the either ends of the distribution."
  },
  {
    "objectID": "posts/engineering/01_adaptive_regression/index.html#final-analysis",
    "href": "posts/engineering/01_adaptive_regression/index.html#final-analysis",
    "title": "Adaptive Regression",
    "section": "Final Analysis",
    "text": "Final Analysis\nNow the DAR framework essentially fixes this weakness of regression method and understands the behavior of data which is stable and can be tweak in a fashion that can be use in general practice.\nPlot of MAPE Bath Tub Curve after applying DAR Framework for Decile Split “mdev” from Data\n\nHow this framework with same method reduced MAPEs so much and made model much more stable…?? Well here it is:\nThe DAR framework splits the data at either ends of the order target variable and performs regression on these “split” data individually. This inherently reduces the so called “noise” part of the data and treat it as an individual data."
  },
  {
    "objectID": "posts/engineering/01_adaptive_regression/index.html#scoring-on-new-data",
    "href": "posts/engineering/01_adaptive_regression/index.html#scoring-on-new-data",
    "title": "Adaptive Regression",
    "section": "Scoring on New Data",
    "text": "Scoring on New Data\nNow you might be thinking while applying regression this sounds good but how will one score this on new data. Well to answer that we used our most simple yet very effective friend “KNN” (Though any multiclass Classifier can be used here). So ideally scoring involves two step method :\n\nScore new value against each KNN / Multiclass Classifier model of the data\nBased on closeness we score it with the regression method used for that part of data.\n\nSo now we know how we can improve the prediction power of data for regression."
  },
  {
    "objectID": "posts/engineering/01_adaptive_regression/index.html#code-and-flowchart",
    "href": "posts/engineering/01_adaptive_regression/index.html#code-and-flowchart",
    "title": "Adaptive Regression",
    "section": "Code and Flowchart",
    "text": "Code and Flowchart\nIf things are simple lets keep it simple. Refer flowchart and code below for implementation of this framework. Paper here!\n\nR codePython codeHere is the Flow Chart\n\n\n\n\n\n\n\n\nTipClick to Expand\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipClick to Expand\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngraph TB\n    \n    subgraph Testing\n        p1(Finding bucket of model to choose)\n        p1 --&gt; p2([Making predictions &lt;br&gt; based on selected model for inference])\n        p2 --&gt; p3(Consolidate final score of prediction)\n    end\n\n    subgraph Training\n        md([Fitting a &lt;br&gt;Regression model])==&gt; di\n        di{Binning Data via &lt;br/&gt; evaluating Distribution &lt;br/&gt; MAPE values }\n        di --&gt; md2([Fitting a Buckteing model &lt;br/&gt; to Binned MAPE Buckets])\n        md2 --&gt; md3([Fitting Regression &lt;br&gt; Models on Binned Data])\n        md == Keeping main&lt;br/&gt;model ==&gt; ro        \n        md3 ==&gt; ro(Final Models &lt;br&gt; Binning Data Models + &lt;br&gt; Set of Regressoin Models)\n    end\n\n    \n    od([Data Input]) -- Training&lt;br&gt; Data--&gt; md\n    od -- Testing&lt;br&gt; Data--&gt; p1\n    ro -.-&gt; p1\n    ro -.-&gt; p2\n\n    classDef green fill:#9f6,stroke:#333,stroke-width:2px;\n    classDef yellow fill:#ff6,stroke:#333,stroke-width:2px;\n    classDef blue fill:#00f,stroke:#333,stroke-width:2px,color:#fff;\n    classDef orange fill:#f96,stroke:#333,stroke-width:4px;\n    class md,md2,md3 green\n    class di orange\n    class p1,p2 yellow\n    class ro,p3 blue"
  },
  {
    "objectID": "about.html#the-ai-architect",
    "href": "about.html#the-ai-architect",
    "title": "Jitin Kapila",
    "section": "",
    "text": "14 years. 6 industries. One insight that changes everything."
  }
]