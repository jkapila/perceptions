[
  {
    "objectID": "posts/engineering/10_treeknn/index.html",
    "href": "posts/engineering/10_treeknn/index.html",
    "title": "TrieKNN: Unleashing KNN’s Power on Mixed Data Types",
    "section": "",
    "text": "Photo by Gelgas Airlangga"
  },
  {
    "objectID": "posts/engineering/10_treeknn/index.html#the-allure-and-limitation-of-knn",
    "href": "posts/engineering/10_treeknn/index.html#the-allure-and-limitation-of-knn",
    "title": "TrieKNN: Unleashing KNN’s Power on Mixed Data Types",
    "section": "The Allure and Limitation of KNN",
    "text": "The Allure and Limitation of KNN\nIn the realm of machine learning, the K-Nearest Neighbors (KNN) algorithm stands out for its intuitive nature and ease of implementation. Its principle is simple: classify a data point based on the majority class among its ‘k’ nearest neighbors in the feature space. This non-parametric approach makes no assumptions about the underlying data distribution, rendering it versatile for various applications. KNN is very popular, but it comes with some limitations.\nHowever, KNN’s Achilles’ heel lies in its reliance on distance metrics, which are inherently designed for numerical data. Real-world datasets often contain a mix of numerical and categorical features, posing a significant challenge for KNN. How do you measure the distance between ‘red’ and ‘blue,’ or ‘large’ and ‘small’?\n\nPrior Art\nSeveral strategies have been proposed to adapt KNN for mixed data:\n\nOne-Hot Encoding: Converts categorical features into numerical vectors, but can lead to high dimensionality.\nDistance Functions for Mixed Data: Develops and apply custom distance metrics that can handle both numerical and categorical features such as HEOM and many others.\nUsing mean/mode values: Replace the missing values with mean/mode.\n\nThese methods often involve compromises, either distorting the data’s inherent structure or adding computational overhead."
  },
  {
    "objectID": "posts/engineering/10_treeknn/index.html#enter-trieknn-a-novel-approach",
    "href": "posts/engineering/10_treeknn/index.html#enter-trieknn-a-novel-approach",
    "title": "TrieKNN: Unleashing KNN’s Power on Mixed Data Types",
    "section": "Enter TrieKNN: A Novel Approach",
    "text": "Enter TrieKNN: A Novel Approach\nWhat if we could cleverly sidestep the distance calculation problem for categorical features, while still leveraging KNN’s power? TrieKNN offers just that—a way to perform KNN on any mixed data!\nTrieKNN combines the strengths of Trie data structures and KNN to handle mixed data types gracefully. Here’s the core idea:\n\nTrie-Based Categorical Encoding: A Trie is used to store the categorical features of the data. Each node in the Trie represents a category.\nLeaf-Node KNN Models: At the leaf nodes of the Trie, where specific combinations of categorical features are found, we fit individual KNN models using only the numerical features.\nWeighted Prediction: To classify a new data point, we traverse the Trie based on its categorical features. At each level, we calculate a weighted distance based on available data, ending in a probability score in each leaf node.\n\n\nWhy This Works\n\nNo Direct Distance Calculation for Categorical Features: The Trie structure implicitly captures the relationships between categorical values.\nLocalized KNN Models: By fitting KNN models at the leaf nodes, we ensure that distance calculations are performed only on relevant numerical features.\nScalability: The Trie structure efficiently handles a large number of categorical features and values."
  },
  {
    "objectID": "posts/engineering/10_treeknn/index.html#building-a-trieknn-model",
    "href": "posts/engineering/10_treeknn/index.html#building-a-trieknn-model",
    "title": "TrieKNN: Unleashing KNN’s Power on Mixed Data Types",
    "section": "Building a TrieKNN Model",
    "text": "Building a TrieKNN Model\nLet’s dive into the implementation. We’ll start with the TrieNode and Trie classes, then move on to the KNN model and the training/prediction process.\n\nTrie Implementation\n\n\nCode\nimport numpy as np\nfrom collections import Counter\n\nclass TrieNode:\n    def __init__(self):\n        self.children = {}  # Dictionary to store child nodes\n        self.is_end_of_word = False  # True if the node is the end of a word\n        self.count = 0  # Count of how many times a word has been inserted\n        self.class_counts = {}  # Class counts\n        self.class_weights = {}\n        self.model = None  # Model at leaf nodes\n        self.indexes = []  # Store data indexes belonging to this leaf\n        self.labels = []  # Store data indexes belonging to this leaf\n        self.node_weight = None\n\nclass Trie:\n    def __init__(self):\n        self.root = TrieNode()  # Root node of the Trie\n        self.data_index = 0  # Initialize data index\n\n    def insert(self, word_val, model):\n        current_node = self.root\n        word, val = word_val\n        current_node.count += 1\n\n        # Adding class counts\n        if val not in current_node.class_counts:\n            current_node.class_counts[val] = 0\n        current_node.class_counts[val] += 1\n\n        for char in word:\n            # If the character is not in children, add a new TrieNode\n            if char not in current_node.children:\n                current_node.children[char] = TrieNode()\n            current_node = current_node.children[char]\n\n            # Adding count of instances\n            current_node.count += 1\n\n            # adding class counts\n            if val not in current_node.class_counts:\n                current_node.class_counts[val] = 0\n            current_node.class_counts[val] += 1\n\n        # Mark the end of the word and increment count\n        current_node.is_end_of_word = True\n        current_node.indexes.append(self.data_index)  # Store the data index\n        current_node.labels.append(val)\n        current_node.model = model\n        self.data_index += 1  # Increment data index\n\n    def search(self, word):\n        current_node = self.root\n        for char in word:\n            # If the character doesn't exist in the children, the word doesn't exist\n            if char not in current_node.children:\n                return False\n            current_node = current_node.children[char]\n\n        # Return True if it's the end of a word and the word exists\n        return current_node.is_end_of_word\n\n    def count_word(self, word):\n        current_node = self.root\n        for char in word:\n            # If the character doesn't exist, the word doesn't exist\n            if char not in current_node.children:\n                return 0, current_node.class_counts  # Correctly return class_counts\n            current_node = current_node.children[char]\n\n        # Return the count of the word\n        return current_node.count, current_node.class_counts\n\n    def display(self):\n        # Recursively display the tree\n        def _display(node, word):\n            if node.is_end_of_word:\n                print(f\"Data: {word}, Count: {node.count}, Indexes: {len(node.indexes)} Classes :{node.class_counts} weights:{len(node.class_weights)}\")  # Display indexes too\n            for char, child in node.children.items():\n                _display(child, word + char)  # corrected the display\n\n        _display(self.root, \"\")\n\n    def apply(self, func):\n        \"\"\"\n        Applies a function to all models in the leaf nodes.\n        \"\"\"\n        def _apply(node):\n            if node.is_end_of_word and node.model is not None:\n                func(node)\n            for child in node.children.values():\n                _apply(child)\n\n        _apply(self.root)\n\n    def apply_weight_to_indexes(self, weight):\n        \"\"\"\n        Applies a weight to the indexes based on the percentage of data available.\n        \"\"\"\n        def _apply_weight_to_indexes(node):\n            if node.is_end_of_word:\n                total_count = sum(self.root.children[child].count for child in self.root.children)\n                percentage = node.count / total_count if total_count &gt; 0 else 0\n                weighted_indexes = [(index, weight * percentage) for index in node.indexes]\n                node.class_weights = weighted_indexes  # Corrected this line\n            for child in node.children.values():\n                _apply_weight_to_indexes(child)\n\n        _apply_weight_to_indexes(self.root)\n\n\n\n\nKNN Model\n\n\nCode\nclass KNNModel:\n    def __init__(self, k=5):\n        self.data = None\n        self.labels = []\n        self.k = k\n\n    def fit(self, data, indexes, labels):\n        # print(\"Fitting model with indexes:\", len(indexes), \"labels:\", len(labels))\n        self.data = data[indexes].astype(float)\n        self.labels = np.array(labels).astype(float)\n\n    def predict(self, data):\n        # print(\"Predicting with data:\", data)\n        dist_ind = np.sqrt(np.sum((self.data - data) ** 2, axis=1) ** 2)  # euclidean distance\n        main_arr = np.column_stack((self.labels, dist_ind))  # labels with distance\n        main = main_arr[main_arr[:, 1].argsort()]  # sorting based on distance\n        count = Counter(main[0:self.k, 0])  # counting labels\n        sums = np.array(list(count.values()))  # getting counts\n        return sums / np.sum(sums)  # prediction as probability\n\n\n\n\nTraining and Evaluation\nHere’s how we train and evaluate the TrieKNN model:\n\n\nCode\n# Sample data\nn = 10000\ndata = np.array((np.random.choice(['Anything ', 'By ','Chance '], p=[0.6,0.1,0.3],size=n),\n                 np.random.choice(['can', 'go', 'here','lets', 'see', \"it\"], p=[0.1, 0.1, 0.1, 0.2, 0.4, 0.1], size=n),\n                 np.random.normal(3, 1, size=n),\n                 np.random.normal(5, 2, size=n))).T\ny_label = np.random.choice([0,1], p=[0.7, 0.3], size=n)\n\n# Trie training\ntrie = Trie()\nfor X, y in zip(data, y_label):\n    trie.insert((X[:2], y),None)\n\n# Apply weights to indexes\ntrie.apply_weight_to_indexes(0.5)\n\n# Fit models of leaf nodes\ndef add_model(node, data):\n    node.model = KNNModel()\n    node.model.fit(data, node.indexes, node.labels)\n\ndef traverse_and_add_model(node, data):\n    if node.is_end_of_word:\n        add_model(node, data)  # Add model to leaf node\n    for child in node.children.values():\n        traverse_and_add_model(child, data)\n\ntraverse_and_add_model(trie.root, data[:, 2:])\n\n\n\n\nExplanation\n\nWe create sample data with mixed categorical and numerical features.\nWe insert each data point into the Trie, using the categorical features as the path.\nAfter the Trie is built, we traverse it and fit a KNN model to the data points stored at each leaf node.\nFinally, we can predict the class of new data points by traversing the Trie and using the KNN model at the corresponding leaf node."
  },
  {
    "objectID": "posts/engineering/10_treeknn/index.html#results-and-discussion",
    "href": "posts/engineering/10_treeknn/index.html#results-and-discussion",
    "title": "TrieKNN: Unleashing KNN’s Power on Mixed Data Types",
    "section": "Results and Discussion",
    "text": "Results and Discussion\nLet us display the trie.\n\n\n\n\nCode\ntrie.display()\n\n\nData: Anything see, Count: 2429, Indexes: 2429 Classes :{np.int64(0): 1690, np.int64(1): 739} weights:2429\nData: Anything here, Count: 576, Indexes: 576 Classes :{np.int64(0): 407, np.int64(1): 169} weights:576\nData: Anything lets, Count: 1166, Indexes: 1166 Classes :{np.int64(1): 335, np.int64(0): 831} weights:1166\nData: Anything go, Count: 599, Indexes: 599 Classes :{np.int64(1): 172, np.int64(0): 427} weights:599\nData: Anything can, Count: 623, Indexes: 623 Classes :{np.int64(0): 439, np.int64(1): 184} weights:623\nData: Anything it, Count: 614, Indexes: 614 Classes :{np.int64(0): 432, np.int64(1): 182} weights:614\nData: Chance can, Count: 300, Indexes: 300 Classes :{np.int64(1): 84, np.int64(0): 216} weights:300\nData: Chance see, Count: 1247, Indexes: 1247 Classes :{np.int64(1): 403, np.int64(0): 844} weights:1247\nData: Chance it, Count: 295, Indexes: 295 Classes :{np.int64(1): 101, np.int64(0): 194} weights:295\nData: Chance lets, Count: 602, Indexes: 602 Classes :{np.int64(1): 182, np.int64(0): 420} weights:602\nData: Chance here, Count: 301, Indexes: 301 Classes :{np.int64(0): 209, np.int64(1): 92} weights:301\nData: Chance go, Count: 314, Indexes: 314 Classes :{np.int64(0): 228, np.int64(1): 86} weights:314\nData: By lets, Count: 177, Indexes: 177 Classes :{np.int64(0): 120, np.int64(1): 57} weights:177\nData: By see, Count: 386, Indexes: 386 Classes :{np.int64(0): 279, np.int64(1): 107} weights:386\nData: By here, Count: 91, Indexes: 91 Classes :{np.int64(0): 74, np.int64(1): 17} weights:91\nData: By go, Count: 83, Indexes: 83 Classes :{np.int64(0): 54, np.int64(1): 29} weights:83\nData: By can, Count: 102, Indexes: 102 Classes :{np.int64(0): 68, np.int64(1): 34} weights:102\nData: By it, Count: 95, Indexes: 95 Classes :{np.int64(1): 24, np.int64(0): 71} weights:95\n\n\nThe model predicted the following values:\n\n\n\n\nCode\n# Prediction example\ndef predict_with_model(node):\n    predictions = node.model.predict(np.array([2,5]))\n    print(\"Predictions:\", predictions)\n\ntrie.apply(predict_with_model)\n\n\nPredictions: [0.6 0.4]\nPredictions: [0.4 0.6]\nPredictions: [0.4 0.6]\nPredictions: [0.6 0.4]\nPredictions: [1.]\nPredictions: [0.8 0.2]\nPredictions: [0.8 0.2]\nPredictions: [1.]\nPredictions: [0.4 0.6]\nPredictions: [0.8 0.2]\nPredictions: [0.8 0.2]\nPredictions: [0.4 0.6]\nPredictions: [0.6 0.4]\nPredictions: [0.8 0.2]\nPredictions: [0.8 0.2]\nPredictions: [0.6 0.4]\nPredictions: [0.8 0.2]\nPredictions: [0.2 0.8]\n\n\nThe predictions will vary on each run. From this we can see that we can use KNN on mixed data types."
  },
  {
    "objectID": "posts/engineering/10_treeknn/index.html#conclusion-a-promising-path-forward",
    "href": "posts/engineering/10_treeknn/index.html#conclusion-a-promising-path-forward",
    "title": "TrieKNN: Unleashing KNN’s Power on Mixed Data Types",
    "section": "Conclusion: A Promising Path Forward",
    "text": "Conclusion: A Promising Path Forward\nTrieKNN presents a compelling solution for extending the applicability of KNN to mixed data types. By leveraging the Trie data structure, it avoids direct distance calculations on categorical features, enabling the use of localized KNN models for numerical data.\nFurther research could explore:\n\nOptimizing the weighting scheme for combining predictions from different Trie levels.\nComparing TrieKNN’s performance against other mixed-data KNN approaches on benchmark datasets.\nExtending TrieKNN to handle missing data and noisy categorical features.\n\nTrieKNN opens up new possibilities for applying KNN in domains where mixed data types are prevalent, such as healthcare, e-commerce, and social science.\nResources and further reads:\n1. Nomclust R package\n2. An Improved kNN Based on Class Contribution and Feature Weighting\n3. An Improved Weighted KNN Algorithm for Imbalanced Data Classification\n4. A weighting approach for KNN classifier\n5. Unsupervised Outlier Detection for Mixed-Valued Dataset Based on the Adaptive k-Nearest Neighbor Global Network\n6. A hybrid approach based on k-nearest neighbors and decision tree for software fault prediction\n7. Analysis of Decision Tree and K-Nearest Neighbor Algorithm in the Classification of Breast Cancer"
  },
  {
    "objectID": "posts/engineering/02_hypothesis_test/index.html",
    "href": "posts/engineering/02_hypothesis_test/index.html",
    "title": "A flow to Test Your Hypothesis in Python",
    "section": "",
    "text": "Hypothesis testing Photo by Tara Winstead\n\n\n\nOverview\nAll the practitioners of data science always hit one giant thing to do with data and you know it well its EDA -Exploratory Data Analysis. This word EDA1 was coined by Tukey himself in his seminal book published in 1983. But do you think that before that EDA doesn’t existed ?\n1 Emerson, J. D., & Hoaglin, D. C. (1983). Stem-and-leaf displays. In D. C. Hoaglin, F. Mosteller, & J. W. Tukey (Eds.) Understanding Robust and Exploratory Data Analysis, pp. 7–32. New York: Wiley. Book is here.Well glad you thought. Before that all were doing what is called as Hypothesis Testing. Yes, before this the race was majorly to fit the data and make most unbiased and robust estimate. But remember one thing when you talk about Hypothesis Testing it was always and majorly would be related to RCTs (Randomized Controlled Trials) a.k.a Randomized Clinical Trials and is Gold Standard of data.\n\n\n\n\n\n\nTipMore on RCTs and ODs\n\n\n\n\n\nNow let me now not hijack the discussion to what is RCTs and Observational Data (ODs) as it is more of Philosophical Reasoning rather than other quality of data, but essentially what we are trying to find is that can we by, using stats, identify interesting patterns in data.\nThe only thing happens wit RCT data is that we tend to believe these interesting patterns coincide with some sort of ‘Cause-Effect’ kind of relationship. But essentially due to bia nature of ODs, we certainly cant conclude this. And hence, can only find interesting patterns.\n\n\n\nLets move on. The big question is, for whatever reason you are doing HT , you are doing it for finding something intreating. And that something interesting is usually found by using Post-Hoc Tests. Now there are variety of Post-Hocs available but what is more know and hence easily found to be implemented in Tukey’s HSD.\nSo lets directly jump to how to follow this procedure. We’ll be using bioinfokit for this, as it is much simpler wrapper around whats implemented in statsmodels.\n\n\nWhat are the results\nPheww… Thats too much code right. But that would save a lot of your time in real life. So in real life you would write code as 3 steps below:\n\n\nCode\n# import libraries\nimport pandas as pd\n\n# Getting car data from UCI\ndf = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data',\n                 sep='\\s+',header=None,\n                 names=['mpg','cylinders','displacement','horsepower','weight',\n                 'acceleration','model_year','origin','car_name'])\ndf.head()\n\n# Syntax to do anove with validating the assumption, doing test and a post-hoc\nresults = do_anova_test(df=df, res_var='mpg',xfac_var='cylinders', \n                        anova_model='mpg ~ C(cylinders)+C(origin)+C(cylinders):C(origin)',\n                        ss_typ=3, result_full=True)\n\n\nResults form the do_anova_test\nLevens Test Result:\n                 Parameter    Value\n0      Test statistics (W)  14.5856\n1  Degrees of freedom (Df)   4.0000\n2                  p value   0.0000\n\nBartletts Test Result:\n                 Parameter    Value\n0      Test statistics (T)  61.2143\n1  Degrees of freedom (Df)   4.0000\n2                  p value   0.0000\n\nANOVA\\ANCOVA Test Result:\n                           df     sum_sq    mean_sq         F  PR(&gt;F)      n2\nIntercept                 1.0  6195.1701  6195.1701  296.3452  0.0000  0.2727\nC(cylinders)              4.0  7574.5864  1893.6466   90.5824  0.0000  0.3334\nC(origin)                 2.0   241.0703   120.5351    5.7658  0.0034  0.0106\nC(cylinders):C(origin)    8.0   577.4821    72.1853    3.4530  0.0046  0.0254\nResidual                389.0  8132.1404    20.9052       NaN     NaN     NaN\n\nTukey HSD Result:\n   group1  group2     Diff    Lower    Upper  q-value  p-value\n0       8       4  14.3237  12.8090  15.8383  36.6527   0.0010\n1       8       6   5.0226   3.1804   6.8648  10.5671   0.0010\n2       8       3   5.5869  -0.7990  11.9728   3.3909   0.1183\n3       8       5  12.4036   5.0643  19.7428   6.5503   0.0010\n4       4       6   9.3011   7.6765  10.9256  22.1910   0.0010\n5       4       3   8.7368   2.4102  15.0633   5.3524   0.0017\n6       4       5   1.9201  -5.3676   9.2078   1.0212   0.9000\n7       6       3   0.5643  -5.8486   6.9772   0.3410   0.9000\n8       6       5   7.3810   0.0182  14.7437   3.8854   0.0491\n9       3       5   6.8167  -2.7539  16.3873   2.7606   0.2919\nNice!!!\n\nAnd plotting is even easier\n\n\nCode\n# Numbers are clumsy for most. Making more interpretable plot on above results.\nplot_hsd(results.tukeyhsd.sort_values('Diff'), title=\"Tukey HSD resutls Anova of MPG ~ Cylinder\")\n\n\nResults form the plot_hsd\n\n\n\nTukey’s HSD comparison based on Anova Results\n\n\nPlots look good with ‘p-values’.\n\n\nConclusion\nNow since we applied the above to a Non RCT we cannot conclude that Difference in mpg based on cylinder is huge specially as number of cylinders goes up. But this statement might not be as explicit as might be appearing from plot. Unless you have a strong believe that the data follows with rules and assumptions of RCTs, we should be only seeking interesting as in associated results and not cause-effect results.\n\n\nGive me “The Code”\n\nPerforming AnovaPlotting Results\n\n\n\n\nAnova Test anova_test.py\nfrom bioinfokit import analys\n\nimport numpy as np\nfrom scipy import stats\n\nclass KeyResults:\n    \"\"\"\n    A basic class to hold all the results\n    \"\"\"\n    \n    def __init__(self,result_full):\n        self.keys = []\n        self.result_full = result_full\n    \n    def add_result(self,name,result):\n        if name == 'tukeyhsd':\n            self.keys.append(name)\n            setattr(self, name, result)\n        elif self.result_full:\n            self.keys.append(name)\n            setattr(self, name, result)\n\n\n# Anova test code\ndef do_anova_test(df, res_var, xfac_var, anova_model,ss_typ=3,\n                  effectsize='n2',result_full=False,add_res=False):\n    \"\"\"\n    Do all sequential anova tests\n    \n    Step 1) Leven's/ bartellet test for checking weather variance is homogenous or not\n    Step 2) Main ANOVA/ANCOVA test\n    Step 3) Tukey's HSD for individual combinations\n    \n    :param df: Pandas DataFrame holding all the columns\n    :param res_var: Variable for which we are checking ANOVA\n    :param xfac_var: Grouping Variables for which we want to do the comparisons\n    :param anova_model: SM formula for the model. This is life savour to make all things work\n    :param result_full: To provide all the results of intermediate steps\n    \n    \"\"\"\n\n    results = KeyResults(result_full)\n    \n    # initialize stat method\n    res = analys.stat()\n    \n    # doing levens test\n    res.levene(df=df, res_var=res_var,xfac_var=xfac_var)\n    print('\\nLeven\\'s Test Result:')\n    print(res.levene_summary)\n    results.add_result('levene',res.levene_summary)\n\n    # doing bartlett test\n    res.bartlett(df=df, res_var=res_var,xfac_var=xfac_var)\n    print('\\nBartlett\\'s Test Result:')\n    print(res.bartlett_summary)\n    results.add_result('bartlett',res.bartlett_summary)\n    \n    # doing anova / ancova\n    res.anova_stat(df=df, res_var=res_var, anova_model=anova_model,ss_typ=ss_typ)\n    aov_res = res.anova_summary\n    \n    # Add effect sizes\n    if effectsize == \"n2\":\n        all_effsize = (aov_res['sum_sq'] / aov_res['sum_sq'].sum()).to_numpy()\n        all_effsize[-1] = np.nan\n    else:\n        ss_resid = aov_res['sum_sq'].iloc[-1]\n        all_effsize = aov_res['sum_sq'].apply(lambda x: x / (x + ss_resid)).to_numpy()\n        all_effsize[-1] = np.nan\n    aov_res[effectsize] = all_effsize\n    #aov_res['bw_'] = res.anova_model_out.params.iloc[-1]\n    aov_res = aov_res.round(4)\n    \n    # printing results\n    print('\\nANOVA\\ANCOVA Test Result:')\n    print(aov_res)\n    results.add_result('anova',res.anova_summary.round(4))\n    results.add_result('anova_model',res.anova_model_out)\n    \n    # doing tukey's hsd top compare the groups\n    res.tukey_hsd(df=df, res_var=res_var,xfac_var=xfac_var, anova_model=anova_model,ss_typ=ss_typ)\n    print('\\nTukey HSD Result:')\n    print(res.tukey_summary.round(4))\n    results.add_result('tukeyhsd',res.tukey_summary.round(4))\n    \n    # add all result componets again if needed \n    if add_res:\n        results.add_result('allresult',res)\n    \n    return results\n\n\n\n\n\n\nPlotting results plot_hsd.py\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.style.use('seaborn-bright')\n\ndef plot_hsd(hsdres,p_cutoff=0.05,title=None,ax=None,figsize=(10,7)):\n     \"\"\"\n     Do plotting of tukeyhsd results\n    \n  \n    :param hsdres: 'tukeyhsd' result form the do_anova_test function\n    :param p_cutoff: Cutoff at which we get say a combination is significant\n    :param title: Title of the plot\n    :param ax: Define or get the matplotlib axes\n    :param figsize: Mention Figure size to draw\n    \n    \"\"\"\n\n    if ax is None:\n        fig,axp = plt.subplots(figsize=figsize)\n    else:\n        axp = ax\n    \n    # helper func\n    p_ind = lambda x : '' if x &gt; 0.1 else ('+' if x &gt; 0.05 else ('*' if x &gt; 0.01 else ('**' if x &gt;0.001 else '***')))\n    label_gen  = lambda x: f\"${x[0]} - {x[1]}\\ |\\ p:{x[2]:0.2f}{p_ind(x[2]):5s}$\"\n    \n    #setting values\n    mask = hsdres['p-value'] &lt;= p_cutoff\n    yticklabs = hsdres[['group1','group2','p-value']].apply(label_gen,axis=1).values\n    ys = np.arange(len(hsdres))\n    \n    # adding plot to axes\n    axp.errorbar(hsdres[~mask]['Diff'],ys[~mask],xerr=np.abs(hsdres[~mask][['Lower',\"Upper\"]]).values.T,\n                fmt='o', color='black', ecolor='lightgray', elinewidth=2, capsize=0)\n    axp.errorbar(hsdres[mask]['Diff'],ys[mask],xerr=np.abs(hsdres[mask][['Lower',\"Upper\"]]).values.T,\n                fmt='o', color='red', ecolor='pink', elinewidth=2, capsize=5)\n    axp.axvline(x=0,linestyle='--',c='skyblue')\n    axp.set_yticks([])\n    (l,u) = axp.get_xlim()\n    axp.set_xlim(l+1.5*l,u)\n    (l,u) = axp.get_xlim()\n    for idx,labs in enumerate(yticklabs):\n        axp.text(l-0.1*l,ys[idx],labs)\n    axp.set_yticklabels([])\n    \n    # finally doing what is needed\n    if ax is None:\n        plt.title('' if title is None else title,fontsize=14)\n        plt.show()\n    else:\n        return axp\n\n\n\n\n\nHope this give you kickstart to find you intresting patterns. Happy Learning!"
  },
  {
    "objectID": "posts/engineering/04_crosstab_sparsity_classification/index.html",
    "href": "posts/engineering/04_crosstab_sparsity_classification/index.html",
    "title": "CrossTab Sparsity for Classification",
    "section": "",
    "text": "Cross Roads where everyone meets!\n\n\n\nIntroduction: A Journey into Data\nPicture this: you’re standing on the icy shores of Antarctica, the wind whipping around you as you watch a colony of Palmer Penguins waddling about, oblivious to the data detective work you’re about to embark on. As a data science architect, you’re not just an observer; you’re a sleuth armed with algorithms and insights, ready to unravel the mysteries hidden within data. Today, we’ll transform raw numbers into powerful narratives using CrossTab Sparsity as our guiding compass. This blog post will demonstrate how this metric can revolutionize classification tasks, shedding light on many fascinating datasets—the charming Palmer Penguins and the serious Obesity, Credit cards data and many more.\n\n\nThe Power of CrossTab Sparsity\n\nWhat is CrossTab Sparsity?\nCrossTab Sparsity isn’t just a fancy term that sounds good at dinner parties; it’s a statistical measure that helps us peer into the intricate relationships between categorical variables. Imagine it as a magnifying glass that reveals how different categories interact within a contingency table. Understanding these interactions is crucial in classification tasks, where the right features can make or break your model (and your day).\nWhy Does It Matter?\nIn the world of data science, especially in classification, selecting relevant features is like picking the right ingredients for a gourmet meal—get it wrong, and you might end up with something unpalatable. CrossTab Sparsity helps us achieve this by:\n\nHighlighting Relationships: It’s like having a friend who always points out when two people are meant to be together—understanding how features interact with the target variable.\nStreamlining Models: Reducing complexity by focusing on significant features means less time spent untangling spaghetti code.\nEnhancing Interpretability: Making models easier to understand and explain to stakeholders is like translating tech jargon into plain English—everyone appreciates that!\n\n\n\n\nData Overview: Our Data People at work here\n\nThe Datasets\nData 1: Estimation of Obesity Levels Based On Eating Habits and Physical Condition\nLittle bit about the data: This dataset, shared on 8/26/2019, looks at obesity levels in people from Mexico, Peru, and Colombia based on their eating habits and physical health. It includes 2,111 records with 16 features, and classifies individuals into different obesity levels, from insufficient weight to obesity type III. Most of the data (77%) was created using a tool, while the rest (23%) was collected directly from users online.\nData 2: Predict Students’ Dropout and Academic Success\nLittle bit about the data: This dataset, shared on 12/12/2021, looks at factors like students’ backgrounds, academic path, and socio-economic status to predict whether they’ll drop out or succeed in their studies. With 4,424 records across 36 features, it covers students from different undergrad programs. The goal is to use machine learning to spot at-risk students early, so schools can offer support. The data has been cleaned and doesn’t have any missing values. It’s a classification task with three outcomes: dropout, still enrolled, or graduated\nKey Features:\n\nMulticlass: Both data set cater a multi class problems with NObeyesdad and Target columns\nMixed Data Type: A good mix of categorical and continuous variables are available for usage.\nSizeable: More than 2 K rows are available for testing.\n\n\n\n\nExploratory Data Analysis (EDA): Setting the Stage\nBefore we dive into model creation, let’s explore our dataset through some quick EDA. Think of this as getting to know your non-obese friends before inviting them to a party.\n\nEDA for Obesity Data\nHere’s a brief code snippet to perform essential EDA on the Obesity dataset:\n\n\nLoading data and generating basic descriptive\n# Load the Obesity data\nraw_df = pd.read_csv('ObesityDataSet_raw_and_data_sinthetic.csv')\ntarget = 'NObeyesdad'\n\n# Load Students data\n\n# Load Credit data\n# raw_data = sm.datasets.get_rdataset(\"credit_data\",'modeldata')\n# raw_df = raw_data.data\n# target = 'Status'\n\n# # Load Palmer penguins data\n# raw_data = sm.datasets.get_rdataset(\"penguins\",'palmerpenguins')\n# raw_df = raw_data.data\n# target = 'species'\n\n\n# # Load Credit data\n# raw_data = sm.datasets.get_rdataset(\"CreditCard\",'AER')\n# raw_df = raw_data.data\n# target = 'card'\n\n\n# setting things up for aal the next steps\nraw_df[target] = raw_df[target].astype('category') \nprint('No of data points available to work:',raw_df.shape)\ndisplay(raw_df.head())\n\n\n# Summary statistics\ndisplay(raw_df.describe())\n\n\nNo of data points available to work: (2111, 17)\n\n\n\n\n\n\n\n\n\nGender\nAge\nHeight\nWeight\nFamil_Hist_Owt\nFAVC\nFCVC\nNCP\nCAEC\nSMOKE\nCH2O\nSCC\nFAF\nTUE\nCALC\nMTRANS\nNObeyesdad\n\n\n\n\n0\nFemale\n21.0\n1.62\n64.0\nyes\nno\n2.0\n3.0\nSometimes\nno\n2.0\nno\n0.0\n1.0\nno\nPublic_Transportation\nNormal_Weight\n\n\n1\nFemale\n21.0\n1.52\n56.0\nyes\nno\n3.0\n3.0\nSometimes\nyes\n3.0\nyes\n3.0\n0.0\nSometimes\nPublic_Transportation\nNormal_Weight\n\n\n2\nMale\n23.0\n1.80\n77.0\nyes\nno\n2.0\n3.0\nSometimes\nno\n2.0\nno\n2.0\n1.0\nFrequently\nPublic_Transportation\nNormal_Weight\n\n\n3\nMale\n27.0\n1.80\n87.0\nno\nno\n3.0\n3.0\nSometimes\nno\n2.0\nno\n2.0\n0.0\nFrequently\nWalking\nOverweight_Level_I\n\n\n4\nMale\n22.0\n1.78\n89.8\nno\nno\n2.0\n1.0\nSometimes\nno\n2.0\nno\n0.0\n0.0\nSometimes\nPublic_Transportation\nOverweight_Level_II\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAge\nHeight\nWeight\nFCVC\nNCP\nCH2O\nFAF\nTUE\n\n\n\n\ncount\n2111.000000\n2111.000000\n2111.000000\n2111.000000\n2111.000000\n2111.000000\n2111.000000\n2111.000000\n\n\nmean\n24.312600\n1.701677\n86.586058\n2.419043\n2.685628\n2.008011\n1.010298\n0.657866\n\n\nstd\n6.345968\n0.093305\n26.191172\n0.533927\n0.778039\n0.612953\n0.850592\n0.608927\n\n\nmin\n14.000000\n1.450000\n39.000000\n1.000000\n1.000000\n1.000000\n0.000000\n0.000000\n\n\n25%\n19.947192\n1.630000\n65.473343\n2.000000\n2.658738\n1.584812\n0.124505\n0.000000\n\n\n50%\n22.777890\n1.700499\n83.000000\n2.385502\n3.000000\n2.000000\n1.000000\n0.625350\n\n\n75%\n26.000000\n1.768464\n107.430682\n3.000000\n3.000000\n2.477420\n1.666678\n1.000000\n\n\nmax\n61.000000\n1.980000\n173.000000\n3.000000\n4.000000\n3.000000\n3.000000\n2.000000\n\n\n\n\n\n\n\n\n\nTarget distribution\n\n\nTarget and Correlation\n# Visualize target data distribution\nplt.figure(figsize=(4, 3))\nsns.countplot(data=raw_df, x=target, hue=target, palette='Set2',)\nplt.title(f'Distribution of {target} levels')\nplt.xticks(rotation=45)\nplt.show()\n\n# Heatmap to check for correlations between numeric variables\ncorr = raw_df.corr('kendall',numeric_only=True)\nsns.heatmap(corr, annot=True, cmap='coolwarm')\nplt.title('Kendall Correlation Heatmap')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoneSome Mode EDA for the data\n\n\n\n\n\n\n\nEDA code\n# Visualize the distribution of numerical variables\nsns.pairplot(raw_df, hue=target, corner=True)\nplt.show()\n\n\n\n\n# Gettign Categorical data\ncategorical_columns = raw_df.select_dtypes(include='object').columns\n\n# Plot categorical variables with respect to the target variable\nfor col in categorical_columns:\n    plt.figure(figsize=(12, 5))\n    sns.countplot(data=raw_df,x=col, hue=target)\n    plt.title(f\"Countplot of {col} with respect to {target}\")\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Creation: Establishing a Baseline\nWith our exploratory analysis complete, we’re ready to create our baseline model using logistic regression with Statsmodels. This initial model will serve as our reference point—like setting up a benchmark for your favorite video game.\n\n\nSplitting data and training a default Multinomila Logit model on our data\ndata_df = raw_df.dropna().reset_index(drop=True)\ndata_df[target] = data_df[target].cat.codes\n# X = data_df[['bill_length_mm','bill_depth_mm','flipper_length_mm','body_mass_g']] \n\ndata_df_test = data_df.sample(frac=0.1,random_state=3)\ndata_df_train = data_df.drop(data_df_test.index)\n\n# Using MN logistic regression model using formula API\n# This would essentially bold down to pair wise logsitic regression\nlogit_model = sm.MNLogit.from_formula(\n    f\"{target} ~ {' + '.join([col for col in data_df_train.columns if col != target])}\", \n    data=data_df_train\n).fit_regularized()\n\n\nOptimization terminated successfully    (Exit mode 0)\n            Current function value: 0.17057119619320013\n            Iterations: 485\n            Function evaluations: 639\n            Gradient evaluations: 485\n\n\n\n\n\n\n\n\n\n\nNoneBase model summary for geeks\n\n\n\n\n\n\n\nDisplay summary\ndisplay(logit_model.summary())\n\n\n\nMNLogit Regression Results\n\n\nDep. Variable:\nNObeyesdad\nNo. Observations:\n1900\n\n\nModel:\nMNLogit\nDf Residuals:\n1756\n\n\nMethod:\nMLE\nDf Model:\n138\n\n\nDate:\nSun, 07 Dec 2025\nPseudo R-squ.:\n0.9122\n\n\nTime:\n16:56:59\nLog-Likelihood:\n-324.09\n\n\nconverged:\nTrue\nLL-Null:\n-3691.8\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n0.000\n\n\n\n\n\n\n\n\nNObeyesdad=1\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-11.2903\n3.25e+05\n-3.48e-05\n1.000\n-6.36e+05\n6.36e+05\n\n\nGender[T.Male]\n-3.4851\n0.817\n-4.268\n0.000\n-5.085\n-1.885\n\n\nFamil_Hist_Owt[T.yes]\n-0.8162\n0.655\n-1.246\n0.213\n-2.100\n0.468\n\n\nFAVC[T.yes]\n0.2636\n0.785\n0.336\n0.737\n-1.275\n1.802\n\n\nCAEC[T.Frequently]\n-8.2402\n2.312\n-3.564\n0.000\n-12.771\n-3.709\n\n\nCAEC[T.Sometimes]\n-6.2226\n2.232\n-2.787\n0.005\n-10.598\n-1.847\n\n\nCAEC[T.no]\n-8.5977\n2.889\n-2.976\n0.003\n-14.260\n-2.935\n\n\nSMOKE[T.yes]\n4.4919\n3.115\n1.442\n0.149\n-1.614\n10.598\n\n\nSCC[T.yes]\n-0.7294\n1.447\n-0.504\n0.614\n-3.565\n2.106\n\n\nCALC[T.Frequently]\n-12.6192\n3.25e+05\n-3.89e-05\n1.000\n-6.36e+05\n6.36e+05\n\n\nCALC[T.Sometimes]\n-13.2985\n3.25e+05\n-4.1e-05\n1.000\n-6.36e+05\n6.36e+05\n\n\nCALC[T.no]\n-14.1585\n3.25e+05\n-4.36e-05\n1.000\n-6.36e+05\n6.36e+05\n\n\nMTRANS[T.Bike]\n15.8909\n2489.580\n0.006\n0.995\n-4863.596\n4895.378\n\n\nMTRANS[T.Motorbike]\n3.9944\n47.659\n0.084\n0.933\n-89.416\n97.405\n\n\nMTRANS[T.Public_Transportation]\n4.4914\n0.995\n4.514\n0.000\n2.541\n6.441\n\n\nMTRANS[T.Walking]\n4.3554\n1.502\n2.900\n0.004\n1.412\n7.299\n\n\nAge\n0.3721\n0.097\n3.833\n0.000\n0.182\n0.562\n\n\nHeight\n-14.4208\n4.118\n-3.502\n0.000\n-22.492\n-6.349\n\n\nWeight\n1.0786\n0.146\n7.378\n0.000\n0.792\n1.365\n\n\nFCVC\n-0.7754\n0.429\n-1.806\n0.071\n-1.617\n0.066\n\n\nNCP\n-1.7094\n0.491\n-3.480\n0.001\n-2.672\n-0.747\n\n\nCH2O\n-1.7291\n0.578\n-2.992\n0.003\n-2.862\n-0.596\n\n\nFAF\n-0.1924\n0.280\n-0.688\n0.491\n-0.740\n0.356\n\n\nTUE\n-0.9320\n0.456\n-2.043\n0.041\n-1.826\n-0.038\n\n\nNObeyesdad=2\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n17.4309\nnan\nnan\nnan\nnan\nnan\n\n\nGender[T.Male]\n-14.0384\n1.983\n-7.079\n0.000\n-17.925\n-10.151\n\n\nFamil_Hist_Owt[T.yes]\n2.0527\n1.717\n1.195\n0.232\n-1.313\n5.418\n\n\nFAVC[T.yes]\n0.9668\n1.752\n0.552\n0.581\n-2.468\n4.401\n\n\nCAEC[T.Frequently]\n-10.0052\n4.352\n-2.299\n0.021\n-18.534\n-1.476\n\n\nCAEC[T.Sometimes]\n-1.0074\n3.427\n-0.294\n0.769\n-7.724\n5.709\n\n\nCAEC[T.no]\n-0.4896\n894.479\n-0.001\n1.000\n-1753.637\n1752.658\n\n\nSMOKE[T.yes]\n8.1410\n4.013\n2.029\n0.042\n0.277\n16.005\n\n\nSCC[T.yes]\n-7.6940\n152.983\n-0.050\n0.960\n-307.535\n292.147\n\n\nCALC[T.Frequently]\n-2.4516\nnan\nnan\nnan\nnan\nnan\n\n\nCALC[T.Sometimes]\n-7.5316\nnan\nnan\nnan\nnan\nnan\n\n\nCALC[T.no]\n-7.2301\nnan\nnan\nnan\nnan\nnan\n\n\nMTRANS[T.Bike]\n-11.9350\n8.09e+07\n-1.47e-07\n1.000\n-1.59e+08\n1.59e+08\n\n\nMTRANS[T.Motorbike]\n10.9226\n48.493\n0.225\n0.822\n-84.123\n105.968\n\n\nMTRANS[T.Public_Transportation]\n11.1756\n1.750\n6.387\n0.000\n7.746\n14.605\n\n\nMTRANS[T.Walking]\n1.7281\n2.759\n0.626\n0.531\n-3.679\n7.135\n\n\nAge\n0.8111\n0.132\n6.139\n0.000\n0.552\n1.070\n\n\nHeight\n-184.0385\n14.746\n-12.481\n0.000\n-212.939\n-155.138\n\n\nWeight\n3.9438\n0.288\n13.688\n0.000\n3.379\n4.508\n\n\nFCVC\n0.8915\n1.014\n0.879\n0.379\n-1.095\n2.878\n\n\nNCP\n-1.1415\n0.711\n-1.605\n0.109\n-2.536\n0.253\n\n\nCH2O\n-1.5390\n0.876\n-1.756\n0.079\n-3.256\n0.179\n\n\nFAF\n-1.5295\n0.591\n-2.586\n0.010\n-2.689\n-0.370\n\n\nTUE\n-0.5710\n0.840\n-0.680\n0.497\n-2.217\n1.075\n\n\nNObeyesdad=3\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-138.5068\n1.47e+07\n-9.41e-06\n1.000\n-2.89e+07\n2.89e+07\n\n\nGender[T.Male]\n-16.6365\n8.279\n-2.010\n0.044\n-32.863\n-0.410\n\n\nFamil_Hist_Owt[T.yes]\n2.3538\n11.601\n0.203\n0.839\n-20.384\n25.092\n\n\nFAVC[T.yes]\n-8.7785\n5.476\n-1.603\n0.109\n-19.512\n1.955\n\n\nCAEC[T.Frequently]\n-71.7022\nnan\nnan\nnan\nnan\nnan\n\n\nCAEC[T.Sometimes]\n-3.9034\n4.734\n-0.824\n0.410\n-13.183\n5.376\n\n\nCAEC[T.no]\n7.7265\n895.063\n0.009\n0.993\n-1746.566\n1762.019\n\n\nSMOKE[T.yes]\n3.5306\n19.342\n0.183\n0.855\n-34.379\n41.440\n\n\nSCC[T.yes]\n-19.4879\n154.607\n-0.126\n0.900\n-322.512\n283.536\n\n\nCALC[T.Frequently]\n-43.6020\n1.48e+07\n-2.95e-06\n1.000\n-2.9e+07\n2.9e+07\n\n\nCALC[T.Sometimes]\n-45.7496\n1.47e+07\n-3.11e-06\n1.000\n-2.88e+07\n2.88e+07\n\n\nCALC[T.no]\n-28.2183\n1.43e+07\n-1.97e-06\n1.000\n-2.81e+07\n2.81e+07\n\n\nMTRANS[T.Bike]\n0.0376\nnan\nnan\nnan\nnan\nnan\n\n\nMTRANS[T.Motorbike]\n-2.3812\n1.05e+11\n-2.27e-11\n1.000\n-2.06e+11\n2.06e+11\n\n\nMTRANS[T.Public_Transportation]\n22.5234\n6.664\n3.380\n0.001\n9.463\n35.584\n\n\nMTRANS[T.Walking]\n-5.3334\n33.279\n-0.160\n0.873\n-70.560\n59.893\n\n\nAge\n2.5106\n0.964\n2.605\n0.009\n0.621\n4.400\n\n\nHeight\n-278.9439\n44.201\n-6.311\n0.000\n-365.576\n-192.312\n\n\nWeight\n7.1539\n1.394\n5.132\n0.000\n4.422\n9.886\n\n\nFCVC\n4.1064\n3.285\n1.250\n0.211\n-2.333\n10.546\n\n\nNCP\n-1.5637\n2.424\n-0.645\n0.519\n-6.315\n3.187\n\n\nCH2O\n-13.4088\n5.560\n-2.412\n0.016\n-24.306\n-2.511\n\n\nFAF\n-9.8534\n4.356\n-2.262\n0.024\n-18.390\n-1.316\n\n\nTUE\n-5.6951\n3.292\n-1.730\n0.084\n-12.147\n0.757\n\n\nNObeyesdad=4\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-87.3214\nnan\nnan\nnan\nnan\nnan\n\n\nGender[T.Male]\n-200.3037\n5.41e+07\n-3.7e-06\n1.000\n-1.06e+08\n1.06e+08\n\n\nFamil_Hist_Owt[T.yes]\n-30.9252\nnan\nnan\nnan\nnan\nnan\n\n\nFAVC[T.yes]\n-53.1818\n3.98e+07\n-1.34e-06\n1.000\n-7.8e+07\n7.8e+07\n\n\nCAEC[T.Frequently]\n-28.5483\nnan\nnan\nnan\nnan\nnan\n\n\nCAEC[T.Sometimes]\n-21.5821\n5.38e+07\n-4.01e-07\n1.000\n-1.05e+08\n1.05e+08\n\n\nCAEC[T.no]\n-2.2000\n4.62e+29\n-4.76e-30\n1.000\n-9.06e+29\n9.06e+29\n\n\nSMOKE[T.yes]\n-6.0944\nnan\nnan\nnan\nnan\nnan\n\n\nSCC[T.yes]\n-12.3054\nnan\nnan\nnan\nnan\nnan\n\n\nCALC[T.Frequently]\n-6.2460\nnan\nnan\nnan\nnan\nnan\n\n\nCALC[T.Sometimes]\n-37.2004\n2.12e+08\n-1.76e-07\n1.000\n-4.15e+08\n4.15e+08\n\n\nCALC[T.no]\n-64.5032\nnan\nnan\nnan\nnan\nnan\n\n\nMTRANS[T.Bike]\n-0.2989\n1.92e+53\n-1.56e-54\n1.000\n-3.76e+53\n3.76e+53\n\n\nMTRANS[T.Motorbike]\n-0.2031\nnan\nnan\nnan\nnan\nnan\n\n\nMTRANS[T.Public_Transportation]\n-57.6854\n7.04e+07\n-8.2e-07\n1.000\n-1.38e+08\n1.38e+08\n\n\nMTRANS[T.Walking]\n-7.4464\n2.03e+15\n-3.66e-15\n1.000\n-3.98e+15\n3.98e+15\n\n\nAge\n-9.3747\n103.246\n-0.091\n0.928\n-211.733\n192.984\n\n\nHeight\n-174.4727\n592.866\n-0.294\n0.769\n-1336.469\n987.523\n\n\nWeight\n8.7405\n35.222\n0.248\n0.804\n-60.293\n77.774\n\n\nFCVC\n49.0613\n3.02e+04\n0.002\n0.999\n-5.91e+04\n5.92e+04\n\n\nNCP\n2.3650\n4572.743\n0.001\n1.000\n-8960.047\n8964.777\n\n\nCH2O\n-18.5809\n34.347\n-0.541\n0.589\n-85.900\n48.738\n\n\nFAF\n-65.1761\n262.887\n-0.248\n0.804\n-580.424\n450.072\n\n\nTUE\n-44.3721\n285.217\n-0.156\n0.876\n-603.387\n514.643\n\n\nNObeyesdad=5\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-12.5683\n3.25e+05\n-3.87e-05\n1.000\n-6.36e+05\n6.36e+05\n\n\nGender[T.Male]\n-6.8149\n1.085\n-6.282\n0.000\n-8.941\n-4.689\n\n\nFamil_Hist_Owt[T.yes]\n-0.5822\n0.790\n-0.737\n0.461\n-2.130\n0.966\n\n\nFAVC[T.yes]\n2.6008\n0.978\n2.660\n0.008\n0.684\n4.517\n\n\nCAEC[T.Frequently]\n-7.2298\n2.507\n-2.884\n0.004\n-12.143\n-2.316\n\n\nCAEC[T.Sometimes]\n-2.8197\n2.413\n-1.168\n0.243\n-7.550\n1.910\n\n\nCAEC[T.no]\n-3.8181\n3.143\n-1.215\n0.224\n-9.977\n2.341\n\n\nSMOKE[T.yes]\n3.1451\n3.296\n0.954\n0.340\n-3.314\n9.604\n\n\nSCC[T.yes]\n2.1647\n1.617\n1.339\n0.181\n-1.004\n5.334\n\n\nCALC[T.Frequently]\n-9.0315\n3.25e+05\n-2.78e-05\n1.000\n-6.36e+05\n6.36e+05\n\n\nCALC[T.Sometimes]\n-9.1446\n3.25e+05\n-2.82e-05\n1.000\n-6.36e+05\n6.36e+05\n\n\nCALC[T.no]\n-10.7708\n3.25e+05\n-3.32e-05\n1.000\n-6.36e+05\n6.36e+05\n\n\nMTRANS[T.Bike]\n19.0425\n2489.581\n0.008\n0.994\n-4860.446\n4898.531\n\n\nMTRANS[T.Motorbike]\n1.6235\n47.716\n0.034\n0.973\n-91.899\n95.146\n\n\nMTRANS[T.Public_Transportation]\n5.9777\n1.209\n4.946\n0.000\n3.609\n8.346\n\n\nMTRANS[T.Walking]\n4.3596\n1.776\n2.454\n0.014\n0.878\n7.841\n\n\nAge\n0.4878\n0.106\n4.597\n0.000\n0.280\n0.696\n\n\nHeight\n-50.0157\n6.721\n-7.442\n0.000\n-63.188\n-36.844\n\n\nWeight\n1.7920\n0.168\n10.651\n0.000\n1.462\n2.122\n\n\nFCVC\n-0.8369\n0.601\n-1.393\n0.164\n-2.014\n0.341\n\n\nNCP\n-1.4453\n0.554\n-2.608\n0.009\n-2.531\n-0.359\n\n\nCH2O\n-1.7648\n0.679\n-2.601\n0.009\n-3.095\n-0.435\n\n\nFAF\n-0.5613\n0.374\n-1.499\n0.134\n-1.295\n0.172\n\n\nTUE\n-0.7982\n0.555\n-1.439\n0.150\n-1.886\n0.289\n\n\nNObeyesdad=6\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-2.1693\n6.28e+06\n-3.45e-07\n1.000\n-1.23e+07\n1.23e+07\n\n\nGender[T.Male]\n-6.6857\n1.207\n-5.537\n0.000\n-9.052\n-4.319\n\n\nFamil_Hist_Owt[T.yes]\n1.9296\n1.076\n1.793\n0.073\n-0.179\n4.038\n\n\nFAVC[T.yes]\n-0.4617\n1.141\n-0.405\n0.686\n-2.698\n1.775\n\n\nCAEC[T.Frequently]\n-5.5324\n3.264\n-1.695\n0.090\n-11.930\n0.866\n\n\nCAEC[T.Sometimes]\n0.7854\n3.044\n0.258\n0.796\n-5.181\n6.752\n\n\nCAEC[T.no]\n1.7141\n3.934\n0.436\n0.663\n-5.997\n9.426\n\n\nSMOKE[T.yes]\n7.0398\n3.570\n1.972\n0.049\n0.043\n14.036\n\n\nSCC[T.yes]\n1.3664\n2.012\n0.679\n0.497\n-2.577\n5.309\n\n\nCALC[T.Frequently]\n-2.1001\n6.28e+06\n-3.34e-07\n1.000\n-1.23e+07\n1.23e+07\n\n\nCALC[T.Sometimes]\n-4.6772\n6.28e+06\n-7.45e-07\n1.000\n-1.23e+07\n1.23e+07\n\n\nCALC[T.no]\n-4.1972\n6.28e+06\n-6.68e-07\n1.000\n-1.23e+07\n1.23e+07\n\n\nMTRANS[T.Bike]\n-21.8420\n6.54e+09\n-3.34e-09\n1.000\n-1.28e+10\n1.28e+10\n\n\nMTRANS[T.Motorbike]\n3.2252\n47.781\n0.068\n0.946\n-90.423\n96.873\n\n\nMTRANS[T.Public_Transportation]\n8.8055\n1.416\n6.219\n0.000\n6.030\n11.581\n\n\nMTRANS[T.Walking]\n1.2540\n2.256\n0.556\n0.578\n-3.168\n5.676\n\n\nAge\n0.7030\n0.116\n6.086\n0.000\n0.477\n0.929\n\n\nHeight\n-104.6838\n9.021\n-11.605\n0.000\n-122.364\n-87.003\n\n\nWeight\n2.6259\n0.190\n13.819\n0.000\n2.253\n2.998\n\n\nFCVC\n0.1776\n0.764\n0.232\n0.816\n-1.320\n1.675\n\n\nNCP\n-1.8276\n0.608\n-3.007\n0.003\n-3.019\n-0.636\n\n\nCH2O\n-1.8930\n0.757\n-2.502\n0.012\n-3.376\n-0.410\n\n\nFAF\n-1.0280\n0.438\n-2.347\n0.019\n-1.887\n-0.169\n\n\nTUE\n0.1282\n0.670\n0.191\n0.848\n-1.186\n1.442\n\n\n\n\n\n\n\n\n\nEvaluating Model Performance\nTo gauge our models’ effectiveness, we’ll employ various metrics such as accuracy, precision, recall, and F1-score. A confusion matrix will help visualize how well our models perform in classifying outcomes—think of it as a report card for your model!\n\n\nEvaluating the Logit model\n# Predict on test data\nbase_preds = logit_model.predict(data_df_test).idxmax(axis=1)\ny_test = data_df_test[target]\n\n# Evaluate the model\naccuracy_orig = accuracy_score(y_test, base_preds)\nreport_orig = classification_report(y_test, base_preds)\n\nprint(\"Accuracy:\", accuracy_orig)\nprint(\"Classification Report:\")\nprint(report_orig)\n\n\nAccuracy: 0.909952606635071\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.93      0.86      0.89        29\n           1       0.86      0.83      0.84        29\n           2       0.95      0.91      0.93        45\n           3       0.94      0.97      0.95        31\n           4       1.00      0.96      0.98        27\n           5       0.83      0.90      0.86        21\n           6       0.84      0.93      0.89        29\n\n    accuracy                           0.91       211\n   macro avg       0.91      0.91      0.91       211\nweighted avg       0.91      0.91      0.91       211\n\n\n\n\n\n\nLooking for some Improvments!\n\nFeature Selection Using CrossTab Sparsity\nNow comes the exciting part—using CrossTab Sparsity to refine our feature selection process! It’s like cleaning up your closet and only keeping the clothes that spark joy (thank you, Marie Kondo). 1\n1 This is based on work in Unique Metric for Health Analysis with Optimization of Clustering Activity and Cross Comparison of Results from Different Approach. Paper LinkCode is here!\n\n\nStandared Steps for Feature Selection\n\nCalculate CrossTab Sparsity: For each feature against the target variable.\nSelect Features: Based on sparsity scores that indicate significant interactions with the target variable.\nRecreate Models: Train new models using only the selected features—less is often more!\n\nHere we go!!!\n\n\n\nDoing what needs to Done Code ;)\nsns.set_style(\"white\")\nsns.set_context(\"paper\")\n# Calculating Crostab sparsity for each Column\nresults = crosstab_sparsity(data_df_train.iloc[:,:-1],data_df_train[target],numeric_bin='decile')\n\n# presenting results for consumption\ndf_long = pd.melt(results['scores'], id_vars=['Columns'], value_vars=['seggregation', 'explaination', 'metric'],\n                  var_name='Metric', value_name='values')\n\n# Adding jitter: small random noise to 'Columns' (x-axis)\n# df_long['values_jittered'] = df_long['Value'] + np.random.uniform(-0.1, 0.1, size=len(df_long))\n\n# Create a seaborn scatter plot with jitter, more professional color palette, and transparency\nplt.figure(figsize=(12, 5))\nsns.scatterplot(x='Columns', y='values', hue='Metric', style='Metric',\n        data=df_long, s=100, alpha=0.7, palette='deep')\n\n# Title and labels\nplt.title('Metrics by Columns', fontsize=16)\nplt.xticks(rotation=45) \nplt.xlabel('Columns', fontsize=10)\nplt.ylabel('Value', fontsize=10)\n\n# Display legend outside the plot for better readability\nplt.legend(title='Metric', loc='upper right', fancybox=True, framealpha=0.5)\n\n# Show the plot\nplt.tight_layout()\nplt.show()\n\n\nCSP calculated with decile for breaks!\n\nScores for 7 groups(s) is : 140.96057955229762\n\n\n\n\n\n\n\n\n\n\n\n\nAnd Drum Rolls pelase!!!\nUsing just top 5 varaibles we are getting almost similar or better overall accuracy. This amounts to greatly simplifing the models and clearly explain why some variable are not useful for modeling.\n\n\nAnd finally training and evaluating with drum rolls\nlogit_model_rev = sm.MNLogit.from_formula(f\"{target} ~ {' + '.join(results['scores'].loc[:5,'Columns'].values)}\", \n    data=data_df_train\n).fit_regularized()\n\n# Predict on test data\nchallenger_preds = logit_model_rev.predict(data_df_test).idxmax(axis=1)\ny_test = data_df_test[target]\n\n# Evaluate the model\naccuracy_new = accuracy_score(y_test, challenger_preds)\nreport_new = classification_report(y_test, challenger_preds)\n\nprint(\"Accuracy:\", accuracy_new)\nprint(\"Classification Report:\")\nprint(report_new)\n\n\nSingular matrix E in LSQ subproblem    (Exit mode 5)\n            Current function value: nan\n            Iterations: 470\n            Function evaluations: 1227\n            Gradient evaluations: 470\nAccuracy: 0.9383886255924171\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.93      0.97      0.95        29\n           1       0.93      0.93      0.93        29\n           2       0.96      1.00      0.98        45\n           3       0.93      0.90      0.92        31\n           4       0.93      0.93      0.93        27\n           5       0.90      0.90      0.90        21\n           6       0.96      0.90      0.93        29\n\n    accuracy                           0.94       211\n   macro avg       0.94      0.93      0.93       211\nweighted avg       0.94      0.94      0.94       211\n\n\n\n/home/jitin/Documents/applications/perceptions/.venv/lib/python3.12/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n  warnings.warn(\"Maximum Likelihood optimization failed to \"\n\n\n\n\n\n\n\n\n\n\nNoneSummary of retrained model\n\n\n\n\n\n\n\nCode\ndisplay(logit_model_rev.summary())\n\n\n\nMNLogit Regression Results\n\n\nDep. Variable:\nNObeyesdad\nNo. Observations:\n1900\n\n\nModel:\nMNLogit\nDf Residuals:\n1858\n\n\nMethod:\nMLE\nDf Model:\n36\n\n\nDate:\nSun, 07 Dec 2025\nPseudo R-squ.:\nnan\n\n\nTime:\n16:57:01\nLog-Likelihood:\nnan\n\n\nconverged:\nFalse\nLL-Null:\n-3691.8\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\nnan\n\n\n\n\n\n\n\n\nNObeyesdad=1\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n58.1248\nnan\nnan\nnan\nnan\nnan\n\n\nTUE\n0.1130\nnan\nnan\nnan\nnan\nnan\n\n\nCH2O\n-0.8634\nnan\nnan\nnan\nnan\nnan\n\n\nFAF\n0.1425\nnan\nnan\nnan\nnan\nnan\n\n\nAge\n0.0579\nnan\nnan\nnan\nnan\nnan\n\n\nHeight\n-76.5735\nnan\nnan\nnan\nnan\nnan\n\n\nWeight\n1.3337\nnan\nnan\nnan\nnan\nnan\n\n\nNObeyesdad=2\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n328.4616\nnan\nnan\nnan\nnan\nnan\n\n\nTUE\n2.2275\nnan\nnan\nnan\nnan\nnan\n\n\nCH2O\n-1.4150\nnan\nnan\nnan\nnan\nnan\n\n\nFAF\n-1.3585\nnan\nnan\nnan\nnan\nnan\n\n\nAge\n0.1537\nnan\nnan\nnan\nnan\nnan\n\n\nHeight\n-426.3945\nnan\nnan\nnan\nnan\nnan\n\n\nWeight\n5.3584\nnan\nnan\nnan\nnan\nnan\n\n\nNObeyesdad=3\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n306.6447\nnan\nnan\nnan\nnan\nnan\n\n\nTUE\n-7.8630\nnan\nnan\nnan\nnan\nnan\n\n\nCH2O\n-21.0118\nnan\nnan\nnan\nnan\nnan\n\n\nFAF\n-11.3624\nnan\nnan\nnan\nnan\nnan\n\n\nAge\n2.4017\nnan\nnan\nnan\nnan\nnan\n\n\nHeight\n-710.3867\nnan\nnan\nnan\nnan\nnan\n\n\nWeight\n10.1072\nnan\nnan\nnan\nnan\nnan\n\n\nNObeyesdad=4\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n352.4249\nnan\nnan\nnan\nnan\nnan\n\n\nTUE\n-9.2469\nnan\nnan\nnan\nnan\nnan\n\n\nCH2O\n-20.6780\nnan\nnan\nnan\nnan\nnan\n\n\nFAF\n-14.7525\nnan\nnan\nnan\nnan\nnan\n\n\nAge\n2.1487\nnan\nnan\nnan\nnan\nnan\n\n\nHeight\n-758.2318\nnan\nnan\nnan\nnan\nnan\n\n\nWeight\n10.5011\nnan\nnan\nnan\nnan\nnan\n\n\nNObeyesdad=5\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n126.2892\nnan\nnan\nnan\nnan\nnan\n\n\nTUE\n0.5832\nnan\nnan\nnan\nnan\nnan\n\n\nCH2O\n-0.8764\nnan\nnan\nnan\nnan\nnan\n\n\nFAF\n-0.1920\nnan\nnan\nnan\nnan\nnan\n\n\nAge\n0.0719\nnan\nnan\nnan\nnan\nnan\n\n\nHeight\n-160.2982\nnan\nnan\nnan\nnan\nnan\n\n\nWeight\n2.3663\nnan\nnan\nnan\nnan\nnan\n\n\nNObeyesdad=6\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n207.3760\nnan\nnan\nnan\nnan\nnan\n\n\nTUE\n1.6561\nnan\nnan\nnan\nnan\nnan\n\n\nCH2O\n-0.6583\nnan\nnan\nnan\nnan\nnan\n\n\nFAF\n-0.1243\nnan\nnan\nnan\nnan\nnan\n\n\nAge\n0.1042\nnan\nnan\nnan\nnan\nnan\n\n\nHeight\n-266.6050\nnan\nnan\nnan\nnan\nnan\n\n\nWeight\n3.6160\nnan\nnan\nnan\nnan\nnan\n\n\n\n\n\n\n\n\n\n\n\nImpact on Model Accuracy\nAfter applying feature selection based on CrossTab Sparsity, we’ll compare the accuracy of our new models against our baseline models. This comparison will reveal how effectively CrossTab Sparsity enhances classification performance.\n\nResults and Discussion: Unveiling Insights\nModel Comparison Table\nAfter implementing CrossTab Sparsity in our feature selection process, let’s take a look at the results:\n\n\nComparision Code\nmetrics = {\n    \"Metric\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"],\n    \"Baseline Model with all Parameters\": [\n        accuracy_score(y_test, base_preds),\n        precision_score(y_test, base_preds, average='weighted'),\n        recall_score(y_test, base_preds, average='weighted'),\n        f1_score(y_test, base_preds, average='weighted'),\n    ],\n    \"Challenger Model with only 5 Variables\": [\n        accuracy_score(y_test, challenger_preds),\n        precision_score(y_test, challenger_preds, average='weighted'),\n        recall_score(y_test, challenger_preds, average='weighted'),\n        f1_score(y_test, challenger_preds, average='weighted'),\n    ]\n}\ndisplay(pd.DataFrame(metrics).round(4).set_index('Metric').T)\n\n\n\n\n\n\n\n\nMetric\nAccuracy\nPrecision\nRecall\nF1-Score\n\n\n\n\nBaseline Model with all Parameters\n0.9100\n0.9123\n0.9100\n0.9103\n\n\nChallenger Model with only 5 Variables\n0.9384\n0.9384\n0.9384\n0.9381\n\n\n\n\n\n\n\nInsights Gained\nThrough this analysis, several key insights emerge:\n\n\nReduction of similar accuracy from 16 to 5 i.e 68.75% reduction\n\n\n\nFeature Interactions Matter: The selected features based on CrossTab Sparsity significantly improved model accuracy—like finding out which ingredients make your favorite dish even better!\nSimplicity is Key: By focusing on relevant features, we enhance accuracy while simplifying model interpretation—because nobody likes unnecessary complexity.\nReal-World Applications: These findings have practical implications in fields such as environmental science where classification plays a critical role—helping us make better decisions for our planet.\n\n\n\n\nConclusion: The Road Ahead\nIn conclusion, this blog has illustrated how CrossTab Sparsity can be a game-changer in classification tasks using the Obesity dataset. By leveraging this metric for feature selection, we achieved notable improvements in model performance—proof that sometimes less really is more!\nFuture Work: Expanding Horizons\nAs we look ahead, there are exciting avenues to explore:\n\nInvestigating regression problems using CrossTab Sparsity.\nComparing its effectiveness with other feature selection methods such as Recursive Feature Elimination (RFE) or comparision with other feature selection mehtods.\n\nBy continuing this journey into data science, we not only enhance our technical skills but also contribute valuable insights that can drive meaningful change in various industries."
  },
  {
    "objectID": "bootcamp.html",
    "href": "bootcamp.html",
    "title": "Build Your Personal AI Operating System",
    "section": "",
    "text": "FOUNDING COHORT\nDecember 19-21, 2025\n\n\n\n\nFORMAT\n3 Days - Live - 7 PM - 10 PM IST\n\n\n\n\nDELIVERY\nVirtual + Lifetime Recordings\n\n\n\n\nCLASS SIZE\nMaximum 20 Participants"
  },
  {
    "objectID": "bootcamp.html#the-market-isnt-waiting-for-you-to-catch-up",
    "href": "bootcamp.html#the-market-isnt-waiting-for-you-to-catch-up",
    "title": "Build Your Personal AI Operating System",
    "section": "The Market Isn’t Waiting for You to Catch Up",
    "text": "The Market Isn’t Waiting for You to Catch Up\n\nYou’re managing $80M portfolios.\nYou’re leading teams of 50+ people.\nYou’re making decisions that move markets.\n\nBut you’re still doing manual data entry.\nMeanwhile, a 22-year-old with ChatGPT is doing your job faster.\nThe gap is widening.\nSenior professionals are being replaced by efficient systems. Not because they lack expertise. Because they lack operational leverage.\nStop watching. Start building."
  },
  {
    "objectID": "bootcamp.html#this-training-is-for-you-if",
    "href": "bootcamp.html#this-training-is-for-you-if",
    "title": "Build Your Personal AI Operating System",
    "section": "This Training Is For You If:",
    "text": "This Training Is For You If:\n\n\nYou’re a senior professional (10+ years experience) managing significant portfolios or P&L\n\nYou’re drowning in repetitive work that AI could handle, but don’t know where to start\n\nYou’re afraid of obsolescence as younger, AI-fluent professionals move faster\n\nYou’ve tried ChatGPT but can’t scale it beyond simple tasks into real operational leverage\n\nYou manage teams and need to 10x their output without 10x-ing headcount\n\nYou’re willing to invest 3 days to build a system that saves 10+ hours every week"
  },
  {
    "objectID": "bootcamp.html#this-training-is-not-for-you-if",
    "href": "bootcamp.html#this-training-is-not-for-you-if",
    "title": "Build Your Personal AI Operating System",
    "section": "This Training Is NOT For You If:",
    "text": "This Training Is NOT For You If:\n\n\nYou’re new to your field (less than 5 years experience)\nYou want to “learn AI” for your resume without implementing anything\nYou’re looking for passive income schemes or get-rich-quick tactics\nYou expect AI to do 100% of your job (it won’t)\nYou’re not willing to invest 2-3 hours per week maintaining your AI systems\nYou want theory and certifications, not practical operational systems"
  },
  {
    "objectID": "bootcamp.html#proof-of-concept-the-grocery-chain-story",
    "href": "bootcamp.html#proof-of-concept-the-grocery-chain-story",
    "title": "Build Your Personal AI Operating System",
    "section": "Proof of Concept: The Grocery Chain Story",
    "text": "Proof of Concept: The Grocery Chain Story\n\nThe Context: A regional grocery chain with ₹50Cr annual revenue, operating 15 stores across 3 cities.\nThe “Before” (Ambiguity & Pain): - Lost 15% of inventory to spoilage (₹7.5Cr annual loss) - Manual ordering took 20 hours per week across all stores - No predictive capability - constantly over-ordering perishables - Supply chain decisions based on “gut feel” from 30-year veterans The AI-ART Intervention: We installed a Predictive Ordering Agent using simple Excel + OpenAI API (no coding, no data scientists, no million-dollar platforms).\nThe system: - Analyzed 3 years of sales data (already sitting in Excel) - Factored in weather patterns, local events, seasonality - Generated daily ordering recommendations per store - Learned from feedback (what sold, what spoiled)\nThe “After” (ROI): - Spoilage reduced by 92% (saving ₹6.9Cr annually) - Ordering time reduced from 20 hours/week to 2 hours/week - Profit margins increased by ₹8.2Cr in first year - Total investment: ₹45,000 (including our training) ROI: 182x in Year 1.\n\nThis isn’t a unicorn story. This is what happens when senior professionals apply operational AI thinking to their existing expertise.\nYou don’t need a PhD. You need a system."
  },
  {
    "objectID": "bootcamp.html#before-this-training-after-this-training",
    "href": "bootcamp.html#before-this-training-after-this-training",
    "title": "Build Your Personal AI Operating System",
    "section": "Before This Training → After This Training",
    "text": "Before This Training → After This Training\n\n\nBefore\n\nSpending 20+ hours/week on repetitive analysis\nDrowning in emails, reports, data entry\nCan’t scale yourself without hiring more people\nWatching younger professionals move faster with AI\nScared of being replaced by automation\nUsing ChatGPT for one-off tasks with no system\nNo operational leverage beyond your own hours\n\n\n\nAfter\n\nAI Profit OS handles 40% of repetitive workflow\nEmail processing, report generation, data analysis automated\n10x team output without 10x headcount\nYou’re the AI-fluent leader others come to\nYou’re building the systems, not being replaced by them\nDeep-Research Prompt System producing insights in minutes\nOperational leverage: Your decisions scale 24/7"
  },
  {
    "objectID": "bootcamp.html#what-youll-build-in-3-days-the-ai-profit-os",
    "href": "bootcamp.html#what-youll-build-in-3-days-the-ai-profit-os",
    "title": "Build Your Personal AI Operating System",
    "section": "What You’ll Build in 3 Days: The AI Profit OS",
    "text": "What You’ll Build in 3 Days: The AI Profit OS\n\n\nDay 1: OBJECTIVE — Your 90-Day AI Roadmap\nMorning: The AI Maturity Assessment - Map your current workflow: Where are you spending 80% of your time? - Identify automation opportunities using the OLCD diagnostic framework - Calculate your “time ROI” - what’s actually worth automating first? - Build your personalized 90-day AI implementation roadmap\nAfternoon: Quick Wins Workshop - Install your first AI agent (email processing or report generation) - See immediate time savings (2-5 hours/week from Day 1) - Understand the “AI-ART Matrix” - why most AI projects fail to scale - Learn the “Grocery Store Framework” - how to think about AI operationally\nDeliverable: Your 90-Day AI Roadmap + First AI agent deployed\n\n\nDay 2: LEARNER — The Deep-Research Prompt System\nMorning: The Research Multiplier - Build your Deep-Research Prompt System (the Excel-based framework) - How to extract insights from data you already have - Pattern recognition across your domain (using AI to spot trends you’d miss) - Case study: How I saved ₹6.9Cr for the grocery chain using this system\nAfternoon: The Data Asset Library - Create your personal “Data Asset Library” template - Learn to structure your knowledge for AI consumption - Build your “Second Brain” that actually works (not just another note-taking system) - How to train AI on your specific domain expertise\nDeliverable: Deep-Research Prompt System + Data Asset Library template\n\n\nDay 3: CONTROLLER & DIAGNOSTIC — Governance + ROI Calculator\nMorning: AI Governance Playbooks\n- How to ensure AI output quality (avoid garbage-in-garbage-out) - Build review systems that catch AI mistakes before they matter - Team training: How to get your team using AI without breaking things - The “Human-in-the-Loop” framework for high-stakes decisions\nAfternoon: Your Custom ROI Calculator - Build your AI ROI calculator (track time saved, output increased, revenue impact) - Measure what matters: Operational leverage, not vanity metrics - The “10-Hour Rule” - how to save minimum 10 hours/week with AI - Final Q&A: Troubleshooting your specific implementation challenges\nDeliverable: AI Governance Playbook + Custom ROI Calculator + Complete AI Profit OS"
  },
  {
    "objectID": "bootcamp.html#meet-your-instructor-jitin-kapila",
    "href": "bootcamp.html#meet-your-instructor-jitin-kapila",
    "title": "Build Your Personal AI Operating System",
    "section": "Meet Your Instructor: Jitin Kapila",
    "text": "Meet Your Instructor: Jitin Kapila\n\n\nEx-Fortune 500 AI Lead. Managed $80M+ Portfolios.\n“I don’t teach theory. I build enterprise-grade systems.”\nBackground: - 14+ years architecting AI solutions for Fortune 500 enterprises - Managed portfolios exceeding $80M across automotive, retail, FMCG, telecommunications - Mechanical Engineering + Applied Mathematics background - Not a career educator - a practitioner who teaches\nTrack Record: - ₹6.9Cr annual savings (grocery chain spoilage reduction) - $11M operational optimization (automotive inventory system) - £80K/month revenue lift (retail predictive forecasting) - 45,000-unit inventory reduction (supply chain optimization)\nPhilosophy: Senior professionals don’t need more theory. They need operational systems that create leverage. The AI Profit OS isn’t a course - it’s a toolkit you’ll use every day for the next decade."
  },
  {
    "objectID": "bootcamp.html#what-senior-professionals-are-saying",
    "href": "bootcamp.html#what-senior-professionals-are-saying",
    "title": "Build Your Personal AI Operating System",
    "section": "What Senior Professionals Are Saying",
    "text": "What Senior Professionals Are Saying\n\n\n“I had the pleasure of working with Jitin for over two years (2023–2025) on a high-intensity AI program that was both technically complex and highly impactful for the business. Jitin played a pivotal role not only in experimenting with and developing the AI-driven forecasting solution but also in serving as an AI coach for end users, helping them understand and trust the forecasts produced by the model(s).\nHis deep data science expertise, combined with his ability to translate technical concepts into clear business language, made a remarkable difference in co-creating a truly futuristic AI product.”\n— Nikhil Jain, Head of Tech Accelerator & AI Innovation Client engagement: 2023-2025\n\n\n“Jitin has the profound expertise of advanced level of R and Python. Many people at Harman Connected Services find his enthusiasm and dedication both inspiring and motivating. He showed a high level of command on Data Science, Machine Learning techniques and was a valuable contributor to our projects. He learns quickly and I would have no hesitation in working with Jitin once again in the future.”\n— Aasheesh Barvey, Data-Informed Decision-Making & Advanced Analytics Senior Manager, Strategy & Project Management\n\n\n“Jitin Kapila is a transformed data scientist with huge enthusiasm to learn new things. I have seen him growing from level to level in data science. Clarity of thoughts and out of the box thinking comes naturally to him.\nHe is a master class coding hand and can decompose any hard problem into simple logic and quickly build a scalable and ready to implement solution. I would like to have him any day any time working with me.”\n— Kumarjit Pathak, Global AI & Data Science Leader @ ABInBev Daniel H. Wagner Prize Winner (INFORMS 2022) | Visionary Innovator"
  },
  {
    "objectID": "bootcamp.html#everything-included-in-your-ai-profit-os",
    "href": "bootcamp.html#everything-included-in-your-ai-profit-os",
    "title": "Build Your Personal AI Operating System",
    "section": "Everything Included in Your AI Profit OS",
    "text": "Everything Included in Your AI Profit OS\n\n\n3 Full Days of Live Training Interactive sessions with direct access to 14+ years of Fortune 500 AI implementation experience Value: ₹75,000\n\n\nYour 90-Day AI Roadmap Personalized implementation plan - start saving time from Week 1 Value: ₹50,000\n\n\nDeep-Research Prompt System (Excel-Based) The same framework that saved ₹6.9Cr for the grocery chain Value: ₹40,000\n\n\nData Asset Library Template Structure your expertise for AI consumption - build your “Second Brain” Value: ₹25,000\n\n\nAI Governance Playbooks Quality control systems for high-stakes AI decisions Value: ₹30,000\n\n\nCustom ROI Calculator Track time saved, output increased, revenue impact - measure what matters Value: ₹20,000\n\n\nLifetime Recording Access All sessions recorded, plus framework updates as systems evolve Value: ₹15,000\n\n\nPrivate Community Access Ongoing support with fellow AI Profit OS users, monthly office hours with Jitin Value: ₹25,000\n\n\nTotal Value: ₹2,80,000 Founding Cohort Investment: ₹7,490 (Saves ₹2,75,001) Regular Price: ₹16,650 (after founding cohort closes)"
  },
  {
    "objectID": "bootcamp.html#the-unheard-of-guarantee",
    "href": "bootcamp.html#the-unheard-of-guarantee",
    "title": "Build Your Personal AI Operating System",
    "section": "The “Unheard Of” Guarantee",
    "text": "The “Unheard Of” Guarantee\nIf you don’t save at least 10 hours of work per week using the AI Profit OS within 30 days, I will refund you and pay you ₹5,000 for wasting your time.\nYes, you read that right.\nRefund + ₹5,000 penalty paid to you.\nWhy am I this confident? Because the system works. The grocery chain saved ₹6.9Cr. The automotive client saved $11M. The retail client generates £80K/month additional revenue.\nIf you implement the frameworks and don’t save minimum 10 hours per week, I’ve failed you - and I’ll pay for it.\nTo claim: Email me within 30 days with (a) your 90-day roadmap, (b) evidence of implementation, (c) time tracking showing &lt;10 hours saved. If you did the work and didn’t get results, you get refund + ₹5,000."
  },
  {
    "objectID": "bootcamp.html#frequently-asked-questions",
    "href": "bootcamp.html#frequently-asked-questions",
    "title": "Build Your Personal AI Operating System",
    "section": "Frequently Asked Questions",
    "text": "Frequently Asked Questions\n\n\n\n\nDo I need to know how to code?\n\n\n\nNo. The AI Profit OS is designed for senior professionals, not engineers. Everything is built in Excel + simple tools you already use. If you can use Google Sheets, you can build these systems.\n\n\n\n\n\nI’m not technical. Will I be able to follow?\n\n\n\nAbsolutely. This training is designed for business leaders, not data scientists. I’ve trained CFOs, COOs, and Senior Directors with zero technical background. If you can manage a P&L, you can operationalize AI.\n\n\n\n\n\nWhat if I can’t attend all 3 days live?\n\n\n\nYou’ll get lifetime recording access. However, the live workshops where you build YOUR specific systems are where the real value comes from. We strongly recommend attending live.\n\n\n\n\n\nIs this just ChatGPT training?\n\n\n\nNo. ChatGPT is one tool in the AI Profit OS, but the system is much bigger. You’ll learn: (a) How to structure your workflow for AI leverage, (b) How to build repeatable systems (not one-off prompts), (c) How to measure ROI and scale what works.\n\n\n\n\n\nWhat industries does this work for?\n\n\n\nIf you’re a senior professional doing knowledge work, this works. I’ve deployed AI systems across automotive, retail, FMCG, telecommunications, finance, and healthcare. The frameworks are industry-agnostic.\n\n\n\n\n\nWhat’s the refund policy?\n\n\n\nThe “Unheard Of” Guarantee (see above). If you implement the system and don’t save 10+ hours/week within 30 days, refund + ₹5,000 paid to you. After 30 days, no refunds - but you can transfer your seat to a colleague.\n\n\n\n\n\nCan I bring my team?\n\n\n\nYes! We offer team pricing for 3+ participants from the same organization. Email jitin@jitinkapila.com for custom team rates. (Recommended: Bring 2-3 key leaders to accelerate adoption.)\n\n\n\n\n\nHow is this different from other AI courses?\n\n\n\nMost AI courses teach theory or simple prompting. The AI Profit OS is a complete operational system you’ll deploy in your workflow. You’re not learning about AI - you’re building leverage. By Day 3, you have working systems saving you time.\n\n\n\n\n\nWhat if AI changes? Will this become outdated?\n\n\n\nThe OLCD Protocol, AI-ART Matrix, and operational frameworks are model-agnostic. Whether it’s ChatGPT, Claude, Gemini, or whatever comes next, the system adapts. Plus, you get lifetime access to framework updates."
  },
  {
    "objectID": "bootcamp.html#founding-cohort-december-19-21-2025",
    "href": "bootcamp.html#founding-cohort-december-19-21-2025",
    "title": "Build Your Personal AI Operating System",
    "section": "Founding Cohort: December 19-21, 2025",
    "text": "Founding Cohort: December 19-21, 2025\n\nOnly 12 Seats Remaining (of 20 total)\nWe deliberately keep cohorts small to ensure personalized feedback on your AI systems and deep troubleshooting for your specific use case.\nFounding Cohort Pricing Ends: When all 20 seats fill OR December 10, 2025 (whichever comes first)\nAfter upcoming cohort closes, price increases to ₹16,650 permanently.\n\n\n\n\nUpcoming Cohort (12 seats left)\nRegular Price\nMy Consulting Rate\n\n\n\n\n₹7,490\n₹16,650\n$185/hour\n\n\nSave ₹1,58,575 in value\nStandard rate\n(200+ hours of frameworks)"
  },
  {
    "objectID": "bootcamp.html#ready-to-build-your-ai-profit-os",
    "href": "bootcamp.html#ready-to-build-your-ai-profit-os",
    "title": "Build Your Personal AI Operating System",
    "section": "Ready to Build Your AI Profit OS?",
    "text": "Ready to Build Your AI Profit OS?\nInvestment: ₹7,490 (Founding Cohort) | ₹16,650(including GST) (Regular) Next Cohort: December 19-21, 2025 Seats Remaining: 12 of 20\nSecure Your Seat Now at ₹7,490 Only\n\nThe “Unheard Of” Guarantee: If you don’t save 10+ hours/week within 30 days, refund + ₹5,000 paid to you.\n\n\nQuestions? Email me directly Or schedule a 15-minute clarity call"
  },
  {
    "objectID": "bootcamp.html#still-on-the-fence",
    "href": "bootcamp.html#still-on-the-fence",
    "title": "Build Your Personal AI Operating System",
    "section": "Still On The Fence?",
    "text": "Still On The Fence?\n\n\n\n\nI’m not sure this will work for my specific industry/role.\n\n\n\nThat’s exactly why Day 1 is focused on YOUR workflow and YOUR 90-day roadmap. The frameworks are industry-agnostic, but the implementation is personalized to your reality.\n\n\n\n\n\n₹7,999 feels expensive for a training.\n\n\n\nCompare it to: - (a) One hour of my consulting at ₹25,000/hour, -(b) The cost of being replaced by a 22-year-old who knows how to use AI, -(c) The ₹6.9Cr the grocery chain saved with this exact system. If you don’t save 10+ hours/week, I’ll refund you AND pay you ₹8,000.\n\n\n\n\n\nWhat if I can’t implement this alone?\n\n\n\nYou won’t be alone. You’ll join a private community of senior professionals building AI systems. Plus monthly office hours with me. And if you need hands-on implementation support, 1:1 consulting is available.\n\n\n\n\n\nHow do I know this isn’t just hype?\n\n\n\nLook at the results: ₹6.9Cr grocery chain savings. $11M automotive optimization. £80K/month retail revenue lift. These are real deployments at Mercedes, L’Oréal, Maruti, British Telecom. This isn’t theory - it’s operational leverage.\n\n\n\n\n\nWhat if AI replaces me anyway?\n\n\n\nAI will replace professionals who DON’T learn to operationalize it. Senior professionals who build AI systems become 10x more valuable. You’re not competing with AI - you’re building leverage ON TOP of your 10+ years of expertise."
  },
  {
    "objectID": "consulting.html",
    "href": "consulting.html",
    "title": "Work Directly With Me",
    "section": "",
    "text": "After 14 years architecting AI systems across automotive, retail, insurance, FMCG, and telecommunications, I’ve learned one thing:\nThe best solutions come from looking sideways.\nMost consultants stay within their industry expertise. I bring patterns from completely different domains to solve problems that seem intractable.\nHealthcare insights solving automotive challenges. Retail strategies transforming manufacturing operations. Mathematical frameworks from applied physics applied to business systems."
  },
  {
    "objectID": "consulting.html#the-approach",
    "href": "consulting.html#the-approach",
    "title": "Work Directly With Me",
    "section": "",
    "text": "After 14 years architecting AI systems across automotive, retail, insurance, FMCG, and telecommunications, I’ve learned one thing:\nThe best solutions come from looking sideways.\nMost consultants stay within their industry expertise. I bring patterns from completely different domains to solve problems that seem intractable.\nHealthcare insights solving automotive challenges. Retail strategies transforming manufacturing operations. Mathematical frameworks from applied physics applied to business systems."
  },
  {
    "objectID": "consulting.html#what-well-work-on",
    "href": "consulting.html#what-well-work-on",
    "title": "Work Directly With Me",
    "section": "What We’ll Work On",
    "text": "What We’ll Work On\n\n\nAI Strategy & Architecture\nMap your AI maturity. Design scalable systems. Build transformation roadmaps that actually deploy.\n\n\nCross-Industry Pattern Recognition\nApply insights from adjacent industries. Solve problems others can’t see because they’re looking straight ahead.\n\n\nExecutive Team Alignment\nGet your leadership on the same page about AI transformation. No vendor pitches. Just architecture.\n\n\nImplementation Support\nI don’t just hand you a deck and leave. I work alongside your team until the system is live and scaled."
  },
  {
    "objectID": "consulting.html#who-this-is-for",
    "href": "consulting.html#who-this-is-for",
    "title": "Work Directly With Me",
    "section": "Who This Is For",
    "text": "Who This Is For\nFortune 500 enterprises with real transformation budgets.\nLeaders who are tired of prescriptive AI and ready for predictive strategy.\nOrganizations ready to commit to systemic change, not point solutions.\nThis isn’t for everyone. But if you’re the right fit, the ROI is undeniable."
  },
  {
    "objectID": "consulting.html#current-engagement-model",
    "href": "consulting.html#current-engagement-model",
    "title": "Work Directly With Me",
    "section": "Current Engagement Model",
    "text": "Current Engagement Model\nI work with a limited number of clients at any given time, typically serving as Acting CTO or Strategic AI Advisor across multiple accounts.\nTypical engagement: 6-12 months, embedded with your leadership team.\nInvestment: Custom scoped based on organization size and transformation goals."
  },
  {
    "objectID": "consulting.html#apply-for-consulting",
    "href": "consulting.html#apply-for-consulting",
    "title": "Work Directly With Me",
    "section": "Apply for Consulting",
    "text": "Apply for Consulting\nCurrent waitlist: [Q2 2025]\nIf you’re exploring a major AI transformation and want to discuss whether we’re a fit, let’s talk.\nSchedule a Discovery Call\n\nOr email me directly at jitin@jitinkapila.com"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Jitin Kapila | The AI Architect",
    "section": "",
    "text": "15 years architecting AI systems across automotive, retail, insurance, FMCG, and telecommunications. Mechanical engineer with applied mathematics background. Former AI-CTO for Fortune 500 accounts.\n \n  \n   \n  \n    \n     LinkedIn\n  \n  \n    \n     Twitter\n  \n  \n    \n     GitHub\n  \n  \n    \n     YouTube\n  \n  \n    \n     SubStack\n  \n  \n    \n     Email"
  },
  {
    "objectID": "about.html#hi---i-am-jitin-i-help-business-leverage-ai-for-profits",
    "href": "about.html#hi---i-am-jitin-i-help-business-leverage-ai-for-profits",
    "title": "Jitin Kapila | The AI Architect",
    "section": "",
    "text": "15 years architecting AI systems across automotive, retail, insurance, FMCG, and telecommunications. Mechanical engineer with applied mathematics background. Former AI-CTO for Fortune 500 accounts.\n \n  \n   \n  \n    \n     LinkedIn\n  \n  \n    \n     Twitter\n  \n  \n    \n     GitHub\n  \n  \n    \n     YouTube\n  \n  \n    \n     SubStack\n  \n  \n    \n     Email"
  },
  {
    "objectID": "about.html#the-origin-story",
    "href": "about.html#the-origin-story",
    "title": "Jitin Kapila | The AI Architect",
    "section": "The Origin Story",
    "text": "The Origin Story\nI didn’t set out to become “The AI Architect.”\nI started as a mechanical engineer with a background in applied mathematics. Early in my career, I was hired to solve automotive inventory problems that traditional approaches couldn’t crack.\nThe patterns were too complex. The variables too numerous. The standard playbooks weren’t working.\nThen I had a realization:\nWhat if I stopped thinking about this like an automotive problem and started thinking about it like a healthcare system managing patient flow?\nSuddenly, the solution became obvious. We eliminated 45,000 excess units.\nNot because I knew more about automotive than the automotive experts.\nBecause I was looking at it sideways."
  },
  {
    "objectID": "about.html#the-pattern-emerges",
    "href": "about.html#the-pattern-emerges",
    "title": "Jitin Kapila | The AI Architect",
    "section": "The Pattern Emerges",
    "text": "The Pattern Emerges\nOver 14 years across automotive, retail, insurance, FMCG, and telecommunications, I kept seeing the same thing:\nThe breakthrough comes from applying insights from completely different domains.\nHealthcare solving automotive. Retail transforming manufacturing. Mathematical frameworks from physics applied to business systems.\nMost experts stay within their industry expertise. They know their domain deeply, but they’re looking straight ahead.\nI learned to look sideways."
  },
  {
    "objectID": "about.html#the-mathematical-mindset",
    "href": "about.html#the-mathematical-mindset",
    "title": "Jitin Kapila | The AI Architect",
    "section": "The Mathematical Mindset",
    "text": "The Mathematical Mindset\nIn mathematics, there’s never just one solution to a problem. Some roots are simply more beautiful than others.\nThis philosophy drives how I approach enterprise AI:\n\nPattern recognition across domains instead of single-industry expertise\nRoot cause analysis instead of surface-level fixes\nElegant solutions over brute force approaches\nSystemic thinking instead of point solutions\n\nWhen you’ve solved similar problems in healthcare, the path through retail becomes visible. When you understand the mathematics of one optimization challenge, you start seeing the same structures everywhere."
  },
  {
    "objectID": "about.html#current-work",
    "href": "about.html#current-work",
    "title": "Jitin Kapila | The AI Architect",
    "section": "Current Work",
    "text": "Current Work\nI serve as Acting CTO across multiple Fortune 500 accounts while mentoring a team of 25+ professionals.\nThe work spans:\n\nPredictive analytics and AI strategy architecture\nCross-industry system optimization\nExecutive team transformation and strategic planning\nImplementation frameworks that actually scale\n\nI also run The AI Profit OS — a 3-day intensive that teaches enterprise leaders to think like architects instead of tool users."
  },
  {
    "objectID": "about.html#beyond-the-work",
    "href": "about.html#beyond-the-work",
    "title": "Jitin Kapila | The AI Architect",
    "section": "Beyond the Work",
    "text": "Beyond the Work\nWhen I’m not architecting AI systems, you’ll find me:\n\nPlaying guitar (badly but enthusiastically)\nPlanning the next off-the-beaten-path adventure\nReading mathematical proofs for fun (yes, really)\n\nGetting lost in unfamiliar places keeps the mind flexible. Some of my best insights come from being completely outside my comfort zone."
  },
  {
    "objectID": "about.html#want-to-work-together",
    "href": "about.html#want-to-work-together",
    "title": "Jitin Kapila | The AI Architect",
    "section": "Want to Work Together?",
    "text": "Want to Work Together?\nI take on a limited number of consulting engagements and run cohort-based training for enterprise AI leaders.\nIf you’re exploring a major AI transformation, let’s talk.\nExplore Consulting Join The Protocol\n\nOr just email me to discuss your challenge."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Don’t Learn AI. Operationalize It.",
    "section": "",
    "text": "Join the Bootcamp (Tier 1)\n\n\nExplore Consulting (Tier 2)"
  },
  {
    "objectID": "index.html#the-ever-widening-gap",
    "href": "index.html#the-ever-widening-gap",
    "title": "Don’t Learn AI. Operationalize It.",
    "section": "The Ever Widening Gap",
    "text": "The Ever Widening Gap\nIF\n\n→ You have 12+ years of experience.\n→ You understand business very well.\n→ You want to grow or make you business grow.\n→ You are managing Multi-millions or Corers of portfolios.\n→ You are having a FOMO of AI Hype\n→ Your boardroom is asking you “What is you AI game-plan?”\n\n\nBUT\n\n→ You are still unable to leverage AI\n→ You are confused what to use and what not to\n→ You are not sure about ROI of AI\n→ You have no game-plan/roadmap!\n\n\n\nIf this is you, then you are at right place !!"
  },
  {
    "objectID": "index.html#how-can-i-help-you",
    "href": "index.html#how-can-i-help-you",
    "title": "Don’t Learn AI. Operationalize It.",
    "section": "How can I help You",
    "text": "How can I help You\nWe can work depending on your speed and needs.\n\n\nTier 1: The AI Profit OS™\n3-Day D.I.Y Implementation Bootcamp\nFor leaders who want to build their personal AI Operating System in one weekend. We install the Deep-Research System, the Governance Playbook, and your first Agentic Workflow.\n\nFormat: Live Virtual Sprint (3 Days)\nNext Cohort: Dec 19-21, 2025\nInvestment: ₹16,650 ₹7,490 (Founding Member)\n\nSecure Your Seat →\n\n\nTier 2: The AI Profit Accelerator™\nD.W.Y 4-Week Consulting Program\nFor business owners who need a full AI-ART Matrix deployment across Sales, Ops, and HR. We audit your Data Assets, design your AI Leverage Roadmap which you can start from Day 0.\n\nFormat: Hybrid Consulting (Small Cohort)\nStatus: Application Only\nInvestment: ₹50,000+\n\nLet’s have a word →"
  },
  {
    "objectID": "insights.html",
    "href": "insights.html",
    "title": "Insights",
    "section": "",
    "text": "Examples of how cross-industry pattern recognition transforms complex challenges into elegant solutions."
  },
  {
    "objectID": "insights.html#mathematical-thinking-applied-to-business-problems",
    "href": "insights.html#mathematical-thinking-applied-to-business-problems",
    "title": "Insights",
    "section": "",
    "text": "Examples of how cross-industry pattern recognition transforms complex challenges into elegant solutions."
  },
  {
    "objectID": "vault.html",
    "href": "vault.html",
    "title": "The Vault a.k.a Blog",
    "section": "",
    "text": "Bringing best of hands-on exprience of strategizing and implementing AI!\nAI frameworks, implementation patterns, and strategic insights for enterprise architects. Strategy guides for C-suite. Technical deep-dives for builders. Decision-first AI, OLCD Protocol, and more.\n\nStrategyEngineering\n\n\nFor the C-Suite. ROI-focused frameworks and business transformation insights.\n\n\n\n\n\n\n\nDecision-First AI: Why Data Should Follow, Not Lead\n\n\n\nstrategy\n\nroi\n\nbusiness\n\nai\n\n\n\nLet’s stop the habit of blaming data and AI and start with what decisions we want !!\n\n\n\nOct 9, 2025\n\n\n\n\n\n\nNo matching items\n\n\n\nFor the builders. Code, implementation patterns, and technical deep-dives.\n\n\n\n\n\n\n\nTrieKNN: Unleashing KNN’s Power on Mixed Data Types\n\n\n\nknn\n\nml\n\nmixed-data\n\n\n\nDiscover TrieKNN, a novel approach to extend the K-Nearest Neighbors algorithm to datasets with both categorical and numerical features. Learn how it works, see it in…\n\n\n\nFeb 26, 2025\n\n\n\n\n\n\n\n\n\n\nCrossTab Sparsity for Classification\n\n\n\nclassification\n\nmetric\n\nfeature selection\n\n\n\nCan our metric help us in making a classification problem work better ?\n\n\n\nJan 3, 2023\n\n\n\n\n\n\n\n\n\n\nCrossTab Sparsity\n\n\n\nclustering\n\nanalysis\n\n\n\nA label and data type agnostic metric for evaluating clustering performance!\n\n\n\nMay 3, 2022\n\n\n\n\n\n\n\n\n\n\nA flow to Test Your Hypothesis in Python\n\n\n\neda\n\nhypothesis\n\nanalysis\n\npython\n\n\n\nMaking life easy to do some serious hypothesis testing in python.\n\n\n\nAug 10, 2021\n\n\n\n\n\n\n\n\n\n\nAdaptive Regression\n\n\n\nstrategy\n\n\n\nWe recently put through our observation on Regression Problem in our research. This post is a nonformal attempt to explain it.\n\n\n\nMay 1, 2018\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "CLAUDE.html",
    "href": "CLAUDE.html",
    "title": "CLAUDE.md",
    "section": "",
    "text": "This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\n\n\nThis is “The AI Architect” brand website for Jitin Kapila, built with Quarto. The site transforms from a portfolio into a premium coaching/consulting platform featuring:\n\nHomepage with hero + frameworks (AI-ART Matrix, OLCD Protocol)\nCourse sales page (AI Profit OS - ₹19,999/₹34,999)\nConsulting waitlist\nBlog (“The Vault”) with Strategy/Engineering tabs\nAbout page (authority building)\n\nLive URLs: - Primary: https://jitinkapila.com - Netlify: https://percieveit.com\n\n\n\n\nStatic Site Generator: Quarto (1.4+)\nStyling Approach: SCSS + CSS (hybrid system - see below)\nCurrent Theme: Brite (Bootstrap-based)\nAnalytics: Umami (includes/umami.html)\nSEO: Schema.org JSON-LD (includes/schema.html)\nComments: Utterances (GitHub-based, on blog posts)\nPublishing: Netlify\n\n\n\n\n\n\nThe project currently uses two styling systems which can cause conflicts:\n\ncustom.css - Direct CSS overrides (current implementation)\nstyles.scss - SCSS with Bootstrap variables (backup/original)\n\n\n\n\nBased on Quarto best practices and the Lumo theme example in extras/, here’s why SCSS is superior:\n\n\n\nNative Quarto Integration: Quarto processes SCSS at build time with Bootstrap\nVariable System: Can override Bootstrap variables ($primary, $font-family-sans-serif)\nTheme Inheritance: Extend themes properly instead of fighting them\nCompile-Time Safety: SCSS errors caught during build, not runtime\nSource Order Control: SCSS compiles BEFORE custom CSS in cascade\n\n\n\n\n\nOverride Battles: Must use !important everywhere to fight Bootstrap\nSpecificity Wars: Constantly battling theme defaults\nNo Variables: Can’t modify Bootstrap’s design tokens cleanly\nRuntime Conflicts: Themes like Cosmo/Brite have their own opinions\n\n\n\n\n\n/*-- scss:defaults --*/\n// Override Bootstrap variables BEFORE they compile\n@import url('https://fonts.googleapis.com/css2?family=Bebas+Neue&family=IBM+Plex+Sans:wght@400;500;600&display=swap');\n\n// Typography Variables\n$font-family-sans-serif: 'IBM Plex Sans', sans-serif;\n$headings-font-family: 'Bebas Neue', sans-serif;\n$headings-font-weight: 400;\n$headings-text-transform: uppercase;\n$headings-letter-spacing: 1px;\n\n// Color Variables\n$primary: #0C2B4E;      // Navy Deep\n$secondary: #213555;    // Navy Royal\n$body-color: #5A6A7A;\n$link-color: #0C2B4E;\n\n// Spacing\n$spacer: 1rem;\n\n// Border Radius (Blueprint aesthetic - no curves)\n$border-radius: 0;\n$border-radius-sm: 0;\n$border-radius-lg: 0;\n\n/*-- scss:rules --*/\n// Custom rules that extend the compiled theme\nbody {\n  background-image:\n    linear-gradient(rgba(33, 53, 85, 0.03) 1px, transparent 1px),\n    linear-gradient(90deg, rgba(33, 53, 85, 0.03) 1px, transparent 1px);\n  background-size: 20px 20px;\n}\n\nh1, h2, h3, h4, h5, h6 {\n  font-family: $headings-font-family;\n  text-transform: $headings-text-transform;\n  letter-spacing: $headings-letter-spacing;\n}\n\n\n\n\nVariables in /*-- scss:defaults --*/ override Bootstrap BEFORE compilation\nRules in /*-- scss:rules --*/ extend the theme cleanly\nNo !important needed - you’re modifying the source, not fighting it\nTheme changes are easy - just change theme name in _quarto.yml\n\n\n\n\n\n# Preview with live reload\nquarto preview\n\n# Production build\nquarto render\n\n# Publish to Netlify\nquarto publish netlify\nNote: Pre-render hook cleans _site/* automatically (see _quarto.yml)\n\n\n\n/\n├── index.qmd              # Homepage (Hero + AI-ART Matrix)\n├── about.qmd              # Authority building\n├── bootcamp.qmd           # AI Profit OS sales page\n├── consulting.qmd         # Consulting waitlist\n├── vault/\n│   ├── index.qmd          # Blog listing with tabs\n│   └── (links to ../posts/)\n├── posts/                 # Blog posts\n│   ├── _metadata.yml      # Default settings for all posts\n│   └── [post_name]/index.qmd\n├── includes/\n│   ├── umami.html         # Analytics\n│   └── schema.html        # SEO structured data\n├── extras/                # Reference materials\n│   ├── BRAND_MEMORY.md    # Complete brand guide\n│   ├── design-tokens.json # Design system tokens\n│   └── lumo theme/        # Example Quarto extension\n├── _quarto.yml            # Site configuration\n├── custom.css             # Current CSS (to migrate to SCSS)\n├── styles.scss            # Backup SCSS system\n└── backup/                # Original files backup\n\n\n\n\n\n\nBrand Name: The AI Architect\nTagline: “Stop Guessing. Start Architecting.”\nVisual Motif: Engineering blueprint aesthetic\nCore Principle: Sharp corners, no gradients, geometric precision\n\n\n\n\n:root {\n  /* Primary */\n  --navy-deep: #0C2B4E;      /* Headers, buttons, primary text */\n  --navy-royal: #213555;     /* Hover states, secondary */\n  --slate-blue: #3E5879;     /* Accent text */\n\n  /* Accent */\n  --cream-gold: #D8C4B6;     /* Dividers, highlights */\n  --gold-hover: #C4B09E;     /* Gold hover state */\n\n  /* Backgrounds */\n  --bg-light: #FAFBFC;       /* Default background */\n  --bg-warm: #F8F6F3;        /* Alternate sections */\n  --bg-grid: rgba(33, 53, 85, 0.03);  /* Blueprint grid */\n\n  /* Text */\n  --text-primary: #0C2B4E;   /* Headlines */\n  --text-body: #5A6A7A;      /* Paragraphs */\n  --text-muted: #8896A6;     /* Meta info */\n}\n\n\n\n/* Headers: Bebas Neue - Bold, Uppercase, Tight */\nh1, h2, h3 {\n  font-family: 'Bebas Neue', sans-serif;\n  text-transform: uppercase;\n  letter-spacing: 1px;\n}\n\n/* Body: IBM Plex Sans - Clean, Professional */\nbody {\n  font-family: 'IBM Plex Sans', sans-serif;\n  font-weight: 400;\n  line-height: 1.7;\n}\n\n/* Responsive Sizing */\nh1 { font-size: clamp(48px, 8vw, 80px); line-height: 0.95; }\nh2 { font-size: clamp(32px, 5vw, 48px); line-height: 1.1; }\nh3 { font-size: clamp(24px, 3vw, 32px); line-height: 1.2; }\n\n/* Eyebrow Text */\n.eyebrow {\n  font-size: 11px;\n  font-weight: 600;\n  letter-spacing: 3px;\n  text-transform: uppercase;\n  color: var(--text-muted);\n}\n\n\n\nAll components follow border-radius: 0 (no rounded corners):\n\nButtons: Sharp, uppercase, slide-in hover effect\nCards: 1-2px border, subtle shadow on hover\nDividers: 60px horizontal lines in cream-gold\nGrid Background: 20px × 20px blueprint grid at 3% opacity\n\n\n\n\n\n\n\nproject:\n  type: website\n  output-dir: _site\n  pre-render: [\"rm -rf _site/*\"]  # Clean build\n  resources:\n    - CNAME\n    - favicon-32x32.png\n    - favicon.ico\n    - apple-touch-icon.png\n\nwebsite:\n  title: \"Jitin Kapila | The AI Architect\"\n  site-url: https://jitinkapila.com\n\n  navbar:\n    background: transparent  # Let CSS control this\n    left:\n      - text: \" \"  # Blank for logo/brand\n        href: index.qmd\n    right:\n      - text: \"The Protocol\"\n        href: bootcamp.qmd\n      # ... other nav items\n\n  page-footer:\n    left: \"© 2025 Jitin Kapila | The AI Architect\"\n    right:\n      - icon: linkedin\n        href: https://linkedin.com/in/jitinkapila\n      # ... other social icons\n\nformat:\n  html:\n    theme: brite  # Can also use: cosmo, flatly, litera\n    css: custom.css  # Or use SCSS with theme: [custom.scss]\n    include-in-header:\n      - includes/umami.html\n      - includes/schema.html\n    toc: false  # Disable by default, enable per-page\n    code-fold: true\n    highlight-style: github\n    smooth-scroll: true\n\nexecute:\n  freeze: auto  # Cache computational output\n  cache: true\n\ngoogle-analytics: \"G-XXXXXXXXXX\"  # Replace with actual ID\n\n\n\n\nTheme Selection: Use Bootstrap-based themes (cosmo, brite, flatly, litera)\nCSS Loading Order: Theme → CSS → Page-specific styles\nFreeze Strategy: freeze: auto prevents re-running code unnecessarily\nResources: Always include CNAME, favicons for deployment\n\n\n\n\n\n\n\n---\ntitle: \"Post Title\"\ndescription: \"Brief description for listings\"\ndate: \"YYYY-MM-DD\"\ncategories: [strategy, code]  # For tab filtering\ntags: [ai, ml, enterprise]\nimage: \"img/feature.jpg\"\nimage-alt: \"Alt text\"\ndraft: false  # Set true to hide\nauthor: \"Jitin Kapila\"  # Inherited from posts/_metadata.yml\n---\n\n\n\nposts/_metadata.yml sets defaults for ALL posts:\nfreeze: true\nauthor: 'Jitin Kapila'\ntoc: true\ntoc-depth: 3\ntoc-location: left\npage-layout: article\nreference-location: margin  # Tufte-style margin notes\ncitation-location: margin\nlightbox: true\nlicense: \"CC BY\"\ncode-line-numbers: true\ncode-fold: true\ncomments:\n  utterances:\n    repo: jkapila/perceptions\n    theme: github-light\n\n\n\nThe Vault uses categories to separate posts:\n\nStrategy Tab: categories: [strategy, business, roi]\nEngineering Tab: categories: [code, llm, agents]\n\nNote: Posts can appear in multiple tabs if they have multiple categories.\n\n\n\n\n\n\nQuarto uses fenced divs for layout control:\n::: {.hero-section}\nContent here\n:::\n\n::: {.column-body-outset}\nWide content that extends beyond normal margins\n:::\n\n::: {.panel-tabset}\n## Tab 1\nContent\n## Tab 2\nMore content\n:::\n\n\n\n\n.column-body - Normal width (default)\n.column-body-outset - Slightly wider\n.column-page - Page width\n.column-screen - Full screen width\n.column-margin - Margin notes (Tufte style)\n\n\n\n\nFor deep customization, use template partials (see extras/lumo theme/):\nformat:\n  html:\n    template-partials:\n      - title-block.html  # Custom title block\n      - footer.html       # Custom footer\n\n\n\n\nQuarto 1.4+ supports brand.yml for centralized design tokens. This is the FUTURE direction:\n\n\nmeta:\n  name: \"The AI Architect\"\n  link: \"https://jitinkapila.com\"\n\ncolor:\n  palette:\n    navy-deep: \"#0C2B4E\"\n    navy-royal: \"#213555\"\n    cream-gold: \"#D8C4B6\"\n  background: \"#FAFBFC\"\n  foreground: \"#0C2B4E\"\n  primary: \"#0C2B4E\"\n  secondary: \"#213555\"\n\ntypography:\n  fonts:\n    - family: \"Bebas Neue\"\n      source: google\n    - family: \"IBM Plex Sans\"\n      source: google\n      weight: [400, 500, 600]\n  base-size: \"16px\"\n  headings:\n    family: \"Bebas Neue\"\n    weight: 400\n  body:\n    family: \"IBM Plex Sans\"\n    weight: 400\n    line-height: 1.7\n\n\n\n\nSingle Source of Truth: All design tokens in one file\nCross-Format: Works across HTML, PDF, DOCX outputs\nTooling Integration: IDEs can autocomplete brand values\nTeam Collaboration: Designers can edit without touching code\n\nStatus: Not yet implemented. Consider for Phase 2 refactor.\n\n\n\n\n\n\nLocated in includes/schema.html:\n&lt;script type=\"application/ld+json\"&gt;\n{\n  \"@context\": \"https://schema.org\",\n  \"@graph\": [\n    {\n      \"@type\": \"ProfessionalService\",\n      \"name\": \"Jitin Kapila - The AI Architect\",\n      \"founder\": {\n        \"@type\": \"Person\",\n        \"name\": \"Jitin Kapila\",\n        \"jobTitle\": \"AI Strategy Consultant\"\n      }\n    },\n    {\n      \"@type\": \"Course\",\n      \"name\": \"The AI Architect Protocol\",\n      \"provider\": { \"@type\": \"Person\", \"name\": \"Jitin Kapila\" }\n    }\n  ]\n}\n&lt;/script&gt;\n\n\n\nLocated in includes/umami.html - lightweight, privacy-focused alternative to Google Analytics.\n\n\n\nConfigured in _quarto.yml:\nwebsite:\n  open-graph: true\n  twitter-card:\n    creator: \"@jitinkapila\"\n    card-style: summary_large_image\n\n\n\n\n\n\nexecute:\n  freeze: auto  # Only re-run when source changes\n  cache: true   # Cache results\nWhen to Clear Cache:\nrm -rf _freeze/\nquarto render\n\n\n\n\nUse thumbnails for listings (-thumb.jpg suffix)\nCompress images before adding to repo\nEnable lightbox for blog images (lightbox: true)\n\n\n\n\n\nUse highlight-style: github for consistency\nEnable code-fold: true for long code blocks\nAdd code-line-numbers: true for readability\n\n\n\n\n\n\n\nEdit _quarto.yml → website.navbar.right:\nnavbar:\n  right:\n    - text: \"New Page\"\n      href: newpage.qmd\n\n\n\nEdit _quarto.yml → page-footer.right:\npage-footer:\n  right:\n    - icon: youtube\n      href: https://youtube.com/@channel\n\n\n\n# Create file\ntouch newpage.qmd\n\n# Add frontmatter\n---\ntitle: \"Page Title\"\npage-layout: full  # or article, custom\n---\n\n\n\nEdit styles.scss → /*-- scss:defaults --*/:\n$primary: #0C2B4E;\n$secondary: #213555;\nThen in _quarto.yml:\nformat:\n  html:\n    theme: [cosmo, styles.scss]  # Theme + custom SCSS\n\n\n\n\n\n\nCause: Using theme: none removes Bootstrap navbar structure. Fix: Use a Bootstrap theme (cosmo, brite, flatly) + CSS overrides.\n\n\n\nCheck Order: 1. Is SCSS in /*-- scss:defaults --*/ section? 2. Is CSS loaded AFTER theme in _quarto.yml? 3. Are you using !important (symptom of wrong approach)?\nDebug:\nquarto preview --log-level debug\n\n\n\nSymptom: Old content showing despite changes. Fix:\nrm -rf _freeze/\nquarto render\n\n\n\nFix:\n# Kill all Quarto processes\npkill -f quarto\n\n# Restart preview\nquarto preview\n\n\n\n\n\n\n\nUse SCSS variables instead of CSS custom properties for Bootstrap integration\nPrefer theme extension over complete override\nAvoid !important - it indicates fighting the cascade\nTest mobile-first - use responsive breakpoints\nKeep specificity low - let cascade work naturally\n\n\n\n\n\nUse divs for layout, not CSS classes in markdown\nEnable freeze for computational posts - save build time\nAdd alt text to all images - SEO + accessibility\nUse relative links for internal navigation\nTest locally before pushing - quarto preview\n\n\n\n\n\nOptimize images before committing (use WebP when possible)\nUse code-fold for long code blocks\nEnable caching for expensive computations\nMinimize custom fonts (currently 2 families - good)\nLazy load images in listings\n\n\n\n\n\n\n\n\nMigrate to Bootstrap theme + CSS overrides\nFix navbar visibility\nConsolidate to SCSS-only approach\nRemove !important from CSS\n\n\n\n\n\nImplement brand.yml for design tokens\nCreate Quarto extension for Blueprint theme\nAdd custom template partials\nImplement custom listing layouts\n\n\n\n\n\nAdd service worker for offline\nImplement lazy loading for images\nOptimize build time (currently ~20s)\nAdd progressive enhancement features\n\n\n\n\n\n\n\n\nQuarto Websites\nHTML Themes\nHTML Theme Customization\nBrand Guide\nHTML Basics\n\n\n\n\n\nextras/BRAND_MEMORY.md - Complete brand guide\nextras/design-tokens.json - Design system reference\nextras/lumo theme/ - Example custom Quarto extension\nbackup/ - Original files before rebuild\n\n\n\n\n\nQuarto Discussions\nAwesome Quarto\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFile\nPurpose\nEdit Frequency\n\n\n\n\n_quarto.yml\nSite config, nav, theme\nOccasionally\n\n\ncustom.css\nStyle overrides\nOften (to migrate to SCSS)\n\n\nstyles.scss\nSCSS variables/rules\nRecommended approach\n\n\nincludes/schema.html\nSEO structured data\nRarely\n\n\nincludes/umami.html\nAnalytics script\nNever (unless changing provider)\n\n\nposts/_metadata.yml\nBlog post defaults\nRarely\n\n\nindex.qmd\nHomepage\nOccasionally\n\n\nvault/index.qmd\nBlog listing\nRarely\n\n\n\n\n\n\n---\ntitle: \"Page Title\"\npagetitle: \"Browser Tab Title\"  # Overrides title for &lt;title&gt; tag\npage-layout: full  # full, article, custom\ntoc: true  # Enable table of contents\ntoc-location: left  # left, right, body\ndescription: \"Meta description for SEO\"\nimage: \"path/to/social-share.jpg\"\ndraft: false  # Hide page if true\n---\n\nThis document should be referenced when making ANY changes to the site architecture, styling, or content structure."
  },
  {
    "objectID": "CLAUDE.html#project-overview",
    "href": "CLAUDE.html#project-overview",
    "title": "CLAUDE.md",
    "section": "",
    "text": "This is “The AI Architect” brand website for Jitin Kapila, built with Quarto. The site transforms from a portfolio into a premium coaching/consulting platform featuring:\n\nHomepage with hero + frameworks (AI-ART Matrix, OLCD Protocol)\nCourse sales page (AI Profit OS - ₹19,999/₹34,999)\nConsulting waitlist\nBlog (“The Vault”) with Strategy/Engineering tabs\nAbout page (authority building)\n\nLive URLs: - Primary: https://jitinkapila.com - Netlify: https://percieveit.com"
  },
  {
    "objectID": "CLAUDE.html#technology-stack",
    "href": "CLAUDE.html#technology-stack",
    "title": "CLAUDE.md",
    "section": "",
    "text": "Static Site Generator: Quarto (1.4+)\nStyling Approach: SCSS + CSS (hybrid system - see below)\nCurrent Theme: Brite (Bootstrap-based)\nAnalytics: Umami (includes/umami.html)\nSEO: Schema.org JSON-LD (includes/schema.html)\nComments: Utterances (GitHub-based, on blog posts)\nPublishing: Netlify"
  },
  {
    "objectID": "CLAUDE.html#architecture-decision-why-scss-css-hybrid",
    "href": "CLAUDE.html#architecture-decision-why-scss-css-hybrid",
    "title": "CLAUDE.md",
    "section": "",
    "text": "The project currently uses two styling systems which can cause conflicts:\n\ncustom.css - Direct CSS overrides (current implementation)\nstyles.scss - SCSS with Bootstrap variables (backup/original)\n\n\n\n\nBased on Quarto best practices and the Lumo theme example in extras/, here’s why SCSS is superior:\n\n\n\nNative Quarto Integration: Quarto processes SCSS at build time with Bootstrap\nVariable System: Can override Bootstrap variables ($primary, $font-family-sans-serif)\nTheme Inheritance: Extend themes properly instead of fighting them\nCompile-Time Safety: SCSS errors caught during build, not runtime\nSource Order Control: SCSS compiles BEFORE custom CSS in cascade\n\n\n\n\n\nOverride Battles: Must use !important everywhere to fight Bootstrap\nSpecificity Wars: Constantly battling theme defaults\nNo Variables: Can’t modify Bootstrap’s design tokens cleanly\nRuntime Conflicts: Themes like Cosmo/Brite have their own opinions\n\n\n\n\n\n/*-- scss:defaults --*/\n// Override Bootstrap variables BEFORE they compile\n@import url('https://fonts.googleapis.com/css2?family=Bebas+Neue&family=IBM+Plex+Sans:wght@400;500;600&display=swap');\n\n// Typography Variables\n$font-family-sans-serif: 'IBM Plex Sans', sans-serif;\n$headings-font-family: 'Bebas Neue', sans-serif;\n$headings-font-weight: 400;\n$headings-text-transform: uppercase;\n$headings-letter-spacing: 1px;\n\n// Color Variables\n$primary: #0C2B4E;      // Navy Deep\n$secondary: #213555;    // Navy Royal\n$body-color: #5A6A7A;\n$link-color: #0C2B4E;\n\n// Spacing\n$spacer: 1rem;\n\n// Border Radius (Blueprint aesthetic - no curves)\n$border-radius: 0;\n$border-radius-sm: 0;\n$border-radius-lg: 0;\n\n/*-- scss:rules --*/\n// Custom rules that extend the compiled theme\nbody {\n  background-image:\n    linear-gradient(rgba(33, 53, 85, 0.03) 1px, transparent 1px),\n    linear-gradient(90deg, rgba(33, 53, 85, 0.03) 1px, transparent 1px);\n  background-size: 20px 20px;\n}\n\nh1, h2, h3, h4, h5, h6 {\n  font-family: $headings-font-family;\n  text-transform: $headings-text-transform;\n  letter-spacing: $headings-letter-spacing;\n}\n\n\n\n\nVariables in /*-- scss:defaults --*/ override Bootstrap BEFORE compilation\nRules in /*-- scss:rules --*/ extend the theme cleanly\nNo !important needed - you’re modifying the source, not fighting it\nTheme changes are easy - just change theme name in _quarto.yml"
  },
  {
    "objectID": "CLAUDE.html#current-build-commands",
    "href": "CLAUDE.html#current-build-commands",
    "title": "CLAUDE.md",
    "section": "",
    "text": "# Preview with live reload\nquarto preview\n\n# Production build\nquarto render\n\n# Publish to Netlify\nquarto publish netlify\nNote: Pre-render hook cleans _site/* automatically (see _quarto.yml)"
  },
  {
    "objectID": "CLAUDE.html#site-structure",
    "href": "CLAUDE.html#site-structure",
    "title": "CLAUDE.md",
    "section": "",
    "text": "/\n├── index.qmd              # Homepage (Hero + AI-ART Matrix)\n├── about.qmd              # Authority building\n├── bootcamp.qmd           # AI Profit OS sales page\n├── consulting.qmd         # Consulting waitlist\n├── vault/\n│   ├── index.qmd          # Blog listing with tabs\n│   └── (links to ../posts/)\n├── posts/                 # Blog posts\n│   ├── _metadata.yml      # Default settings for all posts\n│   └── [post_name]/index.qmd\n├── includes/\n│   ├── umami.html         # Analytics\n│   └── schema.html        # SEO structured data\n├── extras/                # Reference materials\n│   ├── BRAND_MEMORY.md    # Complete brand guide\n│   ├── design-tokens.json # Design system tokens\n│   └── lumo theme/        # Example Quarto extension\n├── _quarto.yml            # Site configuration\n├── custom.css             # Current CSS (to migrate to SCSS)\n├── styles.scss            # Backup SCSS system\n└── backup/                # Original files backup"
  },
  {
    "objectID": "CLAUDE.html#design-system-the-blueprint",
    "href": "CLAUDE.html#design-system-the-blueprint",
    "title": "CLAUDE.md",
    "section": "",
    "text": "Brand Name: The AI Architect\nTagline: “Stop Guessing. Start Architecting.”\nVisual Motif: Engineering blueprint aesthetic\nCore Principle: Sharp corners, no gradients, geometric precision\n\n\n\n\n:root {\n  /* Primary */\n  --navy-deep: #0C2B4E;      /* Headers, buttons, primary text */\n  --navy-royal: #213555;     /* Hover states, secondary */\n  --slate-blue: #3E5879;     /* Accent text */\n\n  /* Accent */\n  --cream-gold: #D8C4B6;     /* Dividers, highlights */\n  --gold-hover: #C4B09E;     /* Gold hover state */\n\n  /* Backgrounds */\n  --bg-light: #FAFBFC;       /* Default background */\n  --bg-warm: #F8F6F3;        /* Alternate sections */\n  --bg-grid: rgba(33, 53, 85, 0.03);  /* Blueprint grid */\n\n  /* Text */\n  --text-primary: #0C2B4E;   /* Headlines */\n  --text-body: #5A6A7A;      /* Paragraphs */\n  --text-muted: #8896A6;     /* Meta info */\n}\n\n\n\n/* Headers: Bebas Neue - Bold, Uppercase, Tight */\nh1, h2, h3 {\n  font-family: 'Bebas Neue', sans-serif;\n  text-transform: uppercase;\n  letter-spacing: 1px;\n}\n\n/* Body: IBM Plex Sans - Clean, Professional */\nbody {\n  font-family: 'IBM Plex Sans', sans-serif;\n  font-weight: 400;\n  line-height: 1.7;\n}\n\n/* Responsive Sizing */\nh1 { font-size: clamp(48px, 8vw, 80px); line-height: 0.95; }\nh2 { font-size: clamp(32px, 5vw, 48px); line-height: 1.1; }\nh3 { font-size: clamp(24px, 3vw, 32px); line-height: 1.2; }\n\n/* Eyebrow Text */\n.eyebrow {\n  font-size: 11px;\n  font-weight: 600;\n  letter-spacing: 3px;\n  text-transform: uppercase;\n  color: var(--text-muted);\n}\n\n\n\nAll components follow border-radius: 0 (no rounded corners):\n\nButtons: Sharp, uppercase, slide-in hover effect\nCards: 1-2px border, subtle shadow on hover\nDividers: 60px horizontal lines in cream-gold\nGrid Background: 20px × 20px blueprint grid at 3% opacity"
  },
  {
    "objectID": "CLAUDE.html#configuration-guide",
    "href": "CLAUDE.html#configuration-guide",
    "title": "CLAUDE.md",
    "section": "",
    "text": "project:\n  type: website\n  output-dir: _site\n  pre-render: [\"rm -rf _site/*\"]  # Clean build\n  resources:\n    - CNAME\n    - favicon-32x32.png\n    - favicon.ico\n    - apple-touch-icon.png\n\nwebsite:\n  title: \"Jitin Kapila | The AI Architect\"\n  site-url: https://jitinkapila.com\n\n  navbar:\n    background: transparent  # Let CSS control this\n    left:\n      - text: \" \"  # Blank for logo/brand\n        href: index.qmd\n    right:\n      - text: \"The Protocol\"\n        href: bootcamp.qmd\n      # ... other nav items\n\n  page-footer:\n    left: \"© 2025 Jitin Kapila | The AI Architect\"\n    right:\n      - icon: linkedin\n        href: https://linkedin.com/in/jitinkapila\n      # ... other social icons\n\nformat:\n  html:\n    theme: brite  # Can also use: cosmo, flatly, litera\n    css: custom.css  # Or use SCSS with theme: [custom.scss]\n    include-in-header:\n      - includes/umami.html\n      - includes/schema.html\n    toc: false  # Disable by default, enable per-page\n    code-fold: true\n    highlight-style: github\n    smooth-scroll: true\n\nexecute:\n  freeze: auto  # Cache computational output\n  cache: true\n\ngoogle-analytics: \"G-XXXXXXXXXX\"  # Replace with actual ID\n\n\n\n\nTheme Selection: Use Bootstrap-based themes (cosmo, brite, flatly, litera)\nCSS Loading Order: Theme → CSS → Page-specific styles\nFreeze Strategy: freeze: auto prevents re-running code unnecessarily\nResources: Always include CNAME, favicons for deployment"
  },
  {
    "objectID": "CLAUDE.html#working-with-blog-posts",
    "href": "CLAUDE.html#working-with-blog-posts",
    "title": "CLAUDE.md",
    "section": "",
    "text": "---\ntitle: \"Post Title\"\ndescription: \"Brief description for listings\"\ndate: \"YYYY-MM-DD\"\ncategories: [strategy, code]  # For tab filtering\ntags: [ai, ml, enterprise]\nimage: \"img/feature.jpg\"\nimage-alt: \"Alt text\"\ndraft: false  # Set true to hide\nauthor: \"Jitin Kapila\"  # Inherited from posts/_metadata.yml\n---\n\n\n\nposts/_metadata.yml sets defaults for ALL posts:\nfreeze: true\nauthor: 'Jitin Kapila'\ntoc: true\ntoc-depth: 3\ntoc-location: left\npage-layout: article\nreference-location: margin  # Tufte-style margin notes\ncitation-location: margin\nlightbox: true\nlicense: \"CC BY\"\ncode-line-numbers: true\ncode-fold: true\ncomments:\n  utterances:\n    repo: jkapila/perceptions\n    theme: github-light\n\n\n\nThe Vault uses categories to separate posts:\n\nStrategy Tab: categories: [strategy, business, roi]\nEngineering Tab: categories: [code, llm, agents]\n\nNote: Posts can appear in multiple tabs if they have multiple categories."
  },
  {
    "objectID": "CLAUDE.html#advanced-quarto-features",
    "href": "CLAUDE.html#advanced-quarto-features",
    "title": "CLAUDE.md",
    "section": "",
    "text": "Quarto uses fenced divs for layout control:\n::: {.hero-section}\nContent here\n:::\n\n::: {.column-body-outset}\nWide content that extends beyond normal margins\n:::\n\n::: {.panel-tabset}\n## Tab 1\nContent\n## Tab 2\nMore content\n:::\n\n\n\n\n.column-body - Normal width (default)\n.column-body-outset - Slightly wider\n.column-page - Page width\n.column-screen - Full screen width\n.column-margin - Margin notes (Tufte style)\n\n\n\n\nFor deep customization, use template partials (see extras/lumo theme/):\nformat:\n  html:\n    template-partials:\n      - title-block.html  # Custom title block\n      - footer.html       # Custom footer"
  },
  {
    "objectID": "CLAUDE.html#brand.yml-implementation-recommended-future",
    "href": "CLAUDE.html#brand.yml-implementation-recommended-future",
    "title": "CLAUDE.md",
    "section": "",
    "text": "Quarto 1.4+ supports brand.yml for centralized design tokens. This is the FUTURE direction:\n\n\nmeta:\n  name: \"The AI Architect\"\n  link: \"https://jitinkapila.com\"\n\ncolor:\n  palette:\n    navy-deep: \"#0C2B4E\"\n    navy-royal: \"#213555\"\n    cream-gold: \"#D8C4B6\"\n  background: \"#FAFBFC\"\n  foreground: \"#0C2B4E\"\n  primary: \"#0C2B4E\"\n  secondary: \"#213555\"\n\ntypography:\n  fonts:\n    - family: \"Bebas Neue\"\n      source: google\n    - family: \"IBM Plex Sans\"\n      source: google\n      weight: [400, 500, 600]\n  base-size: \"16px\"\n  headings:\n    family: \"Bebas Neue\"\n    weight: 400\n  body:\n    family: \"IBM Plex Sans\"\n    weight: 400\n    line-height: 1.7\n\n\n\n\nSingle Source of Truth: All design tokens in one file\nCross-Format: Works across HTML, PDF, DOCX outputs\nTooling Integration: IDEs can autocomplete brand values\nTeam Collaboration: Designers can edit without touching code\n\nStatus: Not yet implemented. Consider for Phase 2 refactor."
  },
  {
    "objectID": "CLAUDE.html#seo-analytics",
    "href": "CLAUDE.html#seo-analytics",
    "title": "CLAUDE.md",
    "section": "",
    "text": "Located in includes/schema.html:\n&lt;script type=\"application/ld+json\"&gt;\n{\n  \"@context\": \"https://schema.org\",\n  \"@graph\": [\n    {\n      \"@type\": \"ProfessionalService\",\n      \"name\": \"Jitin Kapila - The AI Architect\",\n      \"founder\": {\n        \"@type\": \"Person\",\n        \"name\": \"Jitin Kapila\",\n        \"jobTitle\": \"AI Strategy Consultant\"\n      }\n    },\n    {\n      \"@type\": \"Course\",\n      \"name\": \"The AI Architect Protocol\",\n      \"provider\": { \"@type\": \"Person\", \"name\": \"Jitin Kapila\" }\n    }\n  ]\n}\n&lt;/script&gt;\n\n\n\nLocated in includes/umami.html - lightweight, privacy-focused alternative to Google Analytics.\n\n\n\nConfigured in _quarto.yml:\nwebsite:\n  open-graph: true\n  twitter-card:\n    creator: \"@jitinkapila\"\n    card-style: summary_large_image"
  },
  {
    "objectID": "CLAUDE.html#performance-optimizations",
    "href": "CLAUDE.html#performance-optimizations",
    "title": "CLAUDE.md",
    "section": "",
    "text": "execute:\n  freeze: auto  # Only re-run when source changes\n  cache: true   # Cache results\nWhen to Clear Cache:\nrm -rf _freeze/\nquarto render\n\n\n\n\nUse thumbnails for listings (-thumb.jpg suffix)\nCompress images before adding to repo\nEnable lightbox for blog images (lightbox: true)\n\n\n\n\n\nUse highlight-style: github for consistency\nEnable code-fold: true for long code blocks\nAdd code-line-numbers: true for readability"
  },
  {
    "objectID": "CLAUDE.html#common-tasks",
    "href": "CLAUDE.html#common-tasks",
    "title": "CLAUDE.md",
    "section": "",
    "text": "Edit _quarto.yml → website.navbar.right:\nnavbar:\n  right:\n    - text: \"New Page\"\n      href: newpage.qmd\n\n\n\nEdit _quarto.yml → page-footer.right:\npage-footer:\n  right:\n    - icon: youtube\n      href: https://youtube.com/@channel\n\n\n\n# Create file\ntouch newpage.qmd\n\n# Add frontmatter\n---\ntitle: \"Page Title\"\npage-layout: full  # or article, custom\n---\n\n\n\nEdit styles.scss → /*-- scss:defaults --*/:\n$primary: #0C2B4E;\n$secondary: #213555;\nThen in _quarto.yml:\nformat:\n  html:\n    theme: [cosmo, styles.scss]  # Theme + custom SCSS"
  },
  {
    "objectID": "CLAUDE.html#troubleshooting",
    "href": "CLAUDE.html#troubleshooting",
    "title": "CLAUDE.md",
    "section": "",
    "text": "Cause: Using theme: none removes Bootstrap navbar structure. Fix: Use a Bootstrap theme (cosmo, brite, flatly) + CSS overrides.\n\n\n\nCheck Order: 1. Is SCSS in /*-- scss:defaults --*/ section? 2. Is CSS loaded AFTER theme in _quarto.yml? 3. Are you using !important (symptom of wrong approach)?\nDebug:\nquarto preview --log-level debug\n\n\n\nSymptom: Old content showing despite changes. Fix:\nrm -rf _freeze/\nquarto render\n\n\n\nFix:\n# Kill all Quarto processes\npkill -f quarto\n\n# Restart preview\nquarto preview"
  },
  {
    "objectID": "CLAUDE.html#best-practices",
    "href": "CLAUDE.html#best-practices",
    "title": "CLAUDE.md",
    "section": "",
    "text": "Use SCSS variables instead of CSS custom properties for Bootstrap integration\nPrefer theme extension over complete override\nAvoid !important - it indicates fighting the cascade\nTest mobile-first - use responsive breakpoints\nKeep specificity low - let cascade work naturally\n\n\n\n\n\nUse divs for layout, not CSS classes in markdown\nEnable freeze for computational posts - save build time\nAdd alt text to all images - SEO + accessibility\nUse relative links for internal navigation\nTest locally before pushing - quarto preview\n\n\n\n\n\nOptimize images before committing (use WebP when possible)\nUse code-fold for long code blocks\nEnable caching for expensive computations\nMinimize custom fonts (currently 2 families - good)\nLazy load images in listings"
  },
  {
    "objectID": "CLAUDE.html#future-roadmap",
    "href": "CLAUDE.html#future-roadmap",
    "title": "CLAUDE.md",
    "section": "",
    "text": "Migrate to Bootstrap theme + CSS overrides\nFix navbar visibility\nConsolidate to SCSS-only approach\nRemove !important from CSS\n\n\n\n\n\nImplement brand.yml for design tokens\nCreate Quarto extension for Blueprint theme\nAdd custom template partials\nImplement custom listing layouts\n\n\n\n\n\nAdd service worker for offline\nImplement lazy loading for images\nOptimize build time (currently ~20s)\nAdd progressive enhancement features"
  },
  {
    "objectID": "CLAUDE.html#resources",
    "href": "CLAUDE.html#resources",
    "title": "CLAUDE.md",
    "section": "",
    "text": "Quarto Websites\nHTML Themes\nHTML Theme Customization\nBrand Guide\nHTML Basics\n\n\n\n\n\nextras/BRAND_MEMORY.md - Complete brand guide\nextras/design-tokens.json - Design system reference\nextras/lumo theme/ - Example custom Quarto extension\nbackup/ - Original files before rebuild\n\n\n\n\n\nQuarto Discussions\nAwesome Quarto"
  },
  {
    "objectID": "CLAUDE.html#quick-reference",
    "href": "CLAUDE.html#quick-reference",
    "title": "CLAUDE.md",
    "section": "",
    "text": "File\nPurpose\nEdit Frequency\n\n\n\n\n_quarto.yml\nSite config, nav, theme\nOccasionally\n\n\ncustom.css\nStyle overrides\nOften (to migrate to SCSS)\n\n\nstyles.scss\nSCSS variables/rules\nRecommended approach\n\n\nincludes/schema.html\nSEO structured data\nRarely\n\n\nincludes/umami.html\nAnalytics script\nNever (unless changing provider)\n\n\nposts/_metadata.yml\nBlog post defaults\nRarely\n\n\nindex.qmd\nHomepage\nOccasionally\n\n\nvault/index.qmd\nBlog listing\nRarely\n\n\n\n\n\n\n---\ntitle: \"Page Title\"\npagetitle: \"Browser Tab Title\"  # Overrides title for &lt;title&gt; tag\npage-layout: full  # full, article, custom\ntoc: true  # Enable table of contents\ntoc-location: left  # left, right, body\ndescription: \"Meta description for SEO\"\nimage: \"path/to/social-share.jpg\"\ndraft: false  # Hide page if true\n---\n\nThis document should be referenced when making ANY changes to the site architecture, styling, or content structure."
  },
  {
    "objectID": "posts/strategy/decision-first-ai/index.html",
    "href": "posts/strategy/decision-first-ai/index.html",
    "title": "Decision-First AI: Why Data Should Follow, Not Lead",
    "section": "",
    "text": "Start here: don’t open a notebook until you know the decision you want to change.\nSounds obvious. But most AI projects don’t start there. They start with a dataset, or with a “let’s try this model,” or with a platform demo that looks great in the cloud. And then weeks later the obvious question appears: “Okay — what decision does this support?” People shrug. The project stalls. The models are good. The business impact is vague.\nThis is the dataset-first trap. It wastes time, money, and faith. It also gives AI a bad name.\nI’ve seen the opposite work — a lot. Start with the decision. Map the decision. Then pick the simplest data and model that make the decision better. The result? Faster pilots, clearer ROI, and systems that actually get used.\nThat approach is not just a management neat-idea. There’s a real body of research showing that aligning models to downstream decision goals yields better decisions than optimizing prediction accuracy alone. And there are real, practical wins — from telecom fault detection to inventory systems — when you flip the order. arXiv+1"
  },
  {
    "objectID": "posts/strategy/decision-first-ai/index.html#the-dataset-first-trap-what-it-looks-like-and-why-it-hurts",
    "href": "posts/strategy/decision-first-ai/index.html#the-dataset-first-trap-what-it-looks-like-and-why-it-hurts",
    "title": "Decision-First AI: Why Data Should Follow, Not Lead",
    "section": "The “dataset-first” trap — what it looks like, and why it hurts",
    "text": "The “dataset-first” trap — what it looks like, and why it hurts\nHere’s the typical playbook I see in companies:\n\nSomeone discovers a new dataset.\nThey build dashboards, then a model, then a fine model, then a fancier model.\nThey show a demo. The demo gets applause. Then the work hits integration, governance, and the messy reality of people who must make decisions every day. The model’s outputs don’t map to a decision process. So adoption fails.\n\nWhy? Because the project optimized the wrong thing. It optimized prediction metrics — error, F1, AUC, MAPE. And those are useful. But they’re not the measure of business impact. A model with better accuracy can still be useless if it doesn’t change what someone does.\nHarvard Business Review captured this idea well: decisions don’t start with data. They start with a problem, a role, a process, and a behavior. If your analytics don’t connect to that reality, you get slides and disappointment. Harvard Business Review\nThere are more subtle costs too. Dataset-first projects often create models that are brittle in production: they overfit to historical quirks, they require constant data wrangling, and they produce numbers no one trusts. That kills scale."
  },
  {
    "objectID": "posts/strategy/decision-first-ai/index.html#decision-first-in-research-not-new-but-finally-practical",
    "href": "posts/strategy/decision-first-ai/index.html#decision-first-in-research-not-new-but-finally-practical",
    "title": "Decision-First AI: Why Data Should Follow, Not Lead",
    "section": "Decision-first in research: not new, but finally practical",
    "text": "Decision-first in research: not new, but finally practical\nThere’s academic grounding for starting with decisions. In the machine-learning community this shows up as “decision-focused learning” or “smart predict-then-optimize.” The idea: train predictive models not for pure accuracy, but to minimize the loss that matters to the downstream optimization or decision task. When you optimize directly for the decision loss, you often get better business outcomes — even with “worse” prediction metrics. arXiv+1\nRecent papers and reviews show both the theory and practical methods: surrogate losses that reflect decision outcomes, techniques to differentiate through optimization, and heuristics for discrete problems. The takeaway: the math supports the intuition. If you want a model to help choose inventory levels, price points, or routing, train it with that decision in mind — not just with RMSE. Optimization Online+1\nThat doesn’t mean every model must be complex. Often the opposite. Framing the decision reduces model complexity because you only model what matters. This is your Occam’s Razor"
  },
  {
    "objectID": "posts/strategy/decision-first-ai/index.html#the-decision-map-simple-framework-you-can-use-today",
    "href": "posts/strategy/decision-first-ai/index.html#the-decision-map-simple-framework-you-can-use-today",
    "title": "Decision-First AI: Why Data Should Follow, Not Lead",
    "section": "The Decision Map: simple framework you can use today",
    "text": "The Decision Map: simple framework you can use today\nIf you want to flip to decision-first, start with a small, disciplined tool I call a Decision Map. It’s a one-page artefact. Build it before you touch data.\nHere’s the Decision Map — six steps. Do them in order.\n\nName the decision. Who decides, how often, and what options do they choose? Example: “Field-ops decides whether to dispatch a technician to a suspected DSL fault.” Be specific. Frequency matters — hourly, daily, weekly change what you can do.\nDefine the decision metric(s).What counts as success? Lower cost? Faster response time? Increase in net revenue? Pick one primary metric and one secondary. If you can’t name it in a single measurable sentence, you don’t have a decision.\nMap the current process. Where is the decision made today? Which people and tools are involved? Where does data enter? Where do delays happen? This step exposes the friction you must remove.\nIdentify the minimal action the model must trigger. The model doesn’t need to be perfect. It needs to change behavior. If the model’s output is a probability, what threshold triggers action? Who gets the alert? What’s the handoff?\nList the minimal signals (data) needed. Only include data that directly reduces uncertainty for the decision. You’ll be surprised how small this list often is. Think: signal → action. Not “all the data.”\nPlan the feedback loop. How will you measure the decision metric after deployment? How will you collect labels and iterate? Decide that upfront.\n\nIf you complete this map, you’ll have done 80% of the work most teams skip. It forces alignment, and it reveals whether the project is worth doing."
  },
  {
    "objectID": "posts/strategy/decision-first-ai/index.html#a-telecom-example-mapped-end-to-end-real-story",
    "href": "posts/strategy/decision-first-ai/index.html#a-telecom-example-mapped-end-to-end-real-story",
    "title": "Decision-First AI: Why Data Should Follow, Not Lead",
    "section": "A telecom example, mapped end-to-end (real story)",
    "text": "A telecom example, mapped end-to-end (real story)\nA brief, real example matters. I built a real-time anomaly detection system for a European telecom client early in my career. The project didn’t begin with “we have logs.” It began with this decision map:\n\nDecision: Should operations dispatch a field technician proactively for a suspected DSL fault?\nMetric: Reduce customer reported faults and improve Net Promoter Score (NPS) by reducing time-to-detect. Also: monthly cost savings from fewer reactive truck rolls and more planned truck rolls.\nProcess: Operations received the customer call, created a ticket, and dispatched if needed — often hours or days later. That was slow and expensive.\n\nAction: If the system flags an anomaly with high confidence\nRed : Very high probability in next 48 hours,\nAmber: probability in next 2-15 days\nGreen: No visibility of error in next 15 days This creates a high-priority ticket and dispatch a remote check or technician and was planned in region wise manner.\n\nSignals: DSL line metrics, error rates, device telemetry, event logs — a handful of streams, not every log.\nFeedback: Compare flagged incidents to customer complaints and adjust thresholds.\n\nBecause the decision was so clear, we could measure value before full scale. The pilot cut detection time from days to hours, raised customer satisfaction significantly ( We sent messages to possible signal disruption early), and saved ~£80K per month ( because reducing complete breakdown to DSL by heat or so and by planning the route than going everywhere) . It wasn’t an exotic model (or may be it was, tech details some other day); it was a tightly scoped system that informed a clear action.\nNotice how this maps to the Decision-First steps. The model existed to change a single operational choice. That focus made deployment possible and measurable fast."
  },
  {
    "objectID": "posts/strategy/decision-first-ai/index.html#another-quick-example-inventory-decisions",
    "href": "posts/strategy/decision-first-ai/index.html#another-quick-example-inventory-decisions",
    "title": "Decision-First AI: Why Data Should Follow, Not Lead",
    "section": "Another quick example: inventory decisions",
    "text": "Another quick example: inventory decisions\nInventory forecasting is a classic area where decision-first matters. You can chase lower MAPE and never change stocking policy. Or you can ask: what decision do merchandisers make with this forecast? When you frame it as “which SKUs do we order for next week, and what reorder points trigger expedited shipments,” you design the forecast differently: shorter horizons, bias for understock on fast movers, and direct constraints on reorder costs.\nI’ve led projects that delivered $11M in inventory optimization by building forecasts and decision rules that match merchant behavior and supply constraints. The trick was not better models — it was framing forecasts so the merchandisers could act with confidence."
  },
  {
    "objectID": "posts/strategy/decision-first-ai/index.html#practical-tips-for-teams-do-this-in-week-one",
    "href": "posts/strategy/decision-first-ai/index.html#practical-tips-for-teams-do-this-in-week-one",
    "title": "Decision-First AI: Why Data Should Follow, Not Lead",
    "section": "Practical tips for teams (do this in week one)",
    "text": "Practical tips for teams (do this in week one)\n\nRun a one-hour Decision Map workshop. Invite the decision owner, one operator, one engineer, and one product owner. Build the one-page map. If the owner can’t commit to a metric, pause the project.\nStart with a simple rule baseline. Before modeling, define a rule that will be your baseline (e.g., “if X &gt; T, create ticket”). If the model can’t beat that rule in decision impact, scrap it.\nMeasure decision impact, not model accuracy. Your dashboard should show business metric delta — not just RMSE. If you show the board a change in cost or conversion, you’ll get attention.\nPrioritize deployment constraints. Decide telemetry, latency, and handoff requirements first. Models that can’t meet latency or trust constraints are useless no matter how accurate.\nIterate with real feedback. Don’t wait for “perfect.” Ship an MVP that can be measured, then refine. Real decisions provide labels and operational learning."
  },
  {
    "objectID": "posts/strategy/decision-first-ai/index.html#common-objections-and-how-to-handle-them",
    "href": "posts/strategy/decision-first-ai/index.html#common-objections-and-how-to-handle-them",
    "title": "Decision-First AI: Why Data Should Follow, Not Lead",
    "section": "Common objections and how to handle them",
    "text": "Common objections and how to handle them\n“But we don’t have a clear decision owner.” Then don’t build a model. Decisions live in roles. Pull the right owner in early, or you’ll build for nobody.\n“Our data is messy.” Fine. If you can define the minimal signals, you can often create a proxy or start with manual labels. Messy data is easier to handle when you only need a few signals for a specific decision.\n“We need predictions for many uses.” Build a simple decision-first pilot first. Use its success to fund broader platform work. Pilots create proof that unlocks investment.\n“Decision-focused methods are academic — too hard.” There’s truth and myth here. The academic techniques show big wins when decision loss can be written down. But you don’t need complicated differentiable optimization to start. Use a decision map, simple thresholds, A/B tests, and iterative measurement. Graduate to decision-focused training once you have a stable objective. The research just tells us — unsurprisingly — that when you train with the decision in mind, outcomes improve. Optimization Online+1"
  },
  {
    "objectID": "posts/strategy/decision-first-ai/index.html#one-page-checklist-copy-this",
    "href": "posts/strategy/decision-first-ai/index.html#one-page-checklist-copy-this",
    "title": "Decision-First AI: Why Data Should Follow, Not Lead",
    "section": "One-page checklist (copy this)",
    "text": "One-page checklist (copy this)\n\nDecision name (The GOTO problem statement): ____________________\nPrimary / Secondary metric (which can accounted for ROI calculation later ): _____\nDecision owner (I don’t want to debate on this): ________________________\nFrequency of predictions: real-time / hourly / daily / weekly / monthly\nAction / interventions which can be triggered by model: ____________________\nMinimal signals required( The core Data to begin with): ______________________\nBaseline rule (your fail-safe if everything goes wrong): ____________________\nFeedback source ( might be tricky, but you should have one): __________________\n\nIf you can fill this in, you’re set for a pilot."
  },
  {
    "objectID": "posts/strategy/decision-first-ai/index.html#final-note-start-small-measure-fast-then-scale-with-discipline",
    "href": "posts/strategy/decision-first-ai/index.html#final-note-start-small-measure-fast-then-scale-with-discipline",
    "title": "Decision-First AI: Why Data Should Follow, Not Lead",
    "section": "Final note — start small, measure fast, then scale with discipline",
    "text": "Final note — start small, measure fast, then scale with discipline\nThe decision-first approach is simple because business problems are simple when stated well. The hard part is discipline: saying no to shiny demos and yes to measurable change. Start with one decision that matters. Map it. Ship a small system that changes behavior. Measure the business metric. Iterate.\nResearch supports this: models trained with the decision in mind perform better on the actual outcomes you care about. And in practice, teams that flip the order — decision first, data second — get to value faster. arXiv+1\nIf you want help mapping a decision in your company, send me one line describing the decision and the current process. I’ll reply with the Decision Map template you can use in a one-hour workshop. Or if you want we can connect too.\nIf this post help you or think help someone in need, please do share it. Thanks!!!\n##Key sources & further reading Elmachtoub, A.N., & Grigas, P. — Smart “Predict, then Optimize” (foundational paper on decision-focused loss and SPO). arXiv Reviews and recent work on decision-focused learning (predict-and-optimize / decision-focused methods). arXiv+1 Harvard Business Review — Decisions Don’t Start with Data — on why framing the decision matters. Harvard Business Review (And — the telecom and inventory examples referenced above are from projects on my profile/resume: Ask, if you want more details !! )"
  },
  {
    "objectID": "posts/engineering/01_adaptive_regression/index.html",
    "href": "posts/engineering/01_adaptive_regression/index.html",
    "title": "Adaptive Regression",
    "section": "",
    "text": "Adapting path through mountains! Photo by Zülfü Demir📸"
  },
  {
    "objectID": "posts/engineering/01_adaptive_regression/index.html#introduction",
    "href": "posts/engineering/01_adaptive_regression/index.html#introduction",
    "title": "Adaptive Regression",
    "section": "Introduction",
    "text": "Introduction\nHere I am trying to express our logic to find such Observation. Lets dive in.\nThere are different value estimation technique like regression analysis and time-series analysis. Everyone of us has experimented on regression using OLS ,MLE, Ridge, LASSO, Robust etc., and also might have evaluated them using RMSE (Root Mean/Median Square Error), MAD (Mean/Median Absolute Deviation), MAE (Mean / Median Absolute Error) and MAPE (Mean/Median Absolute Percentage Error), etc…\nBut all of these gives a single point estimate that what is the overall error looks like. Just a different thought!! can we be sure that this single value of MAPE or MAE? How easy it is to infer that our trained model has fitted well across the distribution of dependent variable?\n\n\n\n\n\nPlot of Anscombe’s Quartet\n\n\n\n\n\nSome Descriptive Stats for Anscombe’s Quartet\n\n\nLet me give you a pretty small data-set to play with “The Anscombe’s quartet”. This is a very famous data-set by Francis Anscombe. Please refer the plots below to understand the distribution of y1, y2, y3, y4. Isn’t it different?\nWould the measure of central tendency and disportion be same for this data? I am sure none of us would believe but to our utter surprise we see all the descriptive stats are kind of same. Don’t believe me !!! Please see the results below ( Source: Wikipedia ):"
  },
  {
    "objectID": "posts/engineering/01_adaptive_regression/index.html#so-what-we-do-now",
    "href": "posts/engineering/01_adaptive_regression/index.html#so-what-we-do-now",
    "title": "Adaptive Regression",
    "section": "So what we do Now!",
    "text": "So what we do Now!\nAstonished !!! Don’t be. This is what has been hiding behind those numbers. And this is why we really won’t be able to cross certain performance level. Unless you change some features or even do a lot of hyper parameter tuning, your results won’t vary much.\nIf you look at the average value of MAPE in each decile you would see an interesting pattern. Let us show you what we see that pattern. One day while working on a business problem where I was using regression on a discussion with Kumarjit, we deviced a different way of model diagnosis. We worked together to give this a shape and build on it.\n\nAs you can see it is absolutely evident that either of the side in the distribution of MAPE values is going wild!!!!!!! Still overall MAPE is good (18%)."
  },
  {
    "objectID": "posts/engineering/01_adaptive_regression/index.html#seeking-scope-of-improvement",
    "href": "posts/engineering/01_adaptive_regression/index.html#seeking-scope-of-improvement",
    "title": "Adaptive Regression",
    "section": "Seeking Scope of Improvement",
    "text": "Seeking Scope of Improvement\nWe worked together to build a different framework to address such issues on the go and reduce the MAPE deterioration on the edge of the distribution.\nThis problems gives rise to a concept we named as Distribution Assertive Regression (DAR).\nDAR is a framework that is based on cancelling the weakness of one point summaries by using the classical concepts of Reliability Engineering : The Bath Tub Curve.\n\n\n\n\n\nPlot for Classical Bath Tub Curve using a Hazard Function\n\n\nThe Specialty of this curve is that it gives you the likelihood which areas one tends to have high failure rates. In our experiments when we replace failure with MAPE value and the Time with sorted (ascending) value of target / dependent variable, we observe the same phenomenon. This is likely to happen because most of regression techniques assumes Normal (Gaussian) Distribution of data and fits itself towards the central tendency of this distribution.\nBecause of this tendency, any regression methods tends to learn less about data which are away from the central tendency of the target.\nLets look at BostonHousing data from “mlbench” package in R.\n\n\n\nPlot for MAPE Bath Tub Curve for Decile Split “mdev” from Data\n\n\nHere the MAPE is calculated for each decile split of ordered target variable. As you can observe it is following the bath tub curve. Hence the validates our hypothesis that the regression method is not able to understand much about the data at the either ends of the distribution."
  },
  {
    "objectID": "posts/engineering/01_adaptive_regression/index.html#final-analysis",
    "href": "posts/engineering/01_adaptive_regression/index.html#final-analysis",
    "title": "Adaptive Regression",
    "section": "Final Analysis",
    "text": "Final Analysis\nNow the DAR framework essentially fixes this weakness of regression method and understands the behavior of data which is stable and can be tweak in a fashion that can be use in general practice.\nPlot of MAPE Bath Tub Curve after applying DAR Framework for Decile Split “mdev” from Data\n\nHow this framework with same method reduced MAPEs so much and made model much more stable…?? Well here it is:\nThe DAR framework splits the data at either ends of the order target variable and performs regression on these “split” data individually. This inherently reduces the so called “noise” part of the data and treat it as an individual data."
  },
  {
    "objectID": "posts/engineering/01_adaptive_regression/index.html#scoring-on-new-data",
    "href": "posts/engineering/01_adaptive_regression/index.html#scoring-on-new-data",
    "title": "Adaptive Regression",
    "section": "Scoring on New Data",
    "text": "Scoring on New Data\nNow you might be thinking while applying regression this sounds good but how will one score this on new data. Well to answer that we used our most simple yet very effective friend “KNN” (Though any multiclass Classifier can be used here). So ideally scoring involves two step method :\n\nScore new value against each KNN / Multiclass Classifier model of the data\nBased on closeness we score it with the regression method used for that part of data.\n\nSo now we know how we can improve the prediction power of data for regression."
  },
  {
    "objectID": "posts/engineering/01_adaptive_regression/index.html#code-and-flowchart",
    "href": "posts/engineering/01_adaptive_regression/index.html#code-and-flowchart",
    "title": "Adaptive Regression",
    "section": "Code and Flowchart",
    "text": "Code and Flowchart\nIf things are simple lets keep it simple. Refer flowchart and code below for implementation of this framework. Paper here!\n\nR codePython codeHere is the Flow Chart\n\n\n\n\n\n\n\n\nTipClick to Expand\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipClick to Expand\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngraph TB\n    \n    subgraph Testing\n        p1(Finding bucket of model to choose)\n        p1 --&gt; p2([Making predictions &lt;br&gt; based on selected model for inference])\n        p2 --&gt; p3(Consolidate final score of prediction)\n    end\n\n    subgraph Training\n        md([Fitting a &lt;br&gt;Regression model])==&gt; di\n        di{Binning Data via &lt;br/&gt; evaluating Distribution &lt;br/&gt; MAPE values }\n        di --&gt; md2([Fitting a Buckteing model &lt;br/&gt; to Binned MAPE Buckets])\n        md2 --&gt; md3([Fitting Regression &lt;br&gt; Models on Binned Data])\n        md == Keeping main&lt;br/&gt;model ==&gt; ro        \n        md3 ==&gt; ro(Final Models &lt;br&gt; Binning Data Models + &lt;br&gt; Set of Regressoin Models)\n    end\n\n    \n    od([Data Input]) -- Training&lt;br&gt; Data--&gt; md\n    od -- Testing&lt;br&gt; Data--&gt; p1\n    ro -.-&gt; p1\n    ro -.-&gt; p2\n\n    classDef green fill:#9f6,stroke:#333,stroke-width:2px;\n    classDef yellow fill:#ff6,stroke:#333,stroke-width:2px;\n    classDef blue fill:#00f,stroke:#333,stroke-width:2px,color:#fff;\n    classDef orange fill:#f96,stroke:#333,stroke-width:4px;\n    class md,md2,md3 green\n    class di orange\n    class p1,p2 yellow\n    class ro,p3 blue"
  },
  {
    "objectID": "posts/engineering/03_crosstab_sparsity/index.html",
    "href": "posts/engineering/03_crosstab_sparsity/index.html",
    "title": "CrossTab Sparsity",
    "section": "",
    "text": "Cluster analysis has always fascinated me as a window into the hidden structures of data. During my collaboration with Kumarjit Pathak, we grappled with a persistent challenge in unsupervised learning: how to objectively evaluate clustering quality across different algorithms. Traditional metrics like the Silhouette Index or Bayesian Information Criterion felt restrictive—they were siloed within specific methodologies, making cross-algorithm comparisons unreliable.\nThis frustration led us to develop a universal cluster evaluation metric, detailed in our paper “Cross Comparison of Results from Different Clustering Approaches”. Our goal was to create a framework that transcends algorithmic biases, enabling:\n- Direct comparison of K-Means vs GMM vs DBSCAN vs PAM vs SOM vs Anything results\n- Identification of variables muddying cluster separation\n- Automated determination of optimal cluster counts\nIn this blog, I’ll walk you through our journey—from conceptualization to real-world validation—and share insights that didn’t make it into the final paper."
  },
  {
    "objectID": "posts/engineering/03_crosstab_sparsity/index.html#our-aha-moment---crosstab-sparsity",
    "href": "posts/engineering/03_crosstab_sparsity/index.html#our-aha-moment---crosstab-sparsity",
    "title": "CrossTab Sparsity",
    "section": "Our “Aha!” Moment - Crosstab Sparsity",
    "text": "Our “Aha!” Moment - Crosstab Sparsity\n\n\n\n\nBest Cluster for K-means Using Crosstab sparsity\n\n\n\nWhile analyzing cross-tab matrices of variable distributions across clusters, we noticed a pattern: well-segregated clusters consistently showed higher frequencies along matrix diagonals. This inspired our two-part metric:\n\n\n\nSegregation Factor:\n# Simplified calculation from our codebase  \nmedian = np.median(cross_tab)  \nN_vk = np.sum(cross_tab &gt; median)  # Count \"well-segregated\" instances  \nExplanation Factor:\nexplanation = np.log(len(data) / (bins * clusters))  \n\n\nSegregation Factor: Measures how distinctly clusters separate data points. We used the median (not mean) to avoid skew from outlier-dominated matrices.\nExplanation Factor: Quantifies how well clusters capture data variability. The logarithmic term penalizes overfitting—a critical insight from debugging early over-segmented clusters.\n\nAnd the Final Formula:\nFor variable \\(v\\) with \\(k\\) clusters:\n\\[\nS_v^k = \\underbrace{\\frac{N_v^k}{\\max(l, k)}}_{\\text{Segregation}} \\times \\underbrace{\\ln\\left(\\frac{N_d}{l \\times k}\\right)}_{\\text{Explanation}}\n\\]\nwhere:\n- \\(N_v^k\\): Segregated instances (values above cross-tab matrix median)\n- \\(l\\): Number of value intervals for variable \\(v\\)\n- \\(N_d\\): Total observations\nThis formulation ensures algorithmic invariance, allowing comparison across methods like K-Means (distance-based) and GMM (probability-based). Also, now you can see from the formula two scenarios happens: 1. If each variable crosstab is too dense then their is no separation between classes 2. If each variable crosstab is too sparse then we loose on explanation.\nHence the curve reaches a maximum and then falls down giving use the separability that the cluster can produce:"
  }
]