[
  {
    "objectID": "webinar/index.html",
    "href": "webinar/index.html",
    "title": "Webinar",
    "section": "",
    "text": "Webinar Content\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Don’t Learn AI. Operationalize It.",
    "section": "",
    "text": "15+ Years\nSystems Architecture & AI Strategy\n\n\n$80M+ Deployed\nFortune 500 AI Projects\n\n\n50+ Trained\nC-Suite Leaders\n\n\n10 hrs/week Saved\nProven In 30 Days\n\n\n\n  \n  \n    \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n\n\n\n\nYou’re under pressure. Your CEO is asking about AI. Your competition is moving faster.\nEveryone’s selling you tools. Nobody’s teaching you architecture.\nThe gap isn’t tools. It’s strategy.\nSenior professionals aren’t being replaced by AI. They’re being replaced by junior professionals who know how to use it."
  },
  {
    "objectID": "index.html#why-leaders-trust-this",
    "href": "index.html#why-leaders-trust-this",
    "title": "Don’t Learn AI. Operationalize It.",
    "section": "",
    "text": "15+ Years\nSystems Architecture & AI Strategy\n\n\n$80M+ Deployed\nFortune 500 AI Projects\n\n\n50+ Trained\nC-Suite Leaders\n\n\n10 hrs/week Saved\nProven In 30 Days"
  },
  {
    "objectID": "index.html#the-problem",
    "href": "index.html#the-problem",
    "title": "Don’t Learn AI. Operationalize It.",
    "section": "",
    "text": "You’re under pressure. Your CEO is asking about AI. Your competition is moving faster.\nEveryone’s selling you tools. Nobody’s teaching you architecture.\nThe gap isn’t tools. It’s strategy.\nSenior professionals aren’t being replaced by AI. They’re being replaced by junior professionals who know how to use it."
  },
  {
    "objectID": "index.html#this-is-for-you-if",
    "href": "index.html#this-is-for-you-if",
    "title": "Don’t Learn AI. Operationalize It.",
    "section": "This Is For You If:",
    "text": "This Is For You If:\n→ C-suite, VP, or department head with decision-making authority\n→ 12+ years of professional experience\n→ Managing teams, budgets, or strategic initiatives\n→ You’ve watched competitors move faster and asked: “What do we do first?” → You and/or your team struggling to leverage AI\n\nNot For You If:\nYou’re looking for “AI tools” or “ChatGPT shortcuts.” We teach systems thinking, not prompts."
  },
  {
    "objectID": "index.html#what-you-build-the-ai-profit-os",
    "href": "index.html#what-you-build-the-ai-profit-os",
    "title": "Don’t Learn AI. Operationalize It.",
    "section": "What You Build: The AI Profit OS™",
    "text": "What You Build: The AI Profit OS™\n3 days. 3 deliverables.\n\n1. Strategy Framework\nA decision tree for any AI opportunity in your business:\nCan AI solve this? Should we? What’s the cost?\n\n\n2. Execution Roadmap\nStep-by-step plan for your first AI use case.\nYou design. Your team builds. No consultant needed.\n\n\n3. ROI Model\nThe justification leadership wants to see.\nBefore/after impact. Timeline. Break-even. Confidence level."
  },
  {
    "objectID": "index.html#what-happens-next",
    "href": "index.html#what-happens-next",
    "title": "Don’t Learn AI. Operationalize It.",
    "section": "What Happens Next",
    "text": "What Happens Next\n\nDays 1–3: You attend live. You design your first use case.\nDays 4–30: Your team executes the roadmap. You ship something real.\nDay 31+: You measure results. You architect the next use case.\nLifetime: Access to 2x/month coaching sessions + private community.\nFormat: Live Virtual | Dates: January 09-11, 2026 | Seats: Limited"
  },
  {
    "objectID": "index.html#investment",
    "href": "index.html#investment",
    "title": "Don’t Learn AI. Operationalize It.",
    "section": "Investment: ₹24,978",
    "text": "Investment: ₹24,978\nIncludes:\n- 3 days live instruction\n- Recordings if you miss a session\n- Daily assignments + worksheets\n- Private community access\n- Lifetime group coaching (2x/month)"
  },
  {
    "objectID": "index.html#the-guarantee",
    "href": "index.html#the-guarantee",
    "title": "Don’t Learn AI. Operationalize It.",
    "section": "The Guarantee",
    "text": "The Guarantee\nAttend all 3 days. Complete the work. If you don’t map at least one viable AI use case + create a 30–90 day roadmap, I’ll refund 100% + pay you ₹2,500.\nThis is a performance guarantee, not a refund policy. Your success is my incentive."
  },
  {
    "objectID": "index.html#ready",
    "href": "index.html#ready",
    "title": "Don’t Learn AI. Operationalize It.",
    "section": "Ready?",
    "text": "Ready?\n\nSecure Your Seat - ₹24,978\nLimited seats. Next cohort: January 09-11, 2026. Regular Price : ₹42,978"
  },
  {
    "objectID": "index.html#where-this-works",
    "href": "index.html#where-this-works",
    "title": "Don’t Learn AI. Operationalize It.",
    "section": "Where This Works",
    "text": "Where This Works\nFortune 500 Ops Team: 8 Weeks to Shipped\nProblem: 60+ hours/week of manual planning. Consultants wanted $100K+ and 12 weeks.\nWhat Changed: Team attended cohort. Built architecture in 3 days. Shipped MVP in 8 weeks.\nOutcome: 30+ hours/week saved. $100K in consulting fees avoided.\nLesson: Non-technical operators can architect AI when they have a framework.\nRead the full case study →"
  },
  {
    "objectID": "index.html#common-questions",
    "href": "index.html#common-questions",
    "title": "Don’t Learn AI. Operationalize It.",
    "section": "Common Questions",
    "text": "Common Questions\nQ: I don’t have a technical background. Can I do this?\nA: Yes. You’re learning to architect like an executive, not code. That’s your job already.\n\nQ: What if my team doesn’t execute?\nA: Then it’s on you (leadership). But the roadmap is specific enough that your CTO can build it in 30 days.\n\nQ: Can I scale this to multiple departments?\nA: You architect one use case at a time. After the first ships, you can architect the next 3.\n\nQ: How is this different from hiring a consultant?\nA: A consultant does the work for you. This teaches you to do it. Plus, ₹24,999 vs. $100K."
  },
  {
    "objectID": "index.html#stop-guessing.-start-architecting.",
    "href": "index.html#stop-guessing.-start-architecting.",
    "title": "Don’t Learn AI. Operationalize It.",
    "section": "Stop Guessing. Start Architecting.",
    "text": "Stop Guessing. Start Architecting.\n\n\n  January cohort starts in 17 days\n\n\n\n\n\nSecure Your Seat Now - for January Cohort"
  },
  {
    "objectID": "index.html#subscribe-to-updates",
    "href": "index.html#subscribe-to-updates",
    "title": "Don’t Learn AI. Operationalize It.",
    "section": "Subscribe to Updates ",
    "text": "Subscribe to Updates \n\n\nTired of AI news that doesn’t apply to your business? Get curated insights, strategic frameworks, and real-world applications every week. Only the insights that matter.\n\n\n\n\n\n\n  Email Address"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Get in Touch",
    "section": "",
    "text": "Want to explore AI opportunities before committing to consulting? Subscribe for curated case studies, frameworks, and real-world examples from my work.\nFor information about courses and training programs, please visit our Courses page.\n\n\n   \n\n\n\n\n\n\n\n\nFull Name \nEmail Address \nMessage\n\nSend message\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "cohort/index.html",
    "href": "cohort/index.html",
    "title": "Build Your AI Operating System for Business",
    "section": "",
    "text": "NEXT COHORT\nJanuary 09-11, 2026 7 PM–10 PM IST)\n\n\n\n\nFORMAT\nLive Virtual (3 consecutive evenings)\n\n\n\n\nInvestment\n₹24,978 (New Year Discount Applied. Next cohort: ₹42,978)\n\n\n\n\nSeats limited to 50 per cohort. January filling fast. Click above and get enrolled."
  },
  {
    "objectID": "cohort/index.html#for-senior-professionals-who",
    "href": "cohort/index.html#for-senior-professionals-who",
    "title": "Build Your AI Operating System for Business",
    "section": "FOR SENIOR PROFESSIONALS WHO…",
    "text": "FOR SENIOR PROFESSIONALS WHO…\n\n\n\n\n\n\nAre you a VP, C-suite executive, department head, or founder who:\n✓ Manage teams or portfolios with real P&L responsibility\n✓ Have 10+ years of professional experience (you’ve seen fads come and go)\n✓ Know AI is a competitive advantage, but you’re unsure what to implement or how\n✓ Are confident as a strategist, but feel lost on AI specifically\n✓ Want to lead with AI without becoming a data scientist\n\n\n\nIf this describes you, this cohort is designed for you.\nYou’re not looking for a “learn ChatGPT” tutorial. You’re looking for framework—a system to evaluate any AI opportunity, quantify the ROI, and hand it off to your team for execution.\nThat’s exactly what this 3-day intensive delivers."
  },
  {
    "objectID": "cohort/index.html#this-is-not-for-you-if",
    "href": "cohort/index.html#this-is-not-for-you-if",
    "title": "Build Your AI Operating System for Business",
    "section": "THIS IS NOT FOR YOU IF…",
    "text": "THIS IS NOT FOR YOU IF…\n\nSkip this cohort if you:\n✗ Are new to your field (less than 5 years experience) — you’ll get more value from foundational AI courses\n✗ Want to learn how to code or prompt-engineer — this is strategy, not technical implementation\n✗ Expect AI to do 100% of your job without you learning anything — that’s not how this works\n✗ Are looking for quick wins or “side hustle” income — we focus on sustainable competitive advantage\n✗ Won’t invest 2–3 hours/week for 30 days after the cohort — ongoing execution is required\n✗ Want theory and certifications instead of practical operational systems — we don’t do that\n\nNo hard feelings if this isn’t for you. There are great foundational courses out there. This is for builders who are ready to architect."
  },
  {
    "objectID": "cohort/index.html#what-youll-build-in-3-days",
    "href": "cohort/index.html#what-youll-build-in-3-days",
    "title": "Build Your AI Operating System for Business",
    "section": "WHAT YOU’LL BUILD IN 3 DAYS",
    "text": "WHAT YOU’LL BUILD IN 3 DAYS\n\nBy the end of 3 days, you walk away with 3 tangible, executable assets:"
  },
  {
    "objectID": "cohort/index.html#your-3-day-roadmap",
    "href": "cohort/index.html#your-3-day-roadmap",
    "title": "Build Your AI Operating System for Business",
    "section": "YOUR 3-DAY ROADMAP",
    "text": "YOUR 3-DAY ROADMAP\n\n\nDay 1: AI ART Matrix™ – Strategic Foundation\nWhat you’ll learn: How to evaluate ANY business problem and determine if AI is the right solution.\nThe framework: AI ART Matrix (Alignment, Readiness, and Transformation)\n\n1 hour teaching + practice: We walk through the framework with live examples. You’ll see how to ask the right questions about your own business.\n1 hour group breakout: You’ll work in small groups on a real use case. We’ll refine your thinking in real time.\n30 min Q&A: Ask me anything about your specific problems. We’ll help you think through them.\n\nBy end of Day 1: You’ll have evaluated 3–5 of your own business problems and ranked them by AI fit + strategic impact.\n\n\nDay 2: O.L.C.D Framework™ – Diagnostic Mapping\nWhat you’ll learn: How to map problems to actual AI use cases and understand what you’d need to execute.\nThe framework: O.L.C.D (Outcome, Logic, Capability, Data)\n\n1 hour teaching + practice: We walk through real client examples. You’ll see how different problems map to different AI solutions (some are ML, some are automation, some are pure workflow).\n1 hour group breakout: You’ll take your top use case from Day 1 and map it using O.L.C.D. We’ll uncover what data you have, what you’re missing, and what’s actually feasible.\n30 min Q&A: Get clarity on roadblocks, data gaps, or execution challenges.\n\nBy end of Day 2: You’ll have a diagnostic blueprint for your top 1–2 use cases (what to build, what data you need, who should own it).\n\n\nDay 3: ROI Mapping – Execution & Measurement\nWhat you’ll learn: How to quantify the business impact of your AI initiative and lock in stakeholder buy-in.\nThe methodology: Before/After ROI Model (time saved, cost reduced, revenue enabled, risk mitigated)\n\n1 hour teaching + practice: We walk through real ROI scenarios. You’ll see how to translate “saves time” into “saves ₹X” and how to build confidence intervals for your CFO.\n1 hour group breakout: You’ll build the ROI model for your top use case. What’s the before state? What’s the after state? What’s the timeline? What’s the investment required?\n1 hour Q&A + Roadmap finalization: We’ll lock in your 30–90 day execution plan. You’ll leave with a dated, accountable roadmap.\n\nBy end of Day 3: You’ll have a ROI-backed business case and a 90-day roadmap you can hand to your team or present to leadership."
  },
  {
    "objectID": "cohort/index.html#proof-this-actually-works",
    "href": "cohort/index.html#proof-this-actually-works",
    "title": "Build Your AI Operating System for Business",
    "section": "PROOF: THIS ACTUALLY WORKS",
    "text": "PROOF: THIS ACTUALLY WORKS\n\nYou might be thinking: “This sounds good in theory. Does it actually work?”\nHere are 3 real examples of what happened when senior professionals applied this framework:"
  },
  {
    "objectID": "cohort/index.html#what-past-leaders-say",
    "href": "cohort/index.html#what-past-leaders-say",
    "title": "Build Your AI Operating System for Business",
    "section": "WHAT PAST LEADERS SAY",
    "text": "WHAT PAST LEADERS SAY\n\n\n\n“I was skeptical that a 3-day course could teach me strategy. But the frameworks—the AI ART matrix and O.L.C.D—they actually changed how I think about AI in our business. Within 30 days, I’d identified 3 legitimate use cases and got my team aligned on the roadmap. This isn’t a course. It’s a thinking system.”\n\n— [VP of Operations, Fortune 500 Tech Company]\n\n\n“Every consultant we talked to quoted ₹100K+ and 6 months. The frameworks I learned here, I could apply immediately. I didn’t need to hire anyone. My team and I built the first use case in 8 weeks. This is a cheat code for non-technical leaders.”\n\n— [Chief Financial Officer, Mid-Market Services]\n\n\n“I was drowning in AI jargon. This cut through all of it. Instead of learning tools, I learned how to think about problems strategically. The ROI framework is what sold my leadership team. Worth every rupee.”\n\n— [Head of Product, VC-Backed SaaS]\n\nMore testimonials coming after January cohort concludes. Previous 1-on-1 students available for reference upon request.\n\n\nSecure Your Seat – ₹24,978"
  },
  {
    "objectID": "cohort/index.html#your-investment-whats-included",
    "href": "cohort/index.html#your-investment-whats-included",
    "title": "Build Your AI Operating System for Business",
    "section": "YOUR INVESTMENT & WHAT’S INCLUDED",
    "text": "YOUR INVESTMENT & WHAT’S INCLUDED\n\n\n\nThe Investment: ₹24,978 (January Cohort – Founding Rate)\nNext cohort: ₹42,978. Price increases as demand grows.\n\n\n\nHere’s What’s Included:\nThe Live Experience:\n\n3 consecutive evening sessions (7 - 10 PM IST January 09-11, 2026)\n\nLive instruction, group breakouts, live Q&A\n\nAccess to recordings for 2 weeks post-cohort (lifetime access available if preferred)\n\nYour Assets (You Take These With You):\n\nReady-to-use Prompt Library (50+ prompts for AI problem evaluation)\n\nStrategy Framework Worksheets (AI ART matrix, O.L.C.D, ROI templates)\n\nLive Excel Tracker (ROI measurement sheet + use case roadmap template)\n\nDiagnostic Playbook (step-by-step decision framework)\n\nOngoing Access:\n\nPrivate Community (ask me questions anytime; get peer feedback)\n\n2x/month Group Sessions (forever) – Check in on your progress, get clarity, refine strategy\n\nEmail Support (direct access for challenges you hit during implementation)\n\nNewsletter Subscription (stay updated on AI trends, new use cases, framework updates)\n\n\n\nYou’re not buying a course. You’re buying a thinking system + ongoing access to me for the next 90 days (and 2x/month after that)."
  },
  {
    "objectID": "cohort/index.html#the-guarantee-roi-or-your-money-back",
    "href": "cohort/index.html#the-guarantee-roi-or-your-money-back",
    "title": "Build Your AI Operating System for Business",
    "section": "THE GUARANTEE: ROI OR YOUR MONEY BACK",
    "text": "THE GUARANTEE: ROI OR YOUR MONEY BACK\n\nMy Reputation Is Built On ROI, Not Views\n\nHere’s the deal:\nIf you complete the cohort and follow the system, and after 30 days you haven’t:\n\nMapped at least one viable AI use case with clear ROI (before/after scenario modeled), AND\nCreated a 30–90 day roadmap you can hand to your team or leadership,\n\nI’ll refund you 100% of your investment AND pay you ₹2,499 directly for your time."
  },
  {
    "objectID": "cohort/index.html#common-questions",
    "href": "cohort/index.html#common-questions",
    "title": "Build Your AI Operating System for Business",
    "section": "COMMON QUESTIONS",
    "text": "COMMON QUESTIONS\n\n\n\n\nDo I need to know how to code?\n\n\n\nNo. The AI Profit OS is designed for senior professionals, not engineers. Everything is built in Excel + simple tools you already use. If you can use Google Sheets, you can build these systems.\n\n\n\n\n\nI’m not technical. Will I be able to follow?\n\n\n\nAbsolutely. This training is designed for business leaders, not data scientists. I’ve trained CFOs, COOs, and Senior Directors with zero technical background. If you can manage a P&L, you can operationalize AI.\n\n\n\n\n\nWhat if I can’t attend all 3 days live?\n\n\n\nYou’ll get lifetime recording access. However, the live workshops where you build YOUR specific systems are where the real value comes from. We strongly recommend attending live.\n\n\n\n\n\nIs this just ChatGPT training?\n\n\n\nNo. ChatGPT is one tool in the AI Profit OS, but the system is much bigger. You’ll learn: (a) How to structure your workflow for AI leverage, (b) How to build repeatable systems (not one-off prompts), (c) How to measure ROI and scale what works.\n\n\n\n\n\nWhat industries does this work for?\n\n\n\nIf you’re a senior professional doing knowledge work, this works. I’ve deployed AI systems across automotive, retail, FMCG, telecommunications, finance, and healthcare. The frameworks are industry-agnostic.\n\n\n\n\n\nWhat’s the refund policy?\n\n\n\nThe “Unheard Of” Guarantee (see above). If you implement the system and don’t save 10+ hours/week within 30 days, refund + ₹5,000 paid to you. After 30 days, no refunds - but you can transfer your seat to a colleague.\n\n\n\n\n\nCan I bring my team?\n\n\n\nYes! We offer team pricing for 3+ participants from the same organization. Email jitin@jitinkapila.com for custom team rates. (Recommended: Bring 2-3 key leaders to accelerate adoption.)\n\n\n\n\n\nHow is this different from other AI courses?\n\n\n\nMost AI courses teach theory or simple prompting. The AI Profit OS is a complete operational system you’ll deploy in your workflow. You’re not learning about AI - you’re building leverage. By Day 3, you have working systems saving you time.\n\n\n\n\n\nWhat if AI changes? Will this become outdated?\n\n\n\nThe OLCD Protocol, AI-ART Matrix, and operational frameworks are model-agnostic. Whether it’s ChatGPT, Claude, Gemini, or whatever comes next, the system adapts. Plus, you get lifetime access to framework updates.\n\n\n\n\n\nDo I need to be technical to do this?\n\n\n\nNo. This entire cohort is designed for non-technical people. You won’t code. You won’t write prompts. You’ll think strategically about where AI fits in your business and why.   The frameworks work whether you have a technical background or not. Your job is to architect. Your engineer’s job is to build.\n\n\n\n\n\nCan I attend if I’m new to AI?\n\n\n\nYes, as long as you have 10+ years of professional experience. You don’t need to know what ChatGPT is or how transformers work. You need to understand your business, your problems, and what good execution looks like.\n\n\n\n\n\nWhat if I can’t attend live? Can I watch recordings?\n\n\n\nPartially. Recordings are available for 2 weeks post-cohort so you can catch up on lectures. But the breakout room work and live Q&A are the most valuable part—those happen during live sessions only.   If live attendance is an issue, reach out before you enroll. We can discuss options.\n\n\n\n\n\nCan I bring my team?\n\n\n\nNo. This is designed for individual leaders. You become the AI strategist. You take this back to your team.   (If your team needs deep technical training on implementation, that’s a separate conversation—we have resources for that.)\n\n\n\n\n\nIs this just ChatGPT and prompts?\n\n\n\nNo. We don’t teach tools. We teach thinking. ChatGPT might be part of your solution, or it might not be. That depends on your use case.   What we do teach: How to think about problems strategically, how to evaluate which AI approaches fit, how to quantify ROI, and how to execute without hiring consultants.\n\n\n\n\n\nWhat happens after the 3 days?\n\n\n\nYou get: * Access to our community (ask questions, peer learning) * 2x/month group sessions with me (forever) – we review your progress, refine strategy, unblock challenges * Email access (for urgent questions) You’re responsible for execution with your team. I’m your strategic advisor, not your implementer.\n\n\n\n\n\nWhat if my AI implementation doesn’t work?\n\n\n\nThat’s not a refund trigger. The refund is if you can’t map a viable use case or create a roadmap.   If you map something and it doesn’t work in execution, that’s implementation, not strategy. In that case, you have 2x/month sessions to debug. We pivot and adjust.\n\n\n\n\n\nHow is this different from other AI courses?\n\n\n\nMost courses teach you tools: Here’s ChatGPT. Here’s how to write prompts. Here’s how to use AI tools.   We teach you thinking: Here’s how to evaluate any problem. Here’s how to map it to a solution. Here’s how to quantify ROI so your CFO approves it.   Most courses are for beginners. This is for leaders who already understand business—you just need the AI framework.\n\n\n\n\n\nIs this industry-agnostic?\n\n\n\nYes. AI is industry agnostic and we’ve worked with leaders in retail, finance, consulting, healthcare, tech, manufacturing, logistics. The frameworks work everywhere because they’re based on problems, not industries.\n\n\n\n\n\nWhat if I implement something and it doesn’t hit the ROI I modeled?\n\n\n\nThat’s real life. Models are approximations. But here’s what matters: You’ll have a diagnostic framework. If results are worse than expected, you’ll know how to ask the right questions to debug it.   That’s what the 2x/month sessions are for.\n\n\n\n\n\nDo I get a certificate?\n\n\n\nNo. Certificates are for validation. We focus on competence. After this cohort, you’ll be able to architect AI use cases for your business. That’s the real credential.\n\n\n\n\n\nWhat if I decide this isn’t right for me after I enroll?\n\n\n\nYou have a 7-day money-back guarantee. If you enroll and change your mind, get a full refund within 7 days. No questions asked.   (The ROI guarantee kicks in after you complete the cohort, as detailed above.)."
  },
  {
    "objectID": "cohort/index.html#ready-to-architect-your-ai-strategy",
    "href": "cohort/index.html#ready-to-architect-your-ai-strategy",
    "title": "Build Your AI Operating System for Business",
    "section": "READY TO ARCHITECT YOUR AI STRATEGY?",
    "text": "READY TO ARCHITECT YOUR AI STRATEGY?\n\nIn 3 days, you’ll know: - Which AI opportunities matter in your business - How to evaluate them rigorously - What the ROI actually looks like - Who should own execution - How to present it to leadership\nIn 30–90 days, your team will ship the first one.\nNo consultant. No waiting. No confusion.\nJanuary 09-11, 2026. Limited to 50 leaders per cohort.\n\nSecure Your Seat – ₹24,978\nNext cohort: ₹42,978. Invest early."
  },
  {
    "objectID": "cohort/index.html#still-on-the-fence",
    "href": "cohort/index.html#still-on-the-fence",
    "title": "Build Your AI Operating System for Business",
    "section": "Still On The Fence?",
    "text": "Still On The Fence?\nHave a question before you enroll?\n\n\nBook a 15-min call with me | Email me directly\n\nWe’ll make sure this is the right fit for you.\n\n\n\n\nI’m not sure this will work for my specific industry/role.\n\n\n\nThat’s exactly why Day 1 is focused on YOUR workflow and YOUR 90-day roadmap. The frameworks are industry-agnostic, but the implementation is personalized to your reality.\n\n\n\n\n\n feels expensive for a training.\n\n\n\nCompare it to: - (a) One hour of my consulting at ₹25,000/hour, -(b) The cost of being replaced by a 22-year-old who knows how to use AI, -(c) The ₹6.9Cr the grocery chain saved with this exact system. If you don’t save 10+ hours/week, I’ll refund you AND pay you ₹8,000.\n\n\n\n\n\nWhat if I can’t implement this alone?\n\n\n\nYou won’t be alone. You’ll join a private community of senior professionals building AI systems. Plus monthly office hours with me. And if you need hands-on implementation support, 1:1 consulting is available.\n\n\n\n\n\nHow do I know this isn’t just hype?\n\n\n\nLook at the results: ₹6.9Cr grocery chain savings. $11M automotive optimization. £80K/month retail revenue lift. These are real deployments at Mercedes, L’Oréal, Maruti, British Telecom. This isn’t theory - it’s operational leverage.\n\n\n\n\n\nWhat if AI replaces me anyway?\n\n\n\nAI will replace professionals who DON’T learn to operationalize it. Senior professionals who build AI systems become 10x more valuable. You’re not competing with AI - you’re building leverage ON TOP of your 10+ years of expertise."
  },
  {
    "objectID": "blog/strategy/decision-first-ai/index.html",
    "href": "blog/strategy/decision-first-ai/index.html",
    "title": "Decision-First AI: Why Data Should Follow, Not Lead",
    "section": "",
    "text": "Start here: don’t open a notebook until you know the decision you want to change.\nSounds obvious. But most AI projects don’t start there. They start with a dataset, or with a “let’s try this model,” or with a platform demo that looks great in the cloud. And then weeks later the obvious question appears: “Okay — what decision does this support?” People shrug. The project stalls. The models are good. The business impact is vague.\nThis is the dataset-first trap. It wastes time, money, and faith. It also gives AI a bad name.\nI’ve seen the opposite work — a lot. Start with the decision. Map the decision. Then pick the simplest data and model that make the decision better. The result? Faster pilots, clearer ROI, and systems that actually get used.\nThat approach is not just a management neat-idea. There’s a real body of research showing that aligning models to downstream decision goals yields better decisions than optimizing prediction accuracy alone. And there are real, practical wins — from telecom fault detection to inventory systems — when you flip the order. arXiv+1"
  },
  {
    "objectID": "blog/strategy/decision-first-ai/index.html#the-dataset-first-trap-what-it-looks-like-and-why-it-hurts",
    "href": "blog/strategy/decision-first-ai/index.html#the-dataset-first-trap-what-it-looks-like-and-why-it-hurts",
    "title": "Decision-First AI: Why Data Should Follow, Not Lead",
    "section": "The “dataset-first” trap — what it looks like, and why it hurts",
    "text": "The “dataset-first” trap — what it looks like, and why it hurts\nHere’s the typical playbook I see in companies:\n\nSomeone discovers a new dataset.\nThey build dashboards, then a model, then a fine model, then a fancier model.\nThey show a demo. The demo gets applause. Then the work hits integration, governance, and the messy reality of people who must make decisions every day. The model’s outputs don’t map to a decision process. So adoption fails.\n\nWhy? Because the project optimized the wrong thing. It optimized prediction metrics — error, F1, AUC, MAPE. And those are useful. But they’re not the measure of business impact. A model with better accuracy can still be useless if it doesn’t change what someone does.\nHarvard Business Review captured this idea well: decisions don’t start with data. They start with a problem, a role, a process, and a behavior. If your analytics don’t connect to that reality, you get slides and disappointment. Harvard Business Review\nThere are more subtle costs too. Dataset-first projects often create models that are brittle in production: they overfit to historical quirks, they require constant data wrangling, and they produce numbers no one trusts. That kills scale."
  },
  {
    "objectID": "blog/strategy/decision-first-ai/index.html#decision-first-in-research-not-new-but-finally-practical",
    "href": "blog/strategy/decision-first-ai/index.html#decision-first-in-research-not-new-but-finally-practical",
    "title": "Decision-First AI: Why Data Should Follow, Not Lead",
    "section": "Decision-first in research: not new, but finally practical",
    "text": "Decision-first in research: not new, but finally practical\nThere’s academic grounding for starting with decisions. In the machine-learning community this shows up as “decision-focused learning” or “smart predict-then-optimize.” The idea: train predictive models not for pure accuracy, but to minimize the loss that matters to the downstream optimization or decision task. When you optimize directly for the decision loss, you often get better business outcomes — even with “worse” prediction metrics. arXiv+1\nRecent papers and reviews show both the theory and practical methods: surrogate losses that reflect decision outcomes, techniques to differentiate through optimization, and heuristics for discrete problems. The takeaway: the math supports the intuition. If you want a model to help choose inventory levels, price points, or routing, train it with that decision in mind — not just with RMSE. Optimization Online+1\nThat doesn’t mean every model must be complex. Often the opposite. Framing the decision reduces model complexity because you only model what matters. This is your Occam’s Razor"
  },
  {
    "objectID": "blog/strategy/decision-first-ai/index.html#the-decision-map-simple-framework-you-can-use-today",
    "href": "blog/strategy/decision-first-ai/index.html#the-decision-map-simple-framework-you-can-use-today",
    "title": "Decision-First AI: Why Data Should Follow, Not Lead",
    "section": "The Decision Map: simple framework you can use today",
    "text": "The Decision Map: simple framework you can use today\nIf you want to flip to decision-first, start with a small, disciplined tool I call a Decision Map. It’s a one-page artefact. Build it before you touch data.\nHere’s the Decision Map — six steps. Do them in order.\n\nName the decision. Who decides, how often, and what options do they choose? Example: “Field-ops decides whether to dispatch a technician to a suspected DSL fault.” Be specific. Frequency matters — hourly, daily, weekly change what you can do.\nDefine the decision metric(s).What counts as success? Lower cost? Faster response time? Increase in net revenue? Pick one primary metric and one secondary. If you can’t name it in a single measurable sentence, you don’t have a decision.\nMap the current process. Where is the decision made today? Which people and tools are involved? Where does data enter? Where do delays happen? This step exposes the friction you must remove.\nIdentify the minimal action the model must trigger. The model doesn’t need to be perfect. It needs to change behavior. If the model’s output is a probability, what threshold triggers action? Who gets the alert? What’s the handoff?\nList the minimal signals (data) needed. Only include data that directly reduces uncertainty for the decision. You’ll be surprised how small this list often is. Think: signal → action. Not “all the data.”\nPlan the feedback loop. How will you measure the decision metric after deployment? How will you collect labels and iterate? Decide that upfront.\n\nIf you complete this map, you’ll have done 80% of the work most teams skip. It forces alignment, and it reveals whether the project is worth doing."
  },
  {
    "objectID": "blog/strategy/decision-first-ai/index.html#a-telecom-example-mapped-end-to-end-real-story",
    "href": "blog/strategy/decision-first-ai/index.html#a-telecom-example-mapped-end-to-end-real-story",
    "title": "Decision-First AI: Why Data Should Follow, Not Lead",
    "section": "A telecom example, mapped end-to-end (real story)",
    "text": "A telecom example, mapped end-to-end (real story)\nA brief, real example matters. I built a real-time anomaly detection system for a European telecom client early in my career. The project didn’t begin with “we have logs.” It began with this decision map:\n\nDecision: Should operations dispatch a field technician proactively for a suspected DSL fault?\nMetric: Reduce customer reported faults and improve Net Promoter Score (NPS) by reducing time-to-detect. Also: monthly cost savings from fewer reactive truck rolls and more planned truck rolls.\nProcess: Operations received the customer call, created a ticket, and dispatched if needed — often hours or days later. That was slow and expensive.\n\nAction: If the system flags an anomaly with high confidence\nRed : Very high probability in next 48 hours,\nAmber: probability in next 2-15 days\nGreen: No visibility of error in next 15 days This creates a high-priority ticket and dispatch a remote check or technician and was planned in region wise manner.\n\nSignals: DSL line metrics, error rates, device telemetry, event logs — a handful of streams, not every log.\nFeedback: Compare flagged incidents to customer complaints and adjust thresholds.\n\nBecause the decision was so clear, we could measure value before full scale. The pilot cut detection time from days to hours, raised customer satisfaction significantly ( We sent messages to possible signal disruption early), and saved ~£80K per month ( because reducing complete breakdown to DSL by heat or so and by planning the route than going everywhere) . It wasn’t an exotic model (or may be it was, tech details some other day); it was a tightly scoped system that informed a clear action.\nNotice how this maps to the Decision-First steps. The model existed to change a single operational choice. That focus made deployment possible and measurable fast."
  },
  {
    "objectID": "blog/strategy/decision-first-ai/index.html#another-quick-example-inventory-decisions",
    "href": "blog/strategy/decision-first-ai/index.html#another-quick-example-inventory-decisions",
    "title": "Decision-First AI: Why Data Should Follow, Not Lead",
    "section": "Another quick example: inventory decisions",
    "text": "Another quick example: inventory decisions\nInventory forecasting is a classic area where decision-first matters. You can chase lower MAPE and never change stocking policy. Or you can ask: what decision do merchandisers make with this forecast? When you frame it as “which SKUs do we order for next week, and what reorder points trigger expedited shipments,” you design the forecast differently: shorter horizons, bias for understock on fast movers, and direct constraints on reorder costs.\nI’ve led projects that delivered $11M in inventory optimization by building forecasts and decision rules that match merchant behavior and supply constraints. The trick was not better models — it was framing forecasts so the merchandisers could act with confidence."
  },
  {
    "objectID": "blog/strategy/decision-first-ai/index.html#practical-tips-for-teams-do-this-in-week-one",
    "href": "blog/strategy/decision-first-ai/index.html#practical-tips-for-teams-do-this-in-week-one",
    "title": "Decision-First AI: Why Data Should Follow, Not Lead",
    "section": "Practical tips for teams (do this in week one)",
    "text": "Practical tips for teams (do this in week one)\n\nRun a one-hour Decision Map workshop. Invite the decision owner, one operator, one engineer, and one product owner. Build the one-page map. If the owner can’t commit to a metric, pause the project.\nStart with a simple rule baseline. Before modeling, define a rule that will be your baseline (e.g., “if X &gt; T, create ticket”). If the model can’t beat that rule in decision impact, scrap it.\nMeasure decision impact, not model accuracy. Your dashboard should show business metric delta — not just RMSE. If you show the board a change in cost or conversion, you’ll get attention.\nPrioritize deployment constraints. Decide telemetry, latency, and handoff requirements first. Models that can’t meet latency or trust constraints are useless no matter how accurate.\nIterate with real feedback. Don’t wait for “perfect.” Ship an MVP that can be measured, then refine. Real decisions provide labels and operational learning."
  },
  {
    "objectID": "blog/strategy/decision-first-ai/index.html#common-objections-and-how-to-handle-them",
    "href": "blog/strategy/decision-first-ai/index.html#common-objections-and-how-to-handle-them",
    "title": "Decision-First AI: Why Data Should Follow, Not Lead",
    "section": "Common objections and how to handle them",
    "text": "Common objections and how to handle them\n“But we don’t have a clear decision owner.” Then don’t build a model. Decisions live in roles. Pull the right owner in early, or you’ll build for nobody.\n“Our data is messy.” Fine. If you can define the minimal signals, you can often create a proxy or start with manual labels. Messy data is easier to handle when you only need a few signals for a specific decision.\n“We need predictions for many uses.” Build a simple decision-first pilot first. Use its success to fund broader platform work. Pilots create proof that unlocks investment.\n“Decision-focused methods are academic — too hard.” There’s truth and myth here. The academic techniques show big wins when decision loss can be written down. But you don’t need complicated differentiable optimization to start. Use a decision map, simple thresholds, A/B tests, and iterative measurement. Graduate to decision-focused training once you have a stable objective. The research just tells us — unsurprisingly — that when you train with the decision in mind, outcomes improve. Optimization Online+1"
  },
  {
    "objectID": "blog/strategy/decision-first-ai/index.html#one-page-checklist-copy-this",
    "href": "blog/strategy/decision-first-ai/index.html#one-page-checklist-copy-this",
    "title": "Decision-First AI: Why Data Should Follow, Not Lead",
    "section": "One-page checklist (copy this)",
    "text": "One-page checklist (copy this)\n\nDecision name (The GOTO problem statement): ____________________\nPrimary / Secondary metric (which can accounted for ROI calculation later ): _____\nDecision owner (I don’t want to debate on this): ________________________\nFrequency of predictions: real-time / hourly / daily / weekly / monthly\nAction / interventions which can be triggered by model: ____________________\nMinimal signals required( The core Data to begin with): ______________________\nBaseline rule (your fail-safe if everything goes wrong): ____________________\nFeedback source ( might be tricky, but you should have one): __________________\n\nIf you can fill this in, you’re set for a pilot."
  },
  {
    "objectID": "blog/strategy/decision-first-ai/index.html#final-note-start-small-measure-fast-then-scale-with-discipline",
    "href": "blog/strategy/decision-first-ai/index.html#final-note-start-small-measure-fast-then-scale-with-discipline",
    "title": "Decision-First AI: Why Data Should Follow, Not Lead",
    "section": "Final note — start small, measure fast, then scale with discipline",
    "text": "Final note — start small, measure fast, then scale with discipline\nThe decision-first approach is simple because business problems are simple when stated well. The hard part is discipline: saying no to shiny demos and yes to measurable change. Start with one decision that matters. Map it. Ship a small system that changes behavior. Measure the business metric. Iterate.\nResearch supports this: models trained with the decision in mind perform better on the actual outcomes you care about. And in practice, teams that flip the order — decision first, data second — get to value faster. arXiv+1\nIf you want help mapping a decision in your company, send me one line describing the decision and the current process. I’ll reply with the Decision Map template you can use in a one-hour workshop. Or if you want we can connect too.\nIf this post help you or think help someone in need, please do share it. Thanks!!!"
  },
  {
    "objectID": "blog/strategy/decision-first-ai/index.html#key-sources-further-reading",
    "href": "blog/strategy/decision-first-ai/index.html#key-sources-further-reading",
    "title": "Decision-First AI: Why Data Should Follow, Not Lead",
    "section": "Key sources & further reading",
    "text": "Key sources & further reading\n\nElmachtoub, A.N., & Grigas, P. — Smart “Predict, then Optimize” (foundational paper on decision-focused loss and SPO). arXiv\nReviews and recent work on decision-focused learning (predict-and-optimize / decision-focused methods). arXiv+1\nHarvard Business Review — Decisions Don’t Start with Data — on why framing the decision matters. Harvard Business Review\n\n(And — the telecom and inventory examples referenced above are from projects on my profile/resume: Ask, if you want more details !! )"
  },
  {
    "objectID": "blog/engineering/10_treeknn/index.html",
    "href": "blog/engineering/10_treeknn/index.html",
    "title": "TrieKNN: Unleashing KNN’s Power on Mixed Data Types",
    "section": "",
    "text": "Photo by Gelgas Airlangga"
  },
  {
    "objectID": "blog/engineering/10_treeknn/index.html#the-allure-and-limitation-of-knn",
    "href": "blog/engineering/10_treeknn/index.html#the-allure-and-limitation-of-knn",
    "title": "TrieKNN: Unleashing KNN’s Power on Mixed Data Types",
    "section": "The Allure and Limitation of KNN",
    "text": "The Allure and Limitation of KNN\nIn the realm of machine learning, the K-Nearest Neighbors (KNN) algorithm stands out for its intuitive nature and ease of implementation. Its principle is simple: classify a data point based on the majority class among its ‘k’ nearest neighbors in the feature space. This non-parametric approach makes no assumptions about the underlying data distribution, rendering it versatile for various applications. KNN is very popular, but it comes with some limitations.\nHowever, KNN’s Achilles’ heel lies in its reliance on distance metrics, which are inherently designed for numerical data. Real-world datasets often contain a mix of numerical and categorical features, posing a significant challenge for KNN. How do you measure the distance between ‘red’ and ‘blue,’ or ‘large’ and ‘small’?\n\nPrior Art\nSeveral strategies have been proposed to adapt KNN for mixed data:\n\nOne-Hot Encoding: Converts categorical features into numerical vectors, but can lead to high dimensionality.\nDistance Functions for Mixed Data: Develops and apply custom distance metrics that can handle both numerical and categorical features such as HEOM and many others.\nUsing mean/mode values: Replace the missing values with mean/mode.\n\nThese methods often involve compromises, either distorting the data’s inherent structure or adding computational overhead."
  },
  {
    "objectID": "blog/engineering/10_treeknn/index.html#enter-trieknn-a-novel-approach",
    "href": "blog/engineering/10_treeknn/index.html#enter-trieknn-a-novel-approach",
    "title": "TrieKNN: Unleashing KNN’s Power on Mixed Data Types",
    "section": "Enter TrieKNN: A Novel Approach",
    "text": "Enter TrieKNN: A Novel Approach\nWhat if we could cleverly sidestep the distance calculation problem for categorical features, while still leveraging KNN’s power? TrieKNN offers just that—a way to perform KNN on any mixed data!\nTrieKNN combines the strengths of Trie data structures and KNN to handle mixed data types gracefully. Here’s the core idea:\n\nTrie-Based Categorical Encoding: A Trie is used to store the categorical features of the data. Each node in the Trie represents a category.\nLeaf-Node KNN Models: At the leaf nodes of the Trie, where specific combinations of categorical features are found, we fit individual KNN models using only the numerical features.\nWeighted Prediction: To classify a new data point, we traverse the Trie based on its categorical features. At each level, we calculate a weighted distance based on available data, ending in a probability score in each leaf node.\n\n\nWhy This Works\n\nNo Direct Distance Calculation for Categorical Features: The Trie structure implicitly captures the relationships between categorical values.\nLocalized KNN Models: By fitting KNN models at the leaf nodes, we ensure that distance calculations are performed only on relevant numerical features.\nScalability: The Trie structure efficiently handles a large number of categorical features and values."
  },
  {
    "objectID": "blog/engineering/10_treeknn/index.html#building-a-trieknn-model",
    "href": "blog/engineering/10_treeknn/index.html#building-a-trieknn-model",
    "title": "TrieKNN: Unleashing KNN’s Power on Mixed Data Types",
    "section": "Building a TrieKNN Model",
    "text": "Building a TrieKNN Model\nLet’s dive into the implementation. We’ll start with the TrieNode and Trie classes, then move on to the KNN model and the training/prediction process.\n\nTrie Implementation\n\n\nCode\nimport numpy as np\nfrom collections import Counter\n\nclass TrieNode:\n    def __init__(self):\n        self.children = {}  # Dictionary to store child nodes\n        self.is_end_of_word = False  # True if the node is the end of a word\n        self.count = 0  # Count of how many times a word has been inserted\n        self.class_counts = {}  # Class counts\n        self.class_weights = {}\n        self.model = None  # Model at leaf nodes\n        self.indexes = []  # Store data indexes belonging to this leaf\n        self.labels = []  # Store data indexes belonging to this leaf\n        self.node_weight = None\n\nclass Trie:\n    def __init__(self):\n        self.root = TrieNode()  # Root node of the Trie\n        self.data_index = 0  # Initialize data index\n\n    def insert(self, word_val, model):\n        current_node = self.root\n        word, val = word_val\n        current_node.count += 1\n\n        # Adding class counts\n        if val not in current_node.class_counts:\n            current_node.class_counts[val] = 0\n        current_node.class_counts[val] += 1\n\n        for char in word:\n            # If the character is not in children, add a new TrieNode\n            if char not in current_node.children:\n                current_node.children[char] = TrieNode()\n            current_node = current_node.children[char]\n\n            # Adding count of instances\n            current_node.count += 1\n\n            # adding class counts\n            if val not in current_node.class_counts:\n                current_node.class_counts[val] = 0\n            current_node.class_counts[val] += 1\n\n        # Mark the end of the word and increment count\n        current_node.is_end_of_word = True\n        current_node.indexes.append(self.data_index)  # Store the data index\n        current_node.labels.append(val)\n        current_node.model = model\n        self.data_index += 1  # Increment data index\n\n    def search(self, word):\n        current_node = self.root\n        for char in word:\n            # If the character doesn't exist in the children, the word doesn't exist\n            if char not in current_node.children:\n                return False\n            current_node = current_node.children[char]\n\n        # Return True if it's the end of a word and the word exists\n        return current_node.is_end_of_word\n\n    def count_word(self, word):\n        current_node = self.root\n        for char in word:\n            # If the character doesn't exist, the word doesn't exist\n            if char not in current_node.children:\n                return 0, current_node.class_counts  # Correctly return class_counts\n            current_node = current_node.children[char]\n\n        # Return the count of the word\n        return current_node.count, current_node.class_counts\n\n    def display(self):\n        # Recursively display the tree\n        def _display(node, word):\n            if node.is_end_of_word:\n                print(f\"Data: {word}, Count: {node.count}, Indexes: {len(node.indexes)} Classes :{node.class_counts} weights:{len(node.class_weights)}\")  # Display indexes too\n            for char, child in node.children.items():\n                _display(child, word + char)  # corrected the display\n\n        _display(self.root, \"\")\n\n    def apply(self, func):\n        \"\"\"\n        Applies a function to all models in the leaf nodes.\n        \"\"\"\n        def _apply(node):\n            if node.is_end_of_word and node.model is not None:\n                func(node)\n            for child in node.children.values():\n                _apply(child)\n\n        _apply(self.root)\n\n    def apply_weight_to_indexes(self, weight):\n        \"\"\"\n        Applies a weight to the indexes based on the percentage of data available.\n        \"\"\"\n        def _apply_weight_to_indexes(node):\n            if node.is_end_of_word:\n                total_count = sum(self.root.children[child].count for child in self.root.children)\n                percentage = node.count / total_count if total_count &gt; 0 else 0\n                weighted_indexes = [(index, weight * percentage) for index in node.indexes]\n                node.class_weights = weighted_indexes  # Corrected this line\n            for child in node.children.values():\n                _apply_weight_to_indexes(child)\n\n        _apply_weight_to_indexes(self.root)\n\n\n\n\nKNN Model\n\n\nCode\nclass KNNModel:\n    def __init__(self, k=5):\n        self.data = None\n        self.labels = []\n        self.k = k\n\n    def fit(self, data, indexes, labels):\n        # print(\"Fitting model with indexes:\", len(indexes), \"labels:\", len(labels))\n        self.data = data[indexes].astype(float)\n        self.labels = np.array(labels).astype(float)\n\n    def predict(self, data):\n        # print(\"Predicting with data:\", data)\n        dist_ind = np.sqrt(np.sum((self.data - data) ** 2, axis=1) ** 2)  # euclidean distance\n        main_arr = np.column_stack((self.labels, dist_ind))  # labels with distance\n        main = main_arr[main_arr[:, 1].argsort()]  # sorting based on distance\n        count = Counter(main[0:self.k, 0])  # counting labels\n        sums = np.array(list(count.values()))  # getting counts\n        return sums / np.sum(sums)  # prediction as probability\n\n\n\n\nTraining and Evaluation\nHere’s how we train and evaluate the TrieKNN model:\n\n\nCode\n# Sample data\nn = 10000\ndata = np.array((np.random.choice(['Anything ', 'By ','Chance '], p=[0.6,0.1,0.3],size=n),\n                 np.random.choice(['can', 'go', 'here','lets', 'see', \"it\"], p=[0.1, 0.1, 0.1, 0.2, 0.4, 0.1], size=n),\n                 np.random.normal(3, 1, size=n),\n                 np.random.normal(5, 2, size=n))).T\ny_label = np.random.choice([0,1], p=[0.7, 0.3], size=n)\n\n# Trie training\ntrie = Trie()\nfor X, y in zip(data, y_label):\n    trie.insert((X[:2], y),None)\n\n# Apply weights to indexes\ntrie.apply_weight_to_indexes(0.5)\n\n# Fit models of leaf nodes\ndef add_model(node, data):\n    node.model = KNNModel()\n    node.model.fit(data, node.indexes, node.labels)\n\ndef traverse_and_add_model(node, data):\n    if node.is_end_of_word:\n        add_model(node, data)  # Add model to leaf node\n    for child in node.children.values():\n        traverse_and_add_model(child, data)\n\ntraverse_and_add_model(trie.root, data[:, 2:])\n\n\n\n\nExplanation\n\nWe create sample data with mixed categorical and numerical features.\nWe insert each data point into the Trie, using the categorical features as the path.\nAfter the Trie is built, we traverse it and fit a KNN model to the data points stored at each leaf node.\nFinally, we can predict the class of new data points by traversing the Trie and using the KNN model at the corresponding leaf node."
  },
  {
    "objectID": "blog/engineering/10_treeknn/index.html#results-and-discussion",
    "href": "blog/engineering/10_treeknn/index.html#results-and-discussion",
    "title": "TrieKNN: Unleashing KNN’s Power on Mixed Data Types",
    "section": "Results and Discussion",
    "text": "Results and Discussion\nLet us display the trie.\n\n\n\n\nCode\ntrie.display()\n\n\nData: Anything lets, Count: 1209, Indexes: 1209 Classes :{np.int64(0): 832, np.int64(1): 377} weights:1209\nData: Anything here, Count: 636, Indexes: 636 Classes :{np.int64(0): 432, np.int64(1): 204} weights:636\nData: Anything can, Count: 666, Indexes: 666 Classes :{np.int64(0): 455, np.int64(1): 211} weights:666\nData: Anything go, Count: 585, Indexes: 585 Classes :{np.int64(0): 407, np.int64(1): 178} weights:585\nData: Anything see, Count: 2363, Indexes: 2363 Classes :{np.int64(0): 1637, np.int64(1): 726} weights:2363\nData: Anything it, Count: 571, Indexes: 571 Classes :{np.int64(0): 410, np.int64(1): 161} weights:571\nData: Chance see, Count: 1210, Indexes: 1210 Classes :{np.int64(1): 372, np.int64(0): 838} weights:1210\nData: Chance lets, Count: 599, Indexes: 599 Classes :{np.int64(1): 187, np.int64(0): 412} weights:599\nData: Chance go, Count: 270, Indexes: 270 Classes :{np.int64(0): 176, np.int64(1): 94} weights:270\nData: Chance here, Count: 278, Indexes: 278 Classes :{np.int64(0): 200, np.int64(1): 78} weights:278\nData: Chance it, Count: 316, Indexes: 316 Classes :{np.int64(0): 213, np.int64(1): 103} weights:316\nData: Chance can, Count: 309, Indexes: 309 Classes :{np.int64(0): 228, np.int64(1): 81} weights:309\nData: By can, Count: 76, Indexes: 76 Classes :{np.int64(0): 45, np.int64(1): 31} weights:76\nData: By see, Count: 397, Indexes: 397 Classes :{np.int64(1): 128, np.int64(0): 269} weights:397\nData: By here, Count: 107, Indexes: 107 Classes :{np.int64(1): 43, np.int64(0): 64} weights:107\nData: By lets, Count: 226, Indexes: 226 Classes :{np.int64(1): 77, np.int64(0): 149} weights:226\nData: By it, Count: 89, Indexes: 89 Classes :{np.int64(0): 55, np.int64(1): 34} weights:89\nData: By go, Count: 93, Indexes: 93 Classes :{np.int64(0): 64, np.int64(1): 29} weights:93\n\n\nThe model predicted the following values:\n\n\n\n\nCode\n# Prediction example\ndef predict_with_model(node):\n    predictions = node.model.predict(np.array([2,5]))\n    print(\"Predictions:\", predictions)\n\ntrie.apply(predict_with_model)\n\n\nPredictions: [0.4 0.6]\nPredictions: [0.6 0.4]\nPredictions: [0.4 0.6]\nPredictions: [0.8 0.2]\nPredictions: [0.4 0.6]\nPredictions: [0.8 0.2]\nPredictions: [0.4 0.6]\nPredictions: [0.6 0.4]\nPredictions: [0.2 0.8]\nPredictions: [0.4 0.6]\nPredictions: [0.8 0.2]\nPredictions: [0.4 0.6]\nPredictions: [0.4 0.6]\nPredictions: [0.6 0.4]\nPredictions: [1.]\nPredictions: [0.4 0.6]\nPredictions: [0.6 0.4]\nPredictions: [0.6 0.4]\n\n\nThe predictions will vary on each run. From this we can see that we can use KNN on mixed data types."
  },
  {
    "objectID": "blog/engineering/10_treeknn/index.html#conclusion-a-promising-path-forward",
    "href": "blog/engineering/10_treeknn/index.html#conclusion-a-promising-path-forward",
    "title": "TrieKNN: Unleashing KNN’s Power on Mixed Data Types",
    "section": "Conclusion: A Promising Path Forward",
    "text": "Conclusion: A Promising Path Forward\nTrieKNN presents a compelling solution for extending the applicability of KNN to mixed data types. By leveraging the Trie data structure, it avoids direct distance calculations on categorical features, enabling the use of localized KNN models for numerical data.\nFurther research could explore:\n\nOptimizing the weighting scheme for combining predictions from different Trie levels.\nComparing TrieKNN’s performance against other mixed-data KNN approaches on benchmark datasets.\nExtending TrieKNN to handle missing data and noisy categorical features.\n\nTrieKNN opens up new possibilities for applying KNN in domains where mixed data types are prevalent, such as healthcare, e-commerce, and social science.\nResources and further reads:\n1. Nomclust R package\n2. An Improved kNN Based on Class Contribution and Feature Weighting\n3. An Improved Weighted KNN Algorithm for Imbalanced Data Classification\n4. A weighting approach for KNN classifier\n5. Unsupervised Outlier Detection for Mixed-Valued Dataset Based on the Adaptive k-Nearest Neighbor Global Network\n6. A hybrid approach based on k-nearest neighbors and decision tree for software fault prediction\n7. Analysis of Decision Tree and K-Nearest Neighbor Algorithm in the Classification of Breast Cancer"
  },
  {
    "objectID": "blog/engineering/03_crosstab_sparsity/index.html",
    "href": "blog/engineering/03_crosstab_sparsity/index.html",
    "title": "CrossTab Sparsity",
    "section": "",
    "text": "Cluster analysis has always fascinated me as a window into the hidden structures of data. During my collaboration with Kumarjit Pathak, we grappled with a persistent challenge in unsupervised learning: how to objectively evaluate clustering quality across different algorithms. Traditional metrics like the Silhouette Index or Bayesian Information Criterion felt restrictive—they were siloed within specific methodologies, making cross-algorithm comparisons unreliable.\nThis frustration led us to develop a universal cluster evaluation metric, detailed in our paper “Cross Comparison of Results from Different Clustering Approaches”. Our goal was to create a framework that transcends algorithmic biases, enabling:\n- Direct comparison of K-Means vs GMM vs DBSCAN vs PAM vs SOM vs Anything results\n- Identification of variables muddying cluster separation\n- Automated determination of optimal cluster counts\nIn this blog, I’ll walk you through our journey—from conceptualization to real-world validation—and share insights that didn’t make it into the final paper."
  },
  {
    "objectID": "blog/engineering/03_crosstab_sparsity/index.html#introduction",
    "href": "blog/engineering/03_crosstab_sparsity/index.html#introduction",
    "title": "CrossTab Sparsity",
    "section": "",
    "text": "Cluster analysis has always fascinated me as a window into the hidden structures of data. During my collaboration with Kumarjit Pathak, we grappled with a persistent challenge in unsupervised learning: how to objectively evaluate clustering quality across different algorithms. Traditional metrics like the Silhouette Index or Bayesian Information Criterion felt restrictive—they were siloed within specific methodologies, making cross-algorithm comparisons unreliable.\nThis frustration led us to develop a universal cluster evaluation metric, detailed in our paper “Cross Comparison of Results from Different Clustering Approaches”. Our goal was to create a framework that transcends algorithmic biases, enabling:\n- Direct comparison of K-Means vs GMM vs DBSCAN vs PAM vs SOM vs Anything results\n- Identification of variables muddying cluster separation\n- Automated determination of optimal cluster counts\nIn this blog, I’ll walk you through our journey—from conceptualization to real-world validation—and share insights that didn’t make it into the final paper."
  },
  {
    "objectID": "blog/engineering/03_crosstab_sparsity/index.html#the-birth-of-the-metric-a-first-person-perspective",
    "href": "blog/engineering/03_crosstab_sparsity/index.html#the-birth-of-the-metric-a-first-person-perspective",
    "title": "CrossTab Sparsity",
    "section": "The Birth of the Metric: A First-Person Perspective",
    "text": "The Birth of the Metric: A First-Person Perspective\nWhy Existing Methods Fell Short\nEarly in our research, we cataloged limitations of popular evaluation techniques:\n\nMethod Dependency\n\n\n\nSilhouette scores worked beautifully for K-Means but faltered with Gaussian Mixture Models (GMM).\n\nProbability-based metrics like BIC couldn’t handle distance-based clusters.\n\n\nNoise Blindness\nNoisy variables often contaminated clusters, but traditional methods required manual outlier detection.\nSubjective Optimization* Elbow plots and dendrograms left too much room for human interpretation.\n\n\nOur “Aha!” Moment - Crosstab Sparsity\n\n\n\n\nBest Cluster for K-means Using Crosstab sparsity\n\n\n\nWhile analyzing cross-tab matrices of variable distributions across clusters, we noticed a pattern: well-segregated clusters consistently showed higher frequencies along matrix diagonals. This inspired our two-part metric:\n\n\n\nSegregation Factor:\n# Simplified calculation from our codebase  \nmedian = np.median(cross_tab)  \nN_vk = np.sum(cross_tab &gt; median)  # Count \"well-segregated\" instances  \nExplanation Factor:\nexplanation = np.log(len(data) / (bins * clusters))  \n\n\nSegregation Factor: Measures how distinctly clusters separate data points. We used the median (not mean) to avoid skew from outlier-dominated matrices.\nExplanation Factor: Quantifies how well clusters capture data variability. The logarithmic term penalizes overfitting—a critical insight from debugging early over-segmented clusters.\n\nAnd the Final Formula:\nFor variable \\(v\\) with \\(k\\) clusters:\n\\[\nS_v^k = \\underbrace{\\frac{N_v^k}{\\max(l, k)}}_{\\text{Segregation}} \\times \\underbrace{\\ln\\left(\\frac{N_d}{l \\times k}\\right)}_{\\text{Explanation}}\n\\]\nwhere:\n- \\(N_v^k\\): Segregated instances (values above cross-tab matrix median)\n- \\(l\\): Number of value intervals for variable \\(v\\)\n- \\(N_d\\): Total observations\nThis formulation ensures algorithmic invariance, allowing comparison across methods like K-Means (distance-based) and GMM (probability-based). Also, now you can see from the formula two scenarios happens: 1. If each variable crosstab is too dense then their is no separation between classes 2. If each variable crosstab is too sparse then we loose on explanation.\nHence the curve reaches a maximum and then falls down giving use the separability that the cluster can produce:"
  },
  {
    "objectID": "blog/engineering/03_crosstab_sparsity/index.html#case-study-vehicle-silhouettes-through-my-eyes",
    "href": "blog/engineering/03_crosstab_sparsity/index.html#case-study-vehicle-silhouettes-through-my-eyes",
    "title": "CrossTab Sparsity",
    "section": "Case Study: Vehicle Silhouettes (Through My Eyes)",
    "text": "Case Study: Vehicle Silhouettes (Through My Eyes)\n\nThe Dataset That Almost Broke Us\nWe tested our metric on a vehicle silhouette dataset with 18 shape-related features (e.g., compactness, circularity). Initially, inconsistent results plagued us—until we realized our binning strategy for continuous variables was flawed.\n\n\nKey Adjustments:\n- Switched from equal-width to quantile-based binning (10 bins per variable).\n- For categorical variables, retained native levels instead of coercing bins.\n\n\nThe Breakthrough\nAfter refining the preprocessing:\n\nOptimal Clusters: Our metric plateaued at \\(k=6\\) , aligning perfectly with known vehicle categories (sedans, trucks, etc.).\nNoise Detection: Variables like Max.LWR (length-width ratio) scored poorly, revealing inconsistent clustering. We later found this was due to manufacturers’ design variances.\n\nFinding best cluster for K-Means alone:\n\n\n\n\nBest Cluster for PAM method Using Crosstab sparsity\n\n\n\nComparing all cluster methods and find the optimal one:\n\n\n\n\nOptimal Cluster for many methods\n\n\n\nThe chunkiest part : Understanding your variable for separateness. This gives direct insight of what variable in your data is most critical separator.\n\n\n\n\nAll kind of variable scored against Metrics"
  },
  {
    "objectID": "blog/engineering/03_crosstab_sparsity/index.html#comparative-advantages-and-creativity-at-work",
    "href": "blog/engineering/03_crosstab_sparsity/index.html#comparative-advantages-and-creativity-at-work",
    "title": "CrossTab Sparsity",
    "section": "Comparative Advantages and Creativity at Work",
    "text": "Comparative Advantages and Creativity at Work\nComparative Advantage Over Traditional Metrics\n\n\n\n\n\n\n\n\n\nFeature/Scenario\nSilhouette Index\nDavies-Bouldin\nCrosstab Sparsity\n\n\n\n\nAlgorithm Agnostic\n❌ (Distance-based only)\n❌\n✔️\n\n\nHandles Mixed Data\n❌\n❌\n✔️\n\n\nIdentifies Noisy Vars\n❌\n❌\n✔️\n\n\nOptimal Cluster Detection\nManual elbow analysis\nManual analysis\nAutomated plateau detection\n\n\nMixed Algorithms\nFailed (GMM vs K-Means)\nFailed (needs numerical data)\nAchieved 92% consistency[1]\n\n\nNoisy Variables\nManual outlier removal\nManual outlier removal\nAuto-detected (e.g., Max.LWR)\n\n\nOptimal Cluster Detection\nSubjective elbow plots\nSubjective to Elbow plots\nObjective plateau detection\n\n\n\n\nOur creativity yielding boons. We wanted a simple metric to judge different kind of cluster, but we got much more from our experiments and work on this metric:\n\nVariable-Level Diagnostics: Low \\(S_v^k\\) scores pinpoint variables muddying cluster separation.\n\nCross-Method Benchmarking: Compare K-Means (distance) vs GMM (probability) vs hierarchical vs partition clustering fairly using a unified score.\n\nScale Invariance: Logarithmic term makes scores comparable across datasets of varying sizes.\n\nDebug Cluster Quality: Identify and remove noisy variables preemptively\n\nAutomate Model Selection: Objectively choose between K-Means, GMM, PAM, Agglomerative."
  },
  {
    "objectID": "blog/engineering/03_crosstab_sparsity/index.html#lessons-learned-and-future-vision",
    "href": "blog/engineering/03_crosstab_sparsity/index.html#lessons-learned-and-future-vision",
    "title": "CrossTab Sparsity",
    "section": "Lessons Learned and Future Vision",
    "text": "Lessons Learned and Future Vision\nFew take away from these experiments\n1. Binning Sensitivity: Quantile-based binning was transformation. Equal-width bins distorted scores for skewed variables.\n2. Categorical Handling: Native levels for categorical outperformed frequency-based grouping.\n3. Non-Parametric Approach: This approach allowed us to make sense of data without being tied down by assumptions. We have seen how this metric can be a game-changer for statisticians, providing insights not just into cluster behavior but also into rare event modeling.\nThe plots from these experiments not only clarify how clusters behave but also offer valuable insights for identifying outliers. I believe there’s exciting potential to extend this metric into classification and value estimation modeling. Imagine using it as a loss function in both linear and non-linear methods to achieve better data segmentation! Thing for another blog someday!\n\nA Personal Reflection\nDeveloping this metric taught me that simplicity often masks depth. A two-component formula now underpins clustering decisions in industries we never imagined—from fraud detection to genomics. Yet, I’m most proud of how it democratizes cluster analysis: business analysts at our partner firms now optimize clusters without PhD-level stats.\nYou can find implementation of python code here\nThis blog synthesizes findings from our original paper, available here. For a deeper dive into the math, check Section 3 of the paper.\nTo my readers: Have you tried implementing cross-algorithm clustering? Share your war stories in the comments—I’d love to troubleshoot together!"
  },
  {
    "objectID": "blog/engineering/01_adaptive_regression/index.html",
    "href": "blog/engineering/01_adaptive_regression/index.html",
    "title": "Adaptive Regression",
    "section": "",
    "text": "Here I am trying to express our logic to find such Observation. Lets dive in.\nThere are different value estimation technique like regression analysis and time-series analysis. Everyone of us has experimented on regression using OLS ,MLE, Ridge, LASSO, Robust etc., and also might have evaluated them using RMSE (Root Mean/Median Square Error), MAD (Mean/Median Absolute Deviation), MAE (Mean / Median Absolute Error) and MAPE (Mean/Median Absolute Percentage Error), etc…\nBut all of these gives a single point estimate that what is the overall error looks like. Just a different thought!! can we be sure that this single value of MAPE or MAE? How easy it is to infer that our trained model has fitted well across the distribution of dependent variable?\n\n\n\n\n\nPlot of Anscombe’s Quartet\n\n\n\n\n\nSome Descriptive Stats for Anscombe’s Quartet\n\n\nLet me give you a pretty small data-set to play with “The Anscombe’s quartet”. This is a very famous data-set by Francis Anscombe. Please refer the plots below to understand the distribution of y1, y2, y3, y4. Isn’t it different?\nWould the measure of central tendency and disportion be same for this data? I am sure none of us would believe but to our utter surprise we see all the descriptive stats are kind of same. Don’t believe me !!! Please see the results below ( Source: Wikipedia ):"
  },
  {
    "objectID": "blog/engineering/01_adaptive_regression/index.html#introduction",
    "href": "blog/engineering/01_adaptive_regression/index.html#introduction",
    "title": "Adaptive Regression",
    "section": "",
    "text": "Here I am trying to express our logic to find such Observation. Lets dive in.\nThere are different value estimation technique like regression analysis and time-series analysis. Everyone of us has experimented on regression using OLS ,MLE, Ridge, LASSO, Robust etc., and also might have evaluated them using RMSE (Root Mean/Median Square Error), MAD (Mean/Median Absolute Deviation), MAE (Mean / Median Absolute Error) and MAPE (Mean/Median Absolute Percentage Error), etc…\nBut all of these gives a single point estimate that what is the overall error looks like. Just a different thought!! can we be sure that this single value of MAPE or MAE? How easy it is to infer that our trained model has fitted well across the distribution of dependent variable?\n\n\n\n\n\nPlot of Anscombe’s Quartet\n\n\n\n\n\nSome Descriptive Stats for Anscombe’s Quartet\n\n\nLet me give you a pretty small data-set to play with “The Anscombe’s quartet”. This is a very famous data-set by Francis Anscombe. Please refer the plots below to understand the distribution of y1, y2, y3, y4. Isn’t it different?\nWould the measure of central tendency and disportion be same for this data? I am sure none of us would believe but to our utter surprise we see all the descriptive stats are kind of same. Don’t believe me !!! Please see the results below ( Source: Wikipedia ):"
  },
  {
    "objectID": "blog/engineering/01_adaptive_regression/index.html#so-what-we-do-now",
    "href": "blog/engineering/01_adaptive_regression/index.html#so-what-we-do-now",
    "title": "Adaptive Regression",
    "section": "So what we do Now!",
    "text": "So what we do Now!\nAstonished !!! Don’t be. This is what has been hiding behind those numbers. And this is why we really won’t be able to cross certain performance level. Unless you change some features or even do a lot of hyper parameter tuning, your results won’t vary much.\nIf you look at the average value of MAPE in each decile you would see an interesting pattern. Let us show you what we see that pattern. One day while working on a business problem where I was using regression on a discussion with Kumarjit, we deviced a different way of model diagnosis. We worked together to give this a shape and build on it.\n\nAs you can see it is absolutely evident that either of the side in the distribution of MAPE values is going wild!!!!!!! Still overall MAPE is good (18%)."
  },
  {
    "objectID": "blog/engineering/01_adaptive_regression/index.html#seeking-scope-of-improvement",
    "href": "blog/engineering/01_adaptive_regression/index.html#seeking-scope-of-improvement",
    "title": "Adaptive Regression",
    "section": "Seeking Scope of Improvement",
    "text": "Seeking Scope of Improvement\nWe worked together to build a different framework to address such issues on the go and reduce the MAPE deterioration on the edge of the distribution.\nThis problems gives rise to a concept we named as Distribution Assertive Regression (DAR).\nDAR is a framework that is based on cancelling the weakness of one point summaries by using the classical concepts of Reliability Engineering : The Bath Tub Curve.\n\n\n\n\n\nPlot for Classical Bath Tub Curve using a Hazard Function\n\n\nThe Specialty of this curve is that it gives you the likelihood which areas one tends to have high failure rates. In our experiments when we replace failure with MAPE value and the Time with sorted (ascending) value of target / dependent variable, we observe the same phenomenon. This is likely to happen because most of regression techniques assumes Normal (Gaussian) Distribution of data and fits itself towards the central tendency of this distribution.\nBecause of this tendency, any regression methods tends to learn less about data which are away from the central tendency of the target.\nLets look at BostonHousing data from “mlbench” package in R.\n\n\n\nPlot for MAPE Bath Tub Curve for Decile Split “mdev” from Data\n\n\nHere the MAPE is calculated for each decile split of ordered target variable. As you can observe it is following the bath tub curve. Hence the validates our hypothesis that the regression method is not able to understand much about the data at the either ends of the distribution."
  },
  {
    "objectID": "blog/engineering/01_adaptive_regression/index.html#final-analysis",
    "href": "blog/engineering/01_adaptive_regression/index.html#final-analysis",
    "title": "Adaptive Regression",
    "section": "Final Analysis",
    "text": "Final Analysis\nNow the DAR framework essentially fixes this weakness of regression method and understands the behavior of data which is stable and can be tweak in a fashion that can be use in general practice.\nPlot of MAPE Bath Tub Curve after applying DAR Framework for Decile Split “mdev” from Data\n\nHow this framework with same method reduced MAPEs so much and made model much more stable…?? Well here it is:\nThe DAR framework splits the data at either ends of the order target variable and performs regression on these “split” data individually. This inherently reduces the so called “noise” part of the data and treat it as an individual data."
  },
  {
    "objectID": "blog/engineering/01_adaptive_regression/index.html#scoring-on-new-data",
    "href": "blog/engineering/01_adaptive_regression/index.html#scoring-on-new-data",
    "title": "Adaptive Regression",
    "section": "Scoring on New Data",
    "text": "Scoring on New Data\nNow you might be thinking while applying regression this sounds good but how will one score this on new data. Well to answer that we used our most simple yet very effective friend “KNN” (Though any multiclass Classifier can be used here). So ideally scoring involves two step method :\n\nScore new value against each KNN / Multiclass Classifier model of the data\nBased on closeness we score it with the regression method used for that part of data.\n\nSo now we know how we can improve the prediction power of data for regression."
  },
  {
    "objectID": "blog/engineering/01_adaptive_regression/index.html#code-and-flowchart",
    "href": "blog/engineering/01_adaptive_regression/index.html#code-and-flowchart",
    "title": "Adaptive Regression",
    "section": "Code and Flowchart",
    "text": "Code and Flowchart\nIf things are simple lets keep it simple. Refer flowchart and code below for implementation of this framework. Paper here!\n\nR codePython codeHere is the Flow Chart\n\n\n\n\n\n\n\n\nTipClick to Expand\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipClick to Expand\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngraph TB\n    \n    subgraph Testing\n        p1(Finding bucket of model to choose)\n        p1 --&gt; p2([Making predictions &lt;br&gt; based on selected model for inference])\n        p2 --&gt; p3(Consolidate final score of prediction)\n    end\n\n    subgraph Training\n        md([Fitting a &lt;br&gt;Regression model])==&gt; di\n        di{Binning Data via &lt;br/&gt; evaluating Distribution &lt;br/&gt; MAPE values }\n        di --&gt; md2([Fitting a Buckteing model &lt;br/&gt; to Binned MAPE Buckets])\n        md2 --&gt; md3([Fitting Regression &lt;br&gt; Models on Binned Data])\n        md == Keeping main&lt;br/&gt;model ==&gt; ro        \n        md3 ==&gt; ro(Final Models &lt;br&gt; Binning Data Models + &lt;br&gt; Set of Regressoin Models)\n    end\n\n    \n    od([Data Input]) -- Training&lt;br&gt; Data--&gt; md\n    od -- Testing&lt;br&gt; Data--&gt; p1\n    ro -.-&gt; p1\n    ro -.-&gt; p2\n\n    classDef green fill:#9f6,stroke:#333,stroke-width:2px;\n    classDef yellow fill:#ff6,stroke:#333,stroke-width:2px;\n    classDef blue fill:#00f,stroke:#333,stroke-width:2px,color:#fff;\n    classDef orange fill:#f96,stroke:#333,stroke-width:4px;\n    class md,md2,md3 green\n    class di orange\n    class p1,p2 yellow\n    class ro,p3 blue"
  },
  {
    "objectID": "about.html#the-system-i-built",
    "href": "about.html#the-system-i-built",
    "title": "Hi. I’m Jitin. I help Senior Professionals & Business Owners build AI-Enabled Enterprises.",
    "section": "THE SYSTEM I BUILT",
    "text": "THE SYSTEM I BUILT\nThe AI Profit OS is what I couldn’t teach inside corporate walls.\nThree days. Three frameworks. Everything leaders need to:\n\nKnow which problems in their business actually need AI (most don’t)\nMap those problems to real solutions (without hiring consultants)\nQuantify ROI and execute in 30–90 days (with a roadmap that works)\n\nThe result? Leaders become AI strategists. They never get fooled by hype again. They ship real use cases that move the business."
  },
  {
    "objectID": "about.html#how-i-think",
    "href": "about.html#how-i-think",
    "title": "Hi. I’m Jitin. I help Senior Professionals & Business Owners build AI-Enabled Enterprises.",
    "section": "HOW I THINK",
    "text": "HOW I THINK\nI started as a mechanical engineer with a background in applied mathematics. This shaped everything.\nEarly in my career, I solved an inventory problem in automotive by asking: “What if I think about this like a healthcare system, not a car factory?”\nSuddenly, the answer was obvious. We eliminated 45,000 excess units.\nOver 15 years, I noticed a pattern:\nThe breakthroughs don’t come from going deeper in your industry. They come from looking sideways.\nHealthcare solving automotive problems. Retail solving manufacturing challenges. Mathematical patterns from physics applied to business operations.\nMost experts stay in their domain. They look straight ahead. I learned to look sideways.\nThat’s why I can diagnose any AI opportunity in 1 hour.\nNot because I know everything. But because I’ve seen the same patterns 50+ times across different industries. I can spot what fits and what doesn’t."
  },
  {
    "objectID": "about.html#what-ive-done",
    "href": "about.html#what-ive-done",
    "title": "Hi. I’m Jitin. I help Senior Professionals & Business Owners build AI-Enabled Enterprises.",
    "section": "WHAT I’VE DONE",
    "text": "WHAT I’VE DONE\nEnterprise deployments: - ₹6Cr+ saved in inventory optimization (retail) - $11M in downtime prevention (telecom) - ₹2Cr+ in route optimization (logistics) - 99.2% accuracy in invoice automation (finance) - 70% reduction in onboarding time (services)\nIndustries I’ve worked in: - Automotive (Mercedes, Maruti) - Retail (L’Oréal, major chains) - Telecom (British Telecom, major carriers) - Finance (banks, CFOs, procurement) - Logistics (supply chain, inventory) - FMCG (demand forecasting)\nWhat this taught me:\nThere’s no such thing as an “AI problem.” There are only business problems that can sometimes be solved with AI.\nThe skill isn’t knowing AI. It’s knowing the difference.\nThat’s what I teach."
  },
  {
    "objectID": "about.html#why-i-built-this-cohort",
    "href": "about.html#why-i-built-this-cohort",
    "title": "Hi. I’m Jitin. I help Senior Professionals & Business Owners build AI-Enabled Enterprises.",
    "section": "WHY I BUILT THIS COHORT",
    "text": "WHY I BUILT THIS COHORT\nLeaders don’t need more AI education. They need clarity.\nRight now: - YouTube teaches tools (ChatGPT, prompts, etc.) - MBA programs teach strategy (generic, slow) - Consultants charge ₹100K+ (expensive, 6-month timelines) - Nobody teaches: “How to know if AI is even the answer”\nThat’s the gap.\nSo I built a 3-day intensive that teaches the exact diagnostic system I’ve used 50+ times. The one that works whether you’re in automotive, finance, retail, or logistics.\nSame frameworks. Different problems. Same results: clarity + execution."
  },
  {
    "objectID": "about.html#what-changed-for-people",
    "href": "about.html#what-changed-for-people",
    "title": "Hi. I’m Jitin. I help Senior Professionals & Business Owners build AI-Enabled Enterprises.",
    "section": "WHAT CHANGED FOR PEOPLE",
    "text": "WHAT CHANGED FOR PEOPLE\nA VP of Operations at a ₹500Cr retail chain: - Before: “Is AI even right for our problem?” - After 3 days: “Yes. Here’s what we need to build. Here’s the ROI. Here’s the roadmap.” - After 90 days: ₹6Cr saved annually.\nA CFO at a ₹50Cr services company: - Before: “Should we automate invoicing?” - After 3 days: “Yes, with RPA + OCR. Here’s what we need.” - After 6 weeks: 40 hrs/week → 6 hrs/week. 99.2% accuracy.\nA founder at a ₹15Cr consulting firm: - Before: “How do we scale without hiring 10 new ops people?” - After 3 days: “Automate onboarding. Here’s the roadmap.” - After 4 weeks: 3 weeks → 4 days. 3x faster scaling.\nThe pattern: Clarity first. Execution follows. ROI is real."
  },
  {
    "objectID": "about.html#beyond-the-work",
    "href": "about.html#beyond-the-work",
    "title": "Hi. I’m Jitin. I help Senior Professionals & Business Owners build AI-Enabled Enterprises.",
    "section": "BEYOND THE WORK",
    "text": "BEYOND THE WORK\nWhen I’m not building AI systems or teaching leaders, you’ll find me:\n\nReading mathematical proofs for fun (yes, really)\nPlanning next adventure to somewhere unfamiliar\nPlaying guitar badly but enthusiastically\n\nThe unfamiliar keeps me sharp. Some of my best insights come from being completely outside my comfort zone. That’s why I travel to places I’ve never been. That’s why I left corporate (the scariest decision I’ve made).\nGrowth happens in discomfort."
  },
  {
    "objectID": "about.html#why-now",
    "href": "about.html#why-now",
    "title": "Hi. I’m Jitin. I help Senior Professionals & Business Owners build AI-Enabled Enterprises.",
    "section": "WHY NOW?",
    "text": "WHY NOW?\nAI is table stakes. Every company knows this. But most are moving in the wrong direction.\nThey ask: “What AI tools should we use?”\nWrong question.\nRight question: “Which of our problems can actually be solved by AI?”\nBy the time most leaders figure this out, they’ve wasted ₹50L+ and 6 months.\nI’m here to prevent that.\nThe cohort is live. 3 days. Limited to 50 leaders per cohort. January 9–11, 2026."
  },
  {
    "objectID": "about.html#want-to-work-with-me",
    "href": "about.html#want-to-work-with-me",
    "title": "Hi. I’m Jitin. I help Senior Professionals & Business Owners build AI-Enabled Enterprises.",
    "section": "WANT TO WORK WITH ME?",
    "text": "WANT TO WORK WITH ME?\nOption 1: Join the AI Profit OS Cohort\n3 days. Learn the diagnostic system. Execute in 30–90 days. Walk away with frameworks you’ll use forever.\n₹24,978 (founding rate for January cohort).\nJoin The Cohort\n\nOption 2: Direct Consulting (Limited Availability)\nI take on a select number of consulting engagements for organizations ready to move fast.\nExplore Consulting\n\nOption 3: Just Talk\nNo pressure. No sales. Just a conversation about your challenge.\nEmail me\n{{&lt; include includes/_subs.qmd&gt;}}"
  },
  {
    "objectID": "blog/engineering/02_hypothesis_test/index.html",
    "href": "blog/engineering/02_hypothesis_test/index.html",
    "title": "A flow to Test Your Hypothesis in Python",
    "section": "",
    "text": "Hypothesis testing Photo by Tara Winstead"
  },
  {
    "objectID": "blog/engineering/02_hypothesis_test/index.html#overview",
    "href": "blog/engineering/02_hypothesis_test/index.html#overview",
    "title": "A flow to Test Your Hypothesis in Python",
    "section": "Overview",
    "text": "Overview\nAll the practitioners of data science always hit one giant thing to do with data and you know it well its EDA -Exploratory Data Analysis. This word EDA1 was coined by Tukey himself in his seminal book published in 1983. But do you think that before that EDA doesn’t existed ?\n1 Emerson, J. D., & Hoaglin, D. C. (1983). Stem-and-leaf displays. In D. C. Hoaglin, F. Mosteller, & J. W. Tukey (Eds.) Understanding Robust and Exploratory Data Analysis, pp. 7–32. New York: Wiley. Book is here.Well glad you thought. Before that all were doing what is called as Hypothesis Testing. Yes, before this the race was majorly to fit the data and make most unbiased and robust estimate. But remember one thing when you talk about Hypothesis Testing it was always and majorly would be related to RCTs (Randomized Controlled Trials) a.k.a Randomized Clinical Trials and is Gold Standard of data.\n\n\n\n\n\n\nTipMore on RCTs and ODs\n\n\n\n\n\nNow let me now not hijack the discussion to what is RCTs and Observational Data (ODs) as it is more of Philosophical Reasoning rather than other quality of data, but essentially what we are trying to find is that can we by, using stats, identify interesting patterns in data.\nThe only thing happens wit RCT data is that we tend to believe these interesting patterns coincide with some sort of ‘Cause-Effect’ kind of relationship. But essentially due to bia nature of ODs, we certainly cant conclude this. And hence, can only find interesting patterns.\n\n\n\nLets move on. The big question is, for whatever reason you are doing HT , you are doing it for finding something intreating. And that something interesting is usually found by using Post-Hoc Tests. Now there are variety of Post-Hocs available but what is more know and hence easily found to be implemented in Tukey’s HSD.\nSo lets directly jump to how to follow this procedure. We’ll be using bioinfokit for this, as it is much simpler wrapper around whats implemented in statsmodels."
  },
  {
    "objectID": "blog/engineering/02_hypothesis_test/index.html#what-are-the-results",
    "href": "blog/engineering/02_hypothesis_test/index.html#what-are-the-results",
    "title": "A flow to Test Your Hypothesis in Python",
    "section": "What are the results",
    "text": "What are the results\nPheww… Thats too much code right. But that would save a lot of your time in real life. So in real life you would write code as 3 steps below:\n\n\nCode\n# import libraries\nimport pandas as pd\n\n# Getting car data from UCI\ndf = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data',\n                 sep='\\s+',header=None,\n                 names=['mpg','cylinders','displacement','horsepower','weight',\n                 'acceleration','model_year','origin','car_name'])\ndf.head()\n\n# Syntax to do anove with validating the assumption, doing test and a post-hoc\nresults = do_anova_test(df=df, res_var='mpg',xfac_var='cylinders', \n                        anova_model='mpg ~ C(cylinders)+C(origin)+C(cylinders):C(origin)',\n                        ss_typ=3, result_full=True)\n\n\nResults form the do_anova_test\nLevens Test Result:\n                 Parameter    Value\n0      Test statistics (W)  14.5856\n1  Degrees of freedom (Df)   4.0000\n2                  p value   0.0000\n\nBartletts Test Result:\n                 Parameter    Value\n0      Test statistics (T)  61.2143\n1  Degrees of freedom (Df)   4.0000\n2                  p value   0.0000\n\nANOVA\\ANCOVA Test Result:\n                           df     sum_sq    mean_sq         F  PR(&gt;F)      n2\nIntercept                 1.0  6195.1701  6195.1701  296.3452  0.0000  0.2727\nC(cylinders)              4.0  7574.5864  1893.6466   90.5824  0.0000  0.3334\nC(origin)                 2.0   241.0703   120.5351    5.7658  0.0034  0.0106\nC(cylinders):C(origin)    8.0   577.4821    72.1853    3.4530  0.0046  0.0254\nResidual                389.0  8132.1404    20.9052       NaN     NaN     NaN\n\nTukey HSD Result:\n   group1  group2     Diff    Lower    Upper  q-value  p-value\n0       8       4  14.3237  12.8090  15.8383  36.6527   0.0010\n1       8       6   5.0226   3.1804   6.8648  10.5671   0.0010\n2       8       3   5.5869  -0.7990  11.9728   3.3909   0.1183\n3       8       5  12.4036   5.0643  19.7428   6.5503   0.0010\n4       4       6   9.3011   7.6765  10.9256  22.1910   0.0010\n5       4       3   8.7368   2.4102  15.0633   5.3524   0.0017\n6       4       5   1.9201  -5.3676   9.2078   1.0212   0.9000\n7       6       3   0.5643  -5.8486   6.9772   0.3410   0.9000\n8       6       5   7.3810   0.0182  14.7437   3.8854   0.0491\n9       3       5   6.8167  -2.7539  16.3873   2.7606   0.2919\nNice!!!\n\nAnd plotting is even easier\n\n\nCode\n# Numbers are clumsy for most. Making more interpretable plot on above results.\nplot_hsd(results.tukeyhsd.sort_values('Diff'), title=\"Tukey HSD resutls Anova of MPG ~ Cylinder\")\n\n\nResults form the plot_hsd\n\n\n\nTukey’s HSD comparison based on Anova Results\n\n\nPlots look good with ‘p-values’."
  },
  {
    "objectID": "blog/engineering/02_hypothesis_test/index.html#conclusion",
    "href": "blog/engineering/02_hypothesis_test/index.html#conclusion",
    "title": "A flow to Test Your Hypothesis in Python",
    "section": "Conclusion",
    "text": "Conclusion\nNow since we applied the above to a Non RCT we cannot conclude that Difference in mpg based on cylinder is huge specially as number of cylinders goes up. But this statement might not be as explicit as might be appearing from plot. Unless you have a strong believe that the data follows with rules and assumptions of RCTs, we should be only seeking interesting as in associated results and not cause-effect results."
  },
  {
    "objectID": "blog/engineering/02_hypothesis_test/index.html#give-me-the-code",
    "href": "blog/engineering/02_hypothesis_test/index.html#give-me-the-code",
    "title": "A flow to Test Your Hypothesis in Python",
    "section": "Give me “The Code”",
    "text": "Give me “The Code”\n\nPerforming AnovaPlotting Results\n\n\n\n\nAnova Test anova_test.py\nfrom bioinfokit import analys\n\nimport numpy as np\nfrom scipy import stats\n\nclass KeyResults:\n    \"\"\"\n    A basic class to hold all the results\n    \"\"\"\n    \n    def __init__(self,result_full):\n        self.keys = []\n        self.result_full = result_full\n    \n    def add_result(self,name,result):\n        if name == 'tukeyhsd':\n            self.keys.append(name)\n            setattr(self, name, result)\n        elif self.result_full:\n            self.keys.append(name)\n            setattr(self, name, result)\n\n\n# Anova test code\ndef do_anova_test(df, res_var, xfac_var, anova_model,ss_typ=3,\n                  effectsize='n2',result_full=False,add_res=False):\n    \"\"\"\n    Do all sequential anova tests\n    \n    Step 1) Leven's/ bartellet test for checking weather variance is homogenous or not\n    Step 2) Main ANOVA/ANCOVA test\n    Step 3) Tukey's HSD for individual combinations\n    \n    :param df: Pandas DataFrame holding all the columns\n    :param res_var: Variable for which we are checking ANOVA\n    :param xfac_var: Grouping Variables for which we want to do the comparisons\n    :param anova_model: SM formula for the model. This is life savour to make all things work\n    :param result_full: To provide all the results of intermediate steps\n    \n    \"\"\"\n\n    results = KeyResults(result_full)\n    \n    # initialize stat method\n    res = analys.stat()\n    \n    # doing levens test\n    res.levene(df=df, res_var=res_var,xfac_var=xfac_var)\n    print('\\nLeven\\'s Test Result:')\n    print(res.levene_summary)\n    results.add_result('levene',res.levene_summary)\n\n    # doing bartlett test\n    res.bartlett(df=df, res_var=res_var,xfac_var=xfac_var)\n    print('\\nBartlett\\'s Test Result:')\n    print(res.bartlett_summary)\n    results.add_result('bartlett',res.bartlett_summary)\n    \n    # doing anova / ancova\n    res.anova_stat(df=df, res_var=res_var, anova_model=anova_model,ss_typ=ss_typ)\n    aov_res = res.anova_summary\n    \n    # Add effect sizes\n    if effectsize == \"n2\":\n        all_effsize = (aov_res['sum_sq'] / aov_res['sum_sq'].sum()).to_numpy()\n        all_effsize[-1] = np.nan\n    else:\n        ss_resid = aov_res['sum_sq'].iloc[-1]\n        all_effsize = aov_res['sum_sq'].apply(lambda x: x / (x + ss_resid)).to_numpy()\n        all_effsize[-1] = np.nan\n    aov_res[effectsize] = all_effsize\n    #aov_res['bw_'] = res.anova_model_out.params.iloc[-1]\n    aov_res = aov_res.round(4)\n    \n    # printing results\n    print('\\nANOVA\\ANCOVA Test Result:')\n    print(aov_res)\n    results.add_result('anova',res.anova_summary.round(4))\n    results.add_result('anova_model',res.anova_model_out)\n    \n    # doing tukey's hsd top compare the groups\n    res.tukey_hsd(df=df, res_var=res_var,xfac_var=xfac_var, anova_model=anova_model,ss_typ=ss_typ)\n    print('\\nTukey HSD Result:')\n    print(res.tukey_summary.round(4))\n    results.add_result('tukeyhsd',res.tukey_summary.round(4))\n    \n    # add all result componets again if needed \n    if add_res:\n        results.add_result('allresult',res)\n    \n    return results\n\n\n\n\n\n\nPlotting results plot_hsd.py\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.style.use('seaborn-bright')\n\ndef plot_hsd(hsdres,p_cutoff=0.05,title=None,ax=None,figsize=(10,7)):\n     \"\"\"\n     Do plotting of tukeyhsd results\n    \n  \n    :param hsdres: 'tukeyhsd' result form the do_anova_test function\n    :param p_cutoff: Cutoff at which we get say a combination is significant\n    :param title: Title of the plot\n    :param ax: Define or get the matplotlib axes\n    :param figsize: Mention Figure size to draw\n    \n    \"\"\"\n\n    if ax is None:\n        fig,axp = plt.subplots(figsize=figsize)\n    else:\n        axp = ax\n    \n    # helper func\n    p_ind = lambda x : '' if x &gt; 0.1 else ('+' if x &gt; 0.05 else ('*' if x &gt; 0.01 else ('**' if x &gt;0.001 else '***')))\n    label_gen  = lambda x: f\"${x[0]} - {x[1]}\\ |\\ p:{x[2]:0.2f}{p_ind(x[2]):5s}$\"\n    \n    #setting values\n    mask = hsdres['p-value'] &lt;= p_cutoff\n    yticklabs = hsdres[['group1','group2','p-value']].apply(label_gen,axis=1).values\n    ys = np.arange(len(hsdres))\n    \n    # adding plot to axes\n    axp.errorbar(hsdres[~mask]['Diff'],ys[~mask],xerr=np.abs(hsdres[~mask][['Lower',\"Upper\"]]).values.T,\n                fmt='o', color='black', ecolor='lightgray', elinewidth=2, capsize=0)\n    axp.errorbar(hsdres[mask]['Diff'],ys[mask],xerr=np.abs(hsdres[mask][['Lower',\"Upper\"]]).values.T,\n                fmt='o', color='red', ecolor='pink', elinewidth=2, capsize=5)\n    axp.axvline(x=0,linestyle='--',c='skyblue')\n    axp.set_yticks([])\n    (l,u) = axp.get_xlim()\n    axp.set_xlim(l+1.5*l,u)\n    (l,u) = axp.get_xlim()\n    for idx,labs in enumerate(yticklabs):\n        axp.text(l-0.1*l,ys[idx],labs)\n    axp.set_yticklabels([])\n    \n    # finally doing what is needed\n    if ax is None:\n        plt.title('' if title is None else title,fontsize=14)\n        plt.show()\n    else:\n        return axp\n\n\n\n\n\nHope this give you kickstart to find you intresting patterns. Happy Learning!"
  },
  {
    "objectID": "blog/engineering/04_crosstab_sparsity_classification/index.html",
    "href": "blog/engineering/04_crosstab_sparsity_classification/index.html",
    "title": "CrossTab Sparsity for Classification",
    "section": "",
    "text": "Cross Roads where everyone meets!\n\n\n\nIntroduction: A Journey into Data\nPicture this: you’re standing on the icy shores of Antarctica, the wind whipping around you as you watch a colony of Palmer Penguins waddling about, oblivious to the data detective work you’re about to embark on. As a data science architect, you’re not just an observer; you’re a sleuth armed with algorithms and insights, ready to unravel the mysteries hidden within data. Today, we’ll transform raw numbers into powerful narratives using CrossTab Sparsity as our guiding compass. This blog post will demonstrate how this metric can revolutionize classification tasks, shedding light on many fascinating datasets—the charming Palmer Penguins and the serious Obesity, Credit cards data and many more.\n\n\nThe Power of CrossTab Sparsity\n\nWhat is CrossTab Sparsity?\nCrossTab Sparsity isn’t just a fancy term that sounds good at dinner parties; it’s a statistical measure that helps us peer into the intricate relationships between categorical variables. Imagine it as a magnifying glass that reveals how different categories interact within a contingency table. Understanding these interactions is crucial in classification tasks, where the right features can make or break your model (and your day).\nWhy Does It Matter?\nIn the world of data science, especially in classification, selecting relevant features is like picking the right ingredients for a gourmet meal—get it wrong, and you might end up with something unpalatable. CrossTab Sparsity helps us achieve this by:\n\nHighlighting Relationships: It’s like having a friend who always points out when two people are meant to be together—understanding how features interact with the target variable.\nStreamlining Models: Reducing complexity by focusing on significant features means less time spent untangling spaghetti code.\nEnhancing Interpretability: Making models easier to understand and explain to stakeholders is like translating tech jargon into plain English—everyone appreciates that!\n\n\n\n\nData Overview: Our Data People at work here\n\nThe Datasets\nData 1: Estimation of Obesity Levels Based On Eating Habits and Physical Condition\nLittle bit about the data: This dataset, shared on 8/26/2019, looks at obesity levels in people from Mexico, Peru, and Colombia based on their eating habits and physical health. It includes 2,111 records with 16 features, and classifies individuals into different obesity levels, from insufficient weight to obesity type III. Most of the data (77%) was created using a tool, while the rest (23%) was collected directly from users online.\nData 2: Predict Students’ Dropout and Academic Success\nLittle bit about the data: This dataset, shared on 12/12/2021, looks at factors like students’ backgrounds, academic path, and socio-economic status to predict whether they’ll drop out or succeed in their studies. With 4,424 records across 36 features, it covers students from different undergrad programs. The goal is to use machine learning to spot at-risk students early, so schools can offer support. The data has been cleaned and doesn’t have any missing values. It’s a classification task with three outcomes: dropout, still enrolled, or graduated\nKey Features:\n\nMulticlass: Both data set cater a multi class problems with NObeyesdad and Target columns\nMixed Data Type: A good mix of categorical and continuous variables are available for usage.\nSizeable: More than 2 K rows are available for testing.\n\n\n\n\nExploratory Data Analysis (EDA): Setting the Stage\nBefore we dive into model creation, let’s explore our dataset through some quick EDA. Think of this as getting to know your non-obese friends before inviting them to a party.\n\nEDA for Obesity Data\nHere’s a brief code snippet to perform essential EDA on the Obesity dataset:\n\n\nLoading data and generating basic descriptive\n# Load the Obesity data\nraw_df = pd.read_csv('ObesityDataSet_raw_and_data_sinthetic.csv')\ntarget = 'NObeyesdad'\n\n# Load Students data\n\n# Load Credit data\n# raw_data = sm.datasets.get_rdataset(\"credit_data\",'modeldata')\n# raw_df = raw_data.data\n# target = 'Status'\n\n# # Load Palmer penguins data\n# raw_data = sm.datasets.get_rdataset(\"penguins\",'palmerpenguins')\n# raw_df = raw_data.data\n# target = 'species'\n\n\n# # Load Credit data\n# raw_data = sm.datasets.get_rdataset(\"CreditCard\",'AER')\n# raw_df = raw_data.data\n# target = 'card'\n\n\n# setting things up for aal the next steps\nraw_df[target] = raw_df[target].astype('category') \nprint('No of data points available to work:',raw_df.shape)\ndisplay(raw_df.head())\n\n\n# Summary statistics\ndisplay(raw_df.describe())\n\n\nNo of data points available to work: (2111, 17)\n\n\n\n\n\n\n\n\n\nGender\nAge\nHeight\nWeight\nFamil_Hist_Owt\nFAVC\nFCVC\nNCP\nCAEC\nSMOKE\nCH2O\nSCC\nFAF\nTUE\nCALC\nMTRANS\nNObeyesdad\n\n\n\n\n0\nFemale\n21.0\n1.62\n64.0\nyes\nno\n2.0\n3.0\nSometimes\nno\n2.0\nno\n0.0\n1.0\nno\nPublic_Transportation\nNormal_Weight\n\n\n1\nFemale\n21.0\n1.52\n56.0\nyes\nno\n3.0\n3.0\nSometimes\nyes\n3.0\nyes\n3.0\n0.0\nSometimes\nPublic_Transportation\nNormal_Weight\n\n\n2\nMale\n23.0\n1.80\n77.0\nyes\nno\n2.0\n3.0\nSometimes\nno\n2.0\nno\n2.0\n1.0\nFrequently\nPublic_Transportation\nNormal_Weight\n\n\n3\nMale\n27.0\n1.80\n87.0\nno\nno\n3.0\n3.0\nSometimes\nno\n2.0\nno\n2.0\n0.0\nFrequently\nWalking\nOverweight_Level_I\n\n\n4\nMale\n22.0\n1.78\n89.8\nno\nno\n2.0\n1.0\nSometimes\nno\n2.0\nno\n0.0\n0.0\nSometimes\nPublic_Transportation\nOverweight_Level_II\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAge\nHeight\nWeight\nFCVC\nNCP\nCH2O\nFAF\nTUE\n\n\n\n\ncount\n2111.000000\n2111.000000\n2111.000000\n2111.000000\n2111.000000\n2111.000000\n2111.000000\n2111.000000\n\n\nmean\n24.312600\n1.701677\n86.586058\n2.419043\n2.685628\n2.008011\n1.010298\n0.657866\n\n\nstd\n6.345968\n0.093305\n26.191172\n0.533927\n0.778039\n0.612953\n0.850592\n0.608927\n\n\nmin\n14.000000\n1.450000\n39.000000\n1.000000\n1.000000\n1.000000\n0.000000\n0.000000\n\n\n25%\n19.947192\n1.630000\n65.473343\n2.000000\n2.658738\n1.584812\n0.124505\n0.000000\n\n\n50%\n22.777890\n1.700499\n83.000000\n2.385502\n3.000000\n2.000000\n1.000000\n0.625350\n\n\n75%\n26.000000\n1.768464\n107.430682\n3.000000\n3.000000\n2.477420\n1.666678\n1.000000\n\n\nmax\n61.000000\n1.980000\n173.000000\n3.000000\n4.000000\n3.000000\n3.000000\n2.000000\n\n\n\n\n\n\n\n\n\nTarget distribution\n\n\nTarget and Correlation\n# Visualize target data distribution\nplt.figure(figsize=(4, 3))\nsns.countplot(data=raw_df, x=target, hue=target, palette='Set2',)\nplt.title(f'Distribution of {target} levels')\nplt.xticks(rotation=45)\nplt.show()\n\n# Heatmap to check for correlations between numeric variables\ncorr = raw_df.corr('kendall',numeric_only=True)\nsns.heatmap(corr, annot=True, cmap='coolwarm')\nplt.title('Kendall Correlation Heatmap')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoneSome Mode EDA for the data\n\n\n\n\n\n\n\nEDA code\n# Visualize the distribution of numerical variables\nsns.pairplot(raw_df, hue=target, corner=True)\nplt.show()\n\n\n\n\n# Gettign Categorical data\ncategorical_columns = raw_df.select_dtypes(include='object').columns\n\n# Plot categorical variables with respect to the target variable\nfor col in categorical_columns:\n    plt.figure(figsize=(12, 5))\n    sns.countplot(data=raw_df,x=col, hue=target)\n    plt.title(f\"Countplot of {col} with respect to {target}\")\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Creation: Establishing a Baseline\nWith our exploratory analysis complete, we’re ready to create our baseline model using logistic regression with Statsmodels. This initial model will serve as our reference point—like setting up a benchmark for your favorite video game.\n\n\nSplitting data and training a default Multinomila Logit model on our data\ndata_df = raw_df.dropna().reset_index(drop=True)\ndata_df[target] = data_df[target].cat.codes\n# X = data_df[['bill_length_mm','bill_depth_mm','flipper_length_mm','body_mass_g']] \n\ndata_df_test = data_df.sample(frac=0.1,random_state=3)\ndata_df_train = data_df.drop(data_df_test.index)\n\n# Using MN logistic regression model using formula API\n# This would essentially bold down to pair wise logsitic regression\nlogit_model = sm.MNLogit.from_formula(\n    f\"{target} ~ {' + '.join([col for col in data_df_train.columns if col != target])}\", \n    data=data_df_train\n).fit_regularized()\n\n\nOptimization terminated successfully    (Exit mode 0)\n            Current function value: 0.17057119619320013\n            Iterations: 485\n            Function evaluations: 639\n            Gradient evaluations: 485\n\n\n\n\n\n\n\n\n\n\nNoneBase model summary for geeks\n\n\n\n\n\n\n\nDisplay summary\ndisplay(logit_model.summary())\n\n\n\nMNLogit Regression Results\n\n\nDep. Variable:\nNObeyesdad\nNo. Observations:\n1900\n\n\nModel:\nMNLogit\nDf Residuals:\n1756\n\n\nMethod:\nMLE\nDf Model:\n138\n\n\nDate:\nMon, 15 Dec 2025\nPseudo R-squ.:\n0.9122\n\n\nTime:\n16:07:33\nLog-Likelihood:\n-324.09\n\n\nconverged:\nTrue\nLL-Null:\n-3691.8\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n0.000\n\n\n\n\n\n\n\n\nNObeyesdad=1\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-11.2903\n3.25e+05\n-3.48e-05\n1.000\n-6.36e+05\n6.36e+05\n\n\nGender[T.Male]\n-3.4851\n0.817\n-4.268\n0.000\n-5.085\n-1.885\n\n\nFamil_Hist_Owt[T.yes]\n-0.8162\n0.655\n-1.246\n0.213\n-2.100\n0.468\n\n\nFAVC[T.yes]\n0.2636\n0.785\n0.336\n0.737\n-1.275\n1.802\n\n\nCAEC[T.Frequently]\n-8.2402\n2.312\n-3.564\n0.000\n-12.771\n-3.709\n\n\nCAEC[T.Sometimes]\n-6.2226\n2.232\n-2.787\n0.005\n-10.598\n-1.847\n\n\nCAEC[T.no]\n-8.5977\n2.889\n-2.976\n0.003\n-14.260\n-2.935\n\n\nSMOKE[T.yes]\n4.4919\n3.115\n1.442\n0.149\n-1.614\n10.598\n\n\nSCC[T.yes]\n-0.7294\n1.447\n-0.504\n0.614\n-3.565\n2.106\n\n\nCALC[T.Frequently]\n-12.6192\n3.25e+05\n-3.89e-05\n1.000\n-6.36e+05\n6.36e+05\n\n\nCALC[T.Sometimes]\n-13.2985\n3.25e+05\n-4.1e-05\n1.000\n-6.36e+05\n6.36e+05\n\n\nCALC[T.no]\n-14.1585\n3.25e+05\n-4.36e-05\n1.000\n-6.36e+05\n6.36e+05\n\n\nMTRANS[T.Bike]\n15.8909\n2489.580\n0.006\n0.995\n-4863.596\n4895.378\n\n\nMTRANS[T.Motorbike]\n3.9944\n47.659\n0.084\n0.933\n-89.416\n97.405\n\n\nMTRANS[T.Public_Transportation]\n4.4914\n0.995\n4.514\n0.000\n2.541\n6.441\n\n\nMTRANS[T.Walking]\n4.3554\n1.502\n2.900\n0.004\n1.412\n7.299\n\n\nAge\n0.3721\n0.097\n3.833\n0.000\n0.182\n0.562\n\n\nHeight\n-14.4208\n4.118\n-3.502\n0.000\n-22.492\n-6.349\n\n\nWeight\n1.0786\n0.146\n7.378\n0.000\n0.792\n1.365\n\n\nFCVC\n-0.7754\n0.429\n-1.806\n0.071\n-1.617\n0.066\n\n\nNCP\n-1.7094\n0.491\n-3.480\n0.001\n-2.672\n-0.747\n\n\nCH2O\n-1.7291\n0.578\n-2.992\n0.003\n-2.862\n-0.596\n\n\nFAF\n-0.1924\n0.280\n-0.688\n0.491\n-0.740\n0.356\n\n\nTUE\n-0.9320\n0.456\n-2.043\n0.041\n-1.826\n-0.038\n\n\nNObeyesdad=2\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n17.4309\nnan\nnan\nnan\nnan\nnan\n\n\nGender[T.Male]\n-14.0384\n1.983\n-7.079\n0.000\n-17.925\n-10.151\n\n\nFamil_Hist_Owt[T.yes]\n2.0527\n1.717\n1.195\n0.232\n-1.313\n5.418\n\n\nFAVC[T.yes]\n0.9668\n1.752\n0.552\n0.581\n-2.468\n4.401\n\n\nCAEC[T.Frequently]\n-10.0052\n4.352\n-2.299\n0.021\n-18.534\n-1.476\n\n\nCAEC[T.Sometimes]\n-1.0074\n3.427\n-0.294\n0.769\n-7.724\n5.709\n\n\nCAEC[T.no]\n-0.4896\n894.479\n-0.001\n1.000\n-1753.637\n1752.658\n\n\nSMOKE[T.yes]\n8.1410\n4.013\n2.029\n0.042\n0.277\n16.005\n\n\nSCC[T.yes]\n-7.6940\n152.983\n-0.050\n0.960\n-307.535\n292.147\n\n\nCALC[T.Frequently]\n-2.4516\nnan\nnan\nnan\nnan\nnan\n\n\nCALC[T.Sometimes]\n-7.5316\nnan\nnan\nnan\nnan\nnan\n\n\nCALC[T.no]\n-7.2301\nnan\nnan\nnan\nnan\nnan\n\n\nMTRANS[T.Bike]\n-11.9350\n8.09e+07\n-1.47e-07\n1.000\n-1.59e+08\n1.59e+08\n\n\nMTRANS[T.Motorbike]\n10.9226\n48.493\n0.225\n0.822\n-84.123\n105.968\n\n\nMTRANS[T.Public_Transportation]\n11.1756\n1.750\n6.387\n0.000\n7.746\n14.605\n\n\nMTRANS[T.Walking]\n1.7281\n2.759\n0.626\n0.531\n-3.679\n7.135\n\n\nAge\n0.8111\n0.132\n6.139\n0.000\n0.552\n1.070\n\n\nHeight\n-184.0385\n14.746\n-12.481\n0.000\n-212.939\n-155.138\n\n\nWeight\n3.9438\n0.288\n13.688\n0.000\n3.379\n4.508\n\n\nFCVC\n0.8915\n1.014\n0.879\n0.379\n-1.095\n2.878\n\n\nNCP\n-1.1415\n0.711\n-1.605\n0.109\n-2.536\n0.253\n\n\nCH2O\n-1.5390\n0.876\n-1.756\n0.079\n-3.256\n0.179\n\n\nFAF\n-1.5295\n0.591\n-2.586\n0.010\n-2.689\n-0.370\n\n\nTUE\n-0.5710\n0.840\n-0.680\n0.497\n-2.217\n1.075\n\n\nNObeyesdad=3\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-138.5068\n1.47e+07\n-9.41e-06\n1.000\n-2.89e+07\n2.89e+07\n\n\nGender[T.Male]\n-16.6365\n8.279\n-2.010\n0.044\n-32.863\n-0.410\n\n\nFamil_Hist_Owt[T.yes]\n2.3538\n11.601\n0.203\n0.839\n-20.384\n25.092\n\n\nFAVC[T.yes]\n-8.7785\n5.476\n-1.603\n0.109\n-19.512\n1.955\n\n\nCAEC[T.Frequently]\n-71.7022\nnan\nnan\nnan\nnan\nnan\n\n\nCAEC[T.Sometimes]\n-3.9034\n4.734\n-0.824\n0.410\n-13.183\n5.376\n\n\nCAEC[T.no]\n7.7265\n895.063\n0.009\n0.993\n-1746.566\n1762.019\n\n\nSMOKE[T.yes]\n3.5306\n19.342\n0.183\n0.855\n-34.379\n41.440\n\n\nSCC[T.yes]\n-19.4879\n154.607\n-0.126\n0.900\n-322.512\n283.536\n\n\nCALC[T.Frequently]\n-43.6020\n1.48e+07\n-2.95e-06\n1.000\n-2.9e+07\n2.9e+07\n\n\nCALC[T.Sometimes]\n-45.7496\n1.47e+07\n-3.11e-06\n1.000\n-2.88e+07\n2.88e+07\n\n\nCALC[T.no]\n-28.2183\n1.43e+07\n-1.97e-06\n1.000\n-2.81e+07\n2.81e+07\n\n\nMTRANS[T.Bike]\n0.0376\nnan\nnan\nnan\nnan\nnan\n\n\nMTRANS[T.Motorbike]\n-2.3812\n1.05e+11\n-2.27e-11\n1.000\n-2.06e+11\n2.06e+11\n\n\nMTRANS[T.Public_Transportation]\n22.5234\n6.664\n3.380\n0.001\n9.463\n35.584\n\n\nMTRANS[T.Walking]\n-5.3334\n33.279\n-0.160\n0.873\n-70.560\n59.893\n\n\nAge\n2.5106\n0.964\n2.605\n0.009\n0.621\n4.400\n\n\nHeight\n-278.9439\n44.201\n-6.311\n0.000\n-365.576\n-192.312\n\n\nWeight\n7.1539\n1.394\n5.132\n0.000\n4.422\n9.886\n\n\nFCVC\n4.1064\n3.285\n1.250\n0.211\n-2.333\n10.546\n\n\nNCP\n-1.5637\n2.424\n-0.645\n0.519\n-6.315\n3.187\n\n\nCH2O\n-13.4088\n5.560\n-2.412\n0.016\n-24.306\n-2.511\n\n\nFAF\n-9.8534\n4.356\n-2.262\n0.024\n-18.390\n-1.316\n\n\nTUE\n-5.6951\n3.292\n-1.730\n0.084\n-12.147\n0.757\n\n\nNObeyesdad=4\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-87.3214\nnan\nnan\nnan\nnan\nnan\n\n\nGender[T.Male]\n-200.3037\n5.41e+07\n-3.7e-06\n1.000\n-1.06e+08\n1.06e+08\n\n\nFamil_Hist_Owt[T.yes]\n-30.9252\nnan\nnan\nnan\nnan\nnan\n\n\nFAVC[T.yes]\n-53.1818\n3.98e+07\n-1.34e-06\n1.000\n-7.8e+07\n7.8e+07\n\n\nCAEC[T.Frequently]\n-28.5483\nnan\nnan\nnan\nnan\nnan\n\n\nCAEC[T.Sometimes]\n-21.5821\n5.38e+07\n-4.01e-07\n1.000\n-1.05e+08\n1.05e+08\n\n\nCAEC[T.no]\n-2.2000\n4.62e+29\n-4.76e-30\n1.000\n-9.06e+29\n9.06e+29\n\n\nSMOKE[T.yes]\n-6.0944\nnan\nnan\nnan\nnan\nnan\n\n\nSCC[T.yes]\n-12.3054\nnan\nnan\nnan\nnan\nnan\n\n\nCALC[T.Frequently]\n-6.2460\nnan\nnan\nnan\nnan\nnan\n\n\nCALC[T.Sometimes]\n-37.2004\n2.12e+08\n-1.76e-07\n1.000\n-4.15e+08\n4.15e+08\n\n\nCALC[T.no]\n-64.5032\nnan\nnan\nnan\nnan\nnan\n\n\nMTRANS[T.Bike]\n-0.2989\n1.92e+53\n-1.56e-54\n1.000\n-3.76e+53\n3.76e+53\n\n\nMTRANS[T.Motorbike]\n-0.2031\nnan\nnan\nnan\nnan\nnan\n\n\nMTRANS[T.Public_Transportation]\n-57.6854\n7.04e+07\n-8.2e-07\n1.000\n-1.38e+08\n1.38e+08\n\n\nMTRANS[T.Walking]\n-7.4464\n2.03e+15\n-3.66e-15\n1.000\n-3.98e+15\n3.98e+15\n\n\nAge\n-9.3747\n103.246\n-0.091\n0.928\n-211.733\n192.984\n\n\nHeight\n-174.4727\n592.866\n-0.294\n0.769\n-1336.469\n987.523\n\n\nWeight\n8.7405\n35.222\n0.248\n0.804\n-60.293\n77.774\n\n\nFCVC\n49.0613\n3.02e+04\n0.002\n0.999\n-5.91e+04\n5.92e+04\n\n\nNCP\n2.3650\n4572.743\n0.001\n1.000\n-8960.047\n8964.777\n\n\nCH2O\n-18.5809\n34.347\n-0.541\n0.589\n-85.900\n48.738\n\n\nFAF\n-65.1761\n262.887\n-0.248\n0.804\n-580.424\n450.072\n\n\nTUE\n-44.3721\n285.217\n-0.156\n0.876\n-603.387\n514.643\n\n\nNObeyesdad=5\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-12.5683\n3.25e+05\n-3.87e-05\n1.000\n-6.36e+05\n6.36e+05\n\n\nGender[T.Male]\n-6.8149\n1.085\n-6.282\n0.000\n-8.941\n-4.689\n\n\nFamil_Hist_Owt[T.yes]\n-0.5822\n0.790\n-0.737\n0.461\n-2.130\n0.966\n\n\nFAVC[T.yes]\n2.6008\n0.978\n2.660\n0.008\n0.684\n4.517\n\n\nCAEC[T.Frequently]\n-7.2298\n2.507\n-2.884\n0.004\n-12.143\n-2.316\n\n\nCAEC[T.Sometimes]\n-2.8197\n2.413\n-1.168\n0.243\n-7.550\n1.910\n\n\nCAEC[T.no]\n-3.8181\n3.143\n-1.215\n0.224\n-9.977\n2.341\n\n\nSMOKE[T.yes]\n3.1451\n3.296\n0.954\n0.340\n-3.314\n9.604\n\n\nSCC[T.yes]\n2.1647\n1.617\n1.339\n0.181\n-1.004\n5.334\n\n\nCALC[T.Frequently]\n-9.0315\n3.25e+05\n-2.78e-05\n1.000\n-6.36e+05\n6.36e+05\n\n\nCALC[T.Sometimes]\n-9.1446\n3.25e+05\n-2.82e-05\n1.000\n-6.36e+05\n6.36e+05\n\n\nCALC[T.no]\n-10.7708\n3.25e+05\n-3.32e-05\n1.000\n-6.36e+05\n6.36e+05\n\n\nMTRANS[T.Bike]\n19.0425\n2489.581\n0.008\n0.994\n-4860.446\n4898.531\n\n\nMTRANS[T.Motorbike]\n1.6235\n47.716\n0.034\n0.973\n-91.899\n95.146\n\n\nMTRANS[T.Public_Transportation]\n5.9777\n1.209\n4.946\n0.000\n3.609\n8.346\n\n\nMTRANS[T.Walking]\n4.3596\n1.776\n2.454\n0.014\n0.878\n7.841\n\n\nAge\n0.4878\n0.106\n4.597\n0.000\n0.280\n0.696\n\n\nHeight\n-50.0157\n6.721\n-7.442\n0.000\n-63.188\n-36.844\n\n\nWeight\n1.7920\n0.168\n10.651\n0.000\n1.462\n2.122\n\n\nFCVC\n-0.8369\n0.601\n-1.393\n0.164\n-2.014\n0.341\n\n\nNCP\n-1.4453\n0.554\n-2.608\n0.009\n-2.531\n-0.359\n\n\nCH2O\n-1.7648\n0.679\n-2.601\n0.009\n-3.095\n-0.435\n\n\nFAF\n-0.5613\n0.374\n-1.499\n0.134\n-1.295\n0.172\n\n\nTUE\n-0.7982\n0.555\n-1.439\n0.150\n-1.886\n0.289\n\n\nNObeyesdad=6\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-2.1693\n6.28e+06\n-3.45e-07\n1.000\n-1.23e+07\n1.23e+07\n\n\nGender[T.Male]\n-6.6857\n1.207\n-5.537\n0.000\n-9.052\n-4.319\n\n\nFamil_Hist_Owt[T.yes]\n1.9296\n1.076\n1.793\n0.073\n-0.179\n4.038\n\n\nFAVC[T.yes]\n-0.4617\n1.141\n-0.405\n0.686\n-2.698\n1.775\n\n\nCAEC[T.Frequently]\n-5.5324\n3.264\n-1.695\n0.090\n-11.930\n0.866\n\n\nCAEC[T.Sometimes]\n0.7854\n3.044\n0.258\n0.796\n-5.181\n6.752\n\n\nCAEC[T.no]\n1.7141\n3.934\n0.436\n0.663\n-5.997\n9.426\n\n\nSMOKE[T.yes]\n7.0398\n3.570\n1.972\n0.049\n0.043\n14.036\n\n\nSCC[T.yes]\n1.3664\n2.012\n0.679\n0.497\n-2.577\n5.309\n\n\nCALC[T.Frequently]\n-2.1001\n6.28e+06\n-3.34e-07\n1.000\n-1.23e+07\n1.23e+07\n\n\nCALC[T.Sometimes]\n-4.6772\n6.28e+06\n-7.45e-07\n1.000\n-1.23e+07\n1.23e+07\n\n\nCALC[T.no]\n-4.1972\n6.28e+06\n-6.68e-07\n1.000\n-1.23e+07\n1.23e+07\n\n\nMTRANS[T.Bike]\n-21.8420\n6.54e+09\n-3.34e-09\n1.000\n-1.28e+10\n1.28e+10\n\n\nMTRANS[T.Motorbike]\n3.2252\n47.781\n0.068\n0.946\n-90.423\n96.873\n\n\nMTRANS[T.Public_Transportation]\n8.8055\n1.416\n6.219\n0.000\n6.030\n11.581\n\n\nMTRANS[T.Walking]\n1.2540\n2.256\n0.556\n0.578\n-3.168\n5.676\n\n\nAge\n0.7030\n0.116\n6.086\n0.000\n0.477\n0.929\n\n\nHeight\n-104.6838\n9.021\n-11.605\n0.000\n-122.364\n-87.003\n\n\nWeight\n2.6259\n0.190\n13.819\n0.000\n2.253\n2.998\n\n\nFCVC\n0.1776\n0.764\n0.232\n0.816\n-1.320\n1.675\n\n\nNCP\n-1.8276\n0.608\n-3.007\n0.003\n-3.019\n-0.636\n\n\nCH2O\n-1.8930\n0.757\n-2.502\n0.012\n-3.376\n-0.410\n\n\nFAF\n-1.0280\n0.438\n-2.347\n0.019\n-1.887\n-0.169\n\n\nTUE\n0.1282\n0.670\n0.191\n0.848\n-1.186\n1.442\n\n\n\n\n\n\n\n\n\nEvaluating Model Performance\nTo gauge our models’ effectiveness, we’ll employ various metrics such as accuracy, precision, recall, and F1-score. A confusion matrix will help visualize how well our models perform in classifying outcomes—think of it as a report card for your model!\n\n\nEvaluating the Logit model\n# Predict on test data\nbase_preds = logit_model.predict(data_df_test).idxmax(axis=1)\ny_test = data_df_test[target]\n\n# Evaluate the model\naccuracy_orig = accuracy_score(y_test, base_preds)\nreport_orig = classification_report(y_test, base_preds)\n\nprint(\"Accuracy:\", accuracy_orig)\nprint(\"Classification Report:\")\nprint(report_orig)\n\n\nAccuracy: 0.909952606635071\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.93      0.86      0.89        29\n           1       0.86      0.83      0.84        29\n           2       0.95      0.91      0.93        45\n           3       0.94      0.97      0.95        31\n           4       1.00      0.96      0.98        27\n           5       0.83      0.90      0.86        21\n           6       0.84      0.93      0.89        29\n\n    accuracy                           0.91       211\n   macro avg       0.91      0.91      0.91       211\nweighted avg       0.91      0.91      0.91       211\n\n\n\n\n\n\nLooking for some Improvments!\n\nFeature Selection Using CrossTab Sparsity\nNow comes the exciting part—using CrossTab Sparsity to refine our feature selection process! It’s like cleaning up your closet and only keeping the clothes that spark joy (thank you, Marie Kondo). 1\n1 This is based on work in Unique Metric for Health Analysis with Optimization of Clustering Activity and Cross Comparison of Results from Different Approach. Paper LinkCode is here!\n\n\nStandared Steps for Feature Selection\n\nCalculate CrossTab Sparsity: For each feature against the target variable.\nSelect Features: Based on sparsity scores that indicate significant interactions with the target variable.\nRecreate Models: Train new models using only the selected features—less is often more!\n\nHere we go!!!\n\n\n\nDoing what needs to Done Code ;)\nsns.set_style(\"white\")\nsns.set_context(\"paper\")\n# Calculating Crostab sparsity for each Column\nresults = crosstab_sparsity(data_df_train.iloc[:,:-1],data_df_train[target],numeric_bin='decile')\n\n# presenting results for consumption\ndf_long = pd.melt(results['scores'], id_vars=['Columns'], value_vars=['seggregation', 'explaination', 'metric'],\n                  var_name='Metric', value_name='values')\n\n# Adding jitter: small random noise to 'Columns' (x-axis)\n# df_long['values_jittered'] = df_long['Value'] + np.random.uniform(-0.1, 0.1, size=len(df_long))\n\n# Create a seaborn scatter plot with jitter, more professional color palette, and transparency\nplt.figure(figsize=(12, 5))\nsns.scatterplot(x='Columns', y='values', hue='Metric', style='Metric',\n        data=df_long, s=100, alpha=0.7, palette='deep')\n\n# Title and labels\nplt.title('Metrics by Columns', fontsize=16)\nplt.xticks(rotation=45) \nplt.xlabel('Columns', fontsize=10)\nplt.ylabel('Value', fontsize=10)\n\n# Display legend outside the plot for better readability\nplt.legend(title='Metric', loc='upper right', fancybox=True, framealpha=0.5)\n\n# Show the plot\nplt.tight_layout()\nplt.show()\n\n\nCSP calculated with decile for breaks!\n\nScores for 7 groups(s) is : 140.96057955229762\n\n\n\n\n\n\n\n\n\n\n\n\nAnd Drum Rolls pelase!!!\nUsing just top 5 varaibles we are getting almost similar or better overall accuracy. This amounts to greatly simplifing the models and clearly explain why some variable are not useful for modeling.\n\n\nAnd finally training and evaluating with drum rolls\nlogit_model_rev = sm.MNLogit.from_formula(f\"{target} ~ {' + '.join(results['scores'].loc[:5,'Columns'].values)}\", \n    data=data_df_train\n).fit_regularized()\n\n# Predict on test data\nchallenger_preds = logit_model_rev.predict(data_df_test).idxmax(axis=1)\ny_test = data_df_test[target]\n\n# Evaluate the model\naccuracy_new = accuracy_score(y_test, challenger_preds)\nreport_new = classification_report(y_test, challenger_preds)\n\nprint(\"Accuracy:\", accuracy_new)\nprint(\"Classification Report:\")\nprint(report_new)\n\n\nSingular matrix E in LSQ subproblem    (Exit mode 5)\n            Current function value: nan\n            Iterations: 470\n            Function evaluations: 1227\n            Gradient evaluations: 470\nAccuracy: 0.9383886255924171\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.93      0.97      0.95        29\n           1       0.93      0.93      0.93        29\n           2       0.96      1.00      0.98        45\n           3       0.93      0.90      0.92        31\n           4       0.93      0.93      0.93        27\n           5       0.90      0.90      0.90        21\n           6       0.96      0.90      0.93        29\n\n    accuracy                           0.94       211\n   macro avg       0.94      0.93      0.93       211\nweighted avg       0.94      0.94      0.94       211\n\n\n\n/home/jitin/Documents/applications/perceptions/.venv/lib/python3.12/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n  warnings.warn(\"Maximum Likelihood optimization failed to \"\n\n\n\n\n\n\n\n\n\n\nNoneSummary of retrained model\n\n\n\n\n\n\n\nCode\ndisplay(logit_model_rev.summary())\n\n\n\nMNLogit Regression Results\n\n\nDep. Variable:\nNObeyesdad\nNo. Observations:\n1900\n\n\nModel:\nMNLogit\nDf Residuals:\n1858\n\n\nMethod:\nMLE\nDf Model:\n36\n\n\nDate:\nMon, 15 Dec 2025\nPseudo R-squ.:\nnan\n\n\nTime:\n16:07:34\nLog-Likelihood:\nnan\n\n\nconverged:\nFalse\nLL-Null:\n-3691.8\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\nnan\n\n\n\n\n\n\n\n\nNObeyesdad=1\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n58.1248\nnan\nnan\nnan\nnan\nnan\n\n\nTUE\n0.1130\nnan\nnan\nnan\nnan\nnan\n\n\nCH2O\n-0.8634\nnan\nnan\nnan\nnan\nnan\n\n\nFAF\n0.1425\nnan\nnan\nnan\nnan\nnan\n\n\nAge\n0.0579\nnan\nnan\nnan\nnan\nnan\n\n\nHeight\n-76.5735\nnan\nnan\nnan\nnan\nnan\n\n\nWeight\n1.3337\nnan\nnan\nnan\nnan\nnan\n\n\nNObeyesdad=2\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n328.4616\nnan\nnan\nnan\nnan\nnan\n\n\nTUE\n2.2275\nnan\nnan\nnan\nnan\nnan\n\n\nCH2O\n-1.4150\nnan\nnan\nnan\nnan\nnan\n\n\nFAF\n-1.3585\nnan\nnan\nnan\nnan\nnan\n\n\nAge\n0.1537\nnan\nnan\nnan\nnan\nnan\n\n\nHeight\n-426.3945\nnan\nnan\nnan\nnan\nnan\n\n\nWeight\n5.3584\nnan\nnan\nnan\nnan\nnan\n\n\nNObeyesdad=3\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n306.6447\nnan\nnan\nnan\nnan\nnan\n\n\nTUE\n-7.8630\nnan\nnan\nnan\nnan\nnan\n\n\nCH2O\n-21.0118\nnan\nnan\nnan\nnan\nnan\n\n\nFAF\n-11.3624\nnan\nnan\nnan\nnan\nnan\n\n\nAge\n2.4017\nnan\nnan\nnan\nnan\nnan\n\n\nHeight\n-710.3867\nnan\nnan\nnan\nnan\nnan\n\n\nWeight\n10.1072\nnan\nnan\nnan\nnan\nnan\n\n\nNObeyesdad=4\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n352.4249\nnan\nnan\nnan\nnan\nnan\n\n\nTUE\n-9.2469\nnan\nnan\nnan\nnan\nnan\n\n\nCH2O\n-20.6780\nnan\nnan\nnan\nnan\nnan\n\n\nFAF\n-14.7525\nnan\nnan\nnan\nnan\nnan\n\n\nAge\n2.1487\nnan\nnan\nnan\nnan\nnan\n\n\nHeight\n-758.2318\nnan\nnan\nnan\nnan\nnan\n\n\nWeight\n10.5011\nnan\nnan\nnan\nnan\nnan\n\n\nNObeyesdad=5\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n126.2892\nnan\nnan\nnan\nnan\nnan\n\n\nTUE\n0.5832\nnan\nnan\nnan\nnan\nnan\n\n\nCH2O\n-0.8764\nnan\nnan\nnan\nnan\nnan\n\n\nFAF\n-0.1920\nnan\nnan\nnan\nnan\nnan\n\n\nAge\n0.0719\nnan\nnan\nnan\nnan\nnan\n\n\nHeight\n-160.2982\nnan\nnan\nnan\nnan\nnan\n\n\nWeight\n2.3663\nnan\nnan\nnan\nnan\nnan\n\n\nNObeyesdad=6\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n207.3760\nnan\nnan\nnan\nnan\nnan\n\n\nTUE\n1.6561\nnan\nnan\nnan\nnan\nnan\n\n\nCH2O\n-0.6583\nnan\nnan\nnan\nnan\nnan\n\n\nFAF\n-0.1243\nnan\nnan\nnan\nnan\nnan\n\n\nAge\n0.1042\nnan\nnan\nnan\nnan\nnan\n\n\nHeight\n-266.6050\nnan\nnan\nnan\nnan\nnan\n\n\nWeight\n3.6160\nnan\nnan\nnan\nnan\nnan\n\n\n\n\n\n\n\n\n\n\n\nImpact on Model Accuracy\nAfter applying feature selection based on CrossTab Sparsity, we’ll compare the accuracy of our new models against our baseline models. This comparison will reveal how effectively CrossTab Sparsity enhances classification performance.\n\nResults and Discussion: Unveiling Insights\nModel Comparison Table\nAfter implementing CrossTab Sparsity in our feature selection process, let’s take a look at the results:\n\n\nComparision Code\nmetrics = {\n    \"Metric\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"],\n    \"Baseline Model with all Parameters\": [\n        accuracy_score(y_test, base_preds),\n        precision_score(y_test, base_preds, average='weighted'),\n        recall_score(y_test, base_preds, average='weighted'),\n        f1_score(y_test, base_preds, average='weighted'),\n    ],\n    \"Challenger Model with only 5 Variables\": [\n        accuracy_score(y_test, challenger_preds),\n        precision_score(y_test, challenger_preds, average='weighted'),\n        recall_score(y_test, challenger_preds, average='weighted'),\n        f1_score(y_test, challenger_preds, average='weighted'),\n    ]\n}\ndisplay(pd.DataFrame(metrics).round(4).set_index('Metric').T)\n\n\n\n\n\n\n\n\nMetric\nAccuracy\nPrecision\nRecall\nF1-Score\n\n\n\n\nBaseline Model with all Parameters\n0.9100\n0.9123\n0.9100\n0.9103\n\n\nChallenger Model with only 5 Variables\n0.9384\n0.9384\n0.9384\n0.9381\n\n\n\n\n\n\n\nInsights Gained\nThrough this analysis, several key insights emerge:\n\n\nReduction of similar accuracy from 16 to 5 i.e 68.75% reduction\n\n\n\nFeature Interactions Matter: The selected features based on CrossTab Sparsity significantly improved model accuracy—like finding out which ingredients make your favorite dish even better!\nSimplicity is Key: By focusing on relevant features, we enhance accuracy while simplifying model interpretation—because nobody likes unnecessary complexity.\nReal-World Applications: These findings have practical implications in fields such as environmental science where classification plays a critical role—helping us make better decisions for our planet.\n\n\n\n\nConclusion: The Road Ahead\nIn conclusion, this blog has illustrated how CrossTab Sparsity can be a game-changer in classification tasks using the Obesity dataset. By leveraging this metric for feature selection, we achieved notable improvements in model performance—proof that sometimes less really is more!\nFuture Work: Expanding Horizons\nAs we look ahead, there are exciting avenues to explore:\n\nInvestigating regression problems using CrossTab Sparsity.\nComparing its effectiveness with other feature selection methods such as Recursive Feature Elimination (RFE) or comparision with other feature selection mehtods.\n\nBy continuing this journey into data science, we not only enhance our technical skills but also contribute valuable insights that can drive meaningful change in various industries.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Insights",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nDecision-First AI: Why Data Should Follow, Not Lead\n\n\n\nstrategy\n\nroi\n\nbusiness\n\nai\n\n\n\n\nOct 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrieKNN: Unleashing KNN’s Power on Mixed Data Types\n\n\n\nknn\n\nml\n\nmixed-data\n\n\n\n\nFeb 26, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nCrossTab Sparsity for Classification\n\n\n\nclassification\n\nmetric\n\nfeature selection\n\n\n\n\nJan 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nCrossTab Sparsity\n\n\n\nclustering\n\nanalysis\n\n\n\n\nMay 3, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nA flow to Test Your Hypothesis in Python\n\n\n\neda\n\nhypothesis\n\nanalysis\n\npython\n\n\n\n\nAug 10, 2021\n\n\n\n\n\n\n\n\n\n\n\nAdaptive Regression\n\n\n\nstrategy\n\n\n\n\nMay 1, 2018\n\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "case-studies/index.html",
    "href": "case-studies/index.html",
    "title": "Case Studies – Real Results",
    "section": "",
    "text": "Company: Multi-location retail chain (₹500Cr+ revenue)\nIndustry: FMCG/Retail\nChallenge: Manual demand forecasting wasting ₹7.5Cr/year\nTimeline to first result: 3 months\n\n\n\n\n60+ hours per week spent on manual demand forecasting.\nForecast accuracy: 65% (too low for reliable ordering).\nInventory waste: ₹7.5Cr annually (obsolete stock, overstock, markdowns).\nSuppliers upset because orders were unpredictable.\nCompetitors using AI were ordering more accurately. This company was falling behind.\nThe VP of Operations knew AI could help. But didn’t know which AI, how to evaluate it, or whether it was worth the investment.\n\n\n\n\n\nNo dedicated data science team (but had data analysts)\nCouldn’t spend ₹50L+ on a 6-month consulting project\nNeeded proof-of-concept in under 4 months\nLeadership wouldn’t approve ₹100K+ spend without clear ROI\n\n\n\n\n\nDay 1: AI ART Matrix (Is this an AI problem?) - Applied the framework to their forecasting challenge - Confirmed: YES, this is an AI fit (historical data exists, patterns are learnable) - Ruled out: No, other operational problems in their chain weren’t AI-solvable\nDay 2: O.L.C.D Diagnostic (What exactly do we build?) - Outcome: Improve forecast accuracy from 65% → 90%+ - Logic: Predictive model for demand signals - Capability: No custom ML needed—existing tools (ML via API, Python) could work - Data: They had 3 years of historical orders, seasonal patterns, supplier data\nDay 3: ROI Model (What’s the business case?) - Modeled the before/after: - Labor saved: 60 hrs/week → 12 hrs/week = ₹2.4Cr/year saved - Inventory optimization: ₹7.5Cr waste → ₹1.5Cr waste = ₹6Cr saved - Total: ₹8.4Cr ROI in Year 1 - Presented to CFO: “For ₹50L investment + 3-month build, we save ₹8.4Cr.” - CFO said yes.\n\n\n\n\nAfter 3 months of execution:\n✓ Forecast accuracy: 65% → 91%\n✓ Manual hours: 60/week → 12/week (48 hours saved per person per week)\n✓ Inventory waste: ₹7.5Cr → ₹1.5Cr (₹6Cr saved annually)\n✓ Additional win: Supplier relationships improved (orders now predictable)\n✓ Avoided cost: Didn’t need to hire external consultant (₹100K+ saved)\n\n\n\n\n“Non-technical operators can architect AI when they have a framework.”\nThis VP didn’t code. Didn’t know ML. But by following the three frameworks (AI ART → O.L.C.D → ROI), she: - Diagnosed the problem correctly - Understood what her team needed to build - Convinced her CFO in one meeting - Shipped in 3 months instead of 6+\nThe framework works. The clarity is the bottleneck, not the technology.\n\n\n\n\n\nYou’re 3 days away from knowing exactly which problems in your business can be solved by AI.\nThe same framework that worked for this grocery chain works across industries—retail, finance, auto, healthcare, logistics.\nJanuary cohort starts January 9, 2026.\nSecure Your Seat – ₹24,978\nNext cohort: ₹42,978. Lock in the founding rate.\n\n\n\n[CTA: Join Cohort - Learn the framework] 3 days, ₹24,978. Same frameworks that delivered these results.\n[CTA: Work With Me - Custom strategy] 2–12 weeks. Consulting engagement. More guidance."
  },
  {
    "objectID": "case-studies/index.html#case-study-1-grocery-chain-inventory-optimization",
    "href": "case-studies/index.html#case-study-1-grocery-chain-inventory-optimization",
    "title": "Case Studies – Real Results",
    "section": "",
    "text": "Company: Multi-location retail chain (₹500Cr+ revenue)\nIndustry: FMCG/Retail\nChallenge: Manual demand forecasting wasting ₹7.5Cr/year\nTimeline to first result: 3 months\n\n\n\n\n60+ hours per week spent on manual demand forecasting.\nForecast accuracy: 65% (too low for reliable ordering).\nInventory waste: ₹7.5Cr annually (obsolete stock, overstock, markdowns).\nSuppliers upset because orders were unpredictable.\nCompetitors using AI were ordering more accurately. This company was falling behind.\nThe VP of Operations knew AI could help. But didn’t know which AI, how to evaluate it, or whether it was worth the investment.\n\n\n\n\n\nNo dedicated data science team (but had data analysts)\nCouldn’t spend ₹50L+ on a 6-month consulting project\nNeeded proof-of-concept in under 4 months\nLeadership wouldn’t approve ₹100K+ spend without clear ROI\n\n\n\n\n\nDay 1: AI ART Matrix (Is this an AI problem?) - Applied the framework to their forecasting challenge - Confirmed: YES, this is an AI fit (historical data exists, patterns are learnable) - Ruled out: No, other operational problems in their chain weren’t AI-solvable\nDay 2: O.L.C.D Diagnostic (What exactly do we build?) - Outcome: Improve forecast accuracy from 65% → 90%+ - Logic: Predictive model for demand signals - Capability: No custom ML needed—existing tools (ML via API, Python) could work - Data: They had 3 years of historical orders, seasonal patterns, supplier data\nDay 3: ROI Model (What’s the business case?) - Modeled the before/after: - Labor saved: 60 hrs/week → 12 hrs/week = ₹2.4Cr/year saved - Inventory optimization: ₹7.5Cr waste → ₹1.5Cr waste = ₹6Cr saved - Total: ₹8.4Cr ROI in Year 1 - Presented to CFO: “For ₹50L investment + 3-month build, we save ₹8.4Cr.” - CFO said yes.\n\n\n\n\nAfter 3 months of execution:\n✓ Forecast accuracy: 65% → 91%\n✓ Manual hours: 60/week → 12/week (48 hours saved per person per week)\n✓ Inventory waste: ₹7.5Cr → ₹1.5Cr (₹6Cr saved annually)\n✓ Additional win: Supplier relationships improved (orders now predictable)\n✓ Avoided cost: Didn’t need to hire external consultant (₹100K+ saved)\n\n\n\n\n“Non-technical operators can architect AI when they have a framework.”\nThis VP didn’t code. Didn’t know ML. But by following the three frameworks (AI ART → O.L.C.D → ROI), she: - Diagnosed the problem correctly - Understood what her team needed to build - Convinced her CFO in one meeting - Shipped in 3 months instead of 6+\nThe framework works. The clarity is the bottleneck, not the technology."
  },
  {
    "objectID": "case-studies/index.html#ready-for-your-clarity-moment",
    "href": "case-studies/index.html#ready-for-your-clarity-moment",
    "title": "Case Studies – Real Results",
    "section": "",
    "text": "You’re 3 days away from knowing exactly which problems in your business can be solved by AI.\nThe same framework that worked for this grocery chain works across industries—retail, finance, auto, healthcare, logistics.\nJanuary cohort starts January 9, 2026.\nSecure Your Seat – ₹24,978\nNext cohort: ₹42,978. Lock in the founding rate."
  },
  {
    "objectID": "case-studies/index.html#ready-for-your-clarity-moment-1",
    "href": "case-studies/index.html#ready-for-your-clarity-moment-1",
    "title": "Case Studies – Real Results",
    "section": "",
    "text": "[CTA: Join Cohort - Learn the framework] 3 days, ₹24,978. Same frameworks that delivered these results.\n[CTA: Work With Me - Custom strategy] 2–12 weeks. Consulting engagement. More guidance."
  },
  {
    "objectID": "case-studies/index.html#case-study-2-fortune-500-finance-invoice-processing-automation",
    "href": "case-studies/index.html#case-study-2-fortune-500-finance-invoice-processing-automation",
    "title": "Case Studies – Real Results",
    "section": "CASE STUDY #2: Fortune 500 Finance – Invoice Processing Automation",
    "text": "CASE STUDY #2: Fortune 500 Finance – Invoice Processing Automation\n\nThe Snapshot\nCompany: Mid-market B2B services firm (₹50Cr revenue)\nIndustry: Finance/Back-office\nChallenge: Manual invoice processing eating 40 hrs/week + 12% error rate\nTimeline to first result: 6 weeks\n\n\n\nThe Problem\nEvery invoice = 10+ manual steps: 1. Email arrives 2. Data extraction (manual) 3. Reconciliation against PO 4. Exception handling (% don’t match) 5. Approval workflows 6. Entry into accounting system 7. Follow-up on late invoices\n40 hours/week of pure manual work.\nError rate: 12% (wrong amounts, wrong vendors, wrong accounts—costing money and credibility).\nCFO said: “Can AI automate this?” Team said: “Maybe? We don’t know.”\nNo budget for a ₹100K consulting firm.\n\n\n\nThe Constraints\n\nLimited technical capability (had finance team, not data scientists)\nSmall IT budget\nCouldn’t afford to be wrong (accounting errors have real consequences)\nLeadership needed ROI proof before approval\n\n\n\n\nWhat We Did (The Framework)\nDay 1: AI ART Matrix (Is automation even the right answer?) - First question: “Is this an AI problem or an RPA problem or both?” - Applied the matrix: Clarity emerged - AI alone won’t work (unstructured data) - RPA alone won’t work (too many exceptions) - Answer: Hybrid approach (OCR + RPA + Rule-based logic)\nDay 2: O.L.C.D Diagnostic (What’s the architecture?) - Outcome: Process 95% of invoices automatically, 99%+ accuracy - Logic: OCR to extract → RPA to route → Rules engine for exceptions - Capability: All low-code/no-code tools available (UiPath, Zapier, APIs) - Data: Had invoice PDFs + vendor master + PO system\nDay 3: ROI Model (Does it pay for itself?) - Labor savings: 40 hrs/week @ ₹2000/hr = ₹80L/year - Error reduction: 12% errors → 0.5% errors = ₹20L saved - Total Year 1: ₹100L ROI - Cost: ₹10L for tools + implementation - Payback: 1.2 months\n\n\n\nThe Result\nAfter 6 weeks of execution:\n✓ Automation rate: 85% of invoices process automatically (remaining 15% = true exceptions)\n✓ Time saved: 40 hrs/week → 6 hrs/week (34 hrs saved = ₹68L/year)\n✓ Accuracy: 12% errors → 99.2% accuracy (errors nearly eliminated)\n✓ Cost savings: ₹50L+/year ongoing\n✓ Speed: Invoice-to-payment cycle: 12 days → 3 days\n\n\n\nThe Lesson\n“The right diagnosis beats the fanciest technology.”\nThis CFO could have bought “AI invoice processing software” (wrong). Could have hired a custom coding team (expensive, slow).\nInstead, she diagnosed first: “Is this RPA? Is it AI? Is it both?”\nThen architected: “OCR for data extraction, RPA for routing, rules for exceptions.”\nSimple, low-cost, fast to deploy.\nThe framework led to the right answer."
  },
  {
    "objectID": "case-studies/index.html#cta-module-2",
    "href": "case-studies/index.html#cta-module-2",
    "title": "Case Studies – Real Results",
    "section": "CTA MODULE #2",
    "text": "CTA MODULE #2\n\nThis Could Be Your Next Success Story\nImagine: Clear diagnosis. Confident roadmap. Shipped in 8 weeks instead of 6 months.\nThat’s what happens when leaders have the framework.\nWant to be the next case study?\nJoin the January Cohort – Learn the same diagnostic system that’s worked 50+ times.\nOr if you’d rather explore a custom engagement:\nBook a Strategy Consultation – We’ll map your top 3 opportunities in 60 minutes."
  },
  {
    "objectID": "case-studies/index.html#case-study-3-automotive-supplier-demand-planning",
    "href": "case-studies/index.html#case-study-3-automotive-supplier-demand-planning",
    "title": "Case Studies – Real Results",
    "section": "CASE STUDY #3: Automotive Supplier – Demand Planning",
    "text": "CASE STUDY #3: Automotive Supplier – Demand Planning\n\nThe Snapshot\nCompany: Tier-1 automotive supplier (₹2000Cr+ revenue)\nIndustry: Manufacturing/Supply Chain\nChallenge: Inaccurate demand forecasts causing excess inventory + stockouts\nTimeline to results: 4 months\n\n\n\nThe Problem\nDemand planning was a guessing game:\n\nCustomers’ actual orders arrived at unpredictable times\nSupplier built inventory based on estimates (not real data)\nResult: 35% overstock in some SKUs, 12% stockouts in others\nTied up ₹50Cr in excess inventory\nAngry OEMs because deliveries were inconsistent\n\nThe VP of Supply Chain knew competitors were using AI for this. But didn’t know: - Which AI approach (predictive models? Reinforcement learning? Simple forecasting?) - How much it would cost - How long it would take - Whether it would actually work\n\n\n\nThe Constraints\n\nLarge company, but supply chain team had limited data science access\nNeeded to work within existing IT infrastructure\nBoard approval required (meant explaining in plain business terms)\n₹1Cr budget cap (couldn’t overspend)\n\n\n\n\nWhat We Did (The Framework)\nDay 1: AI ART Matrix (Strategic fit check) - Alignment: YES (competitive advantage if we forecast 10% better than competitors) - Readiness: Partial (had data, but it was siloed across 3 systems) - Transformation: YES (₹50Cr inventory reduction = massive impact) - Decision: Proceed, but phase in\nDay 2: O.L.C.D Diagnostic (Build vs. buy vs. hybrid?) - Outcome: Improve demand forecast accuracy to 88%+ (from 65%) - Logic: Time-series forecasting model + incorporating customer signals - Capability: Could use existing ML platform (already had cloud ML subscription) - Data: Historical orders (5 years) + customer signals + market data\nDay 3: ROI Model (Board presentation) - Inventory reduction: ₹50Cr excess → ₹15Cr excess = ₹35Cr freed up - Working capital improvement: ₹35Cr deployed elsewhere - Stockout reduction: 12% → 3% = Improved customer satisfaction - Year 1 ROI: ₹35Cr + ₹5Cr customer satisfaction gains - Cost: ₹80L for build + deployment\n\n\n\nThe Result\nAfter 4 months:\n✓ Forecast accuracy: 65% → 88%\n✓ Inventory optimization: ₹50Cr → ₹15Cr excess inventory (₹35Cr working capital freed)\n✓ Stockouts: 12% → 3% (customer satisfaction improved)\n✓ Timeline: Built and deployed in 4 months (faster than industry standard of 6-9 months)\n✓ Team adoption: Supply chain team now owns the forecasting system (not dependent on external consultants)\n\n\n\nThe Lesson\n“Strategic clarity beats technical complexity.”\nThis VP of Supply Chain could have: - Hired a ₹500L+ consulting firm (too expensive, too slow) - Bought an off-the-shelf solution (doesn’t fit their specific data) - Built something custom from scratch (6+ months, ₹300L+)\nInstead, she: 1. Diagnosed the problem clearly (AI ART Matrix) 2. Mapped the solution (O.L.C.D) 3. Built the business case (ROI model) 4. Executed with her internal team in 4 months\nThe framework saved time, money, and gave her team ownership."
  },
  {
    "objectID": "case-studies/index.html#cta-module-3",
    "href": "case-studies/index.html#cta-module-3",
    "title": "Case Studies – Real Results",
    "section": "CTA MODULE #3",
    "text": "CTA MODULE #3\n\nThe Pattern Is Clear\nClear diagnosis → Right solution → Real results.\nWhether you’re in retail, finance, auto, or logistics, the framework works.\nYou’re 3 days away from having your own clarity.\nSecure Your Spot in January Cohort\nCost: ₹24,978\nTime: 3 days (Jan 9–11, 2026)\nOutcome: Diagnostic framework + roadmap + ROI model for YOUR top opportunity\nOr book time with me directly if you want to explore before committing:\nSchedule a 15-Min Strategy Call"
  },
  {
    "objectID": "case-studies/index.html#case-study-4-consulting-founder-client-onboarding",
    "href": "case-studies/index.html#case-study-4-consulting-founder-client-onboarding",
    "title": "Case Studies – Real Results",
    "section": "CASE STUDY #4: Consulting Founder – Client Onboarding",
    "text": "CASE STUDY #4: Consulting Founder – Client Onboarding\n\nThe Snapshot\nCompany: Consulting services firm (₹5Cr ARR)\nIndustry: Services/Professional Services\nChallenge: Manual client onboarding limiting growth\nTimeline to results: 4 weeks\n\n\n\nThe Problem\nEvery new client = 3–4 weeks of onboarding:\n\nManual contract review\nData entry into CRM\nApproval loops (2–3 rounds)\nDocument collection (scattered across emails)\nCompliance checks\nAccess provisioning\n\n15+ manual steps = bottleneck.\nFounder wanted to scale to 3x revenue, but couldn’t do it without hiring 3 new ops people.\nOps people cost ₹40L/year each = ₹120L overhead.\nThe founder asked: “Can AI or automation help here?”\nTeam said: “Maybe? We use Zapier for some things, but don’t know if AI changes anything.”\n\n\n\nThe Constraints\n\nBootstrapped company (no VC money, limited budget)\nFounder is the only technical person\nNeeded fast turnaround (wanted to scale now, not in 6 months)\nCouldn’t afford to break the onboarding process (client experience matters)\n\n\n\n\nWhat We Did (The Framework)\nDay 1: AI ART Matrix (Is this an AI, RPA, or workflow problem?) - Applied the matrix: “Is onboarding really an AI problem?” - Honest answer: Not pure AI. More RPA + document automation + workflow - Key insight: Document extraction (AI piece) + automation (RPA piece)\nDay 2: O.L.C.D Diagnostic (What’s the specific architecture?) - Outcome: 3–4 weeks → 4 days for standard clients - Logic: Automate document upload → Extract key fields (AI) → Route to approval → Provision access - Capability: Zapier + Make.com + document extraction API (no-code/low-code) - Data: Had past contracts to train extraction rules\nDay 3: ROI Model (Cost vs. hiring) - Automation cost: ₹2L for tools + build-out - Founder’s time: 2 weeks to implement - Savings vs. hiring: Avoid ₹120L/year in 3 new hires - Payback: &lt; 1 month - Additional benefit: 3x faster onboarding = faster revenue recognition\n\n\n\nThe Result\nAfter 4 weeks:\n✓ Onboarding timeline: 3–4 weeks → 4 days (for standard contracts)\n✓ Manual steps: 15 → 2 (automation handles 13)\n✓ Founder time saved: 8 hrs/week → 30 mins/week\n✓ Scaling enabled: Founder could now handle 3x revenue without new hires\n✓ Cost avoided: Didn’t need to hire 3 ops people (saved ₹120L/year)\n✓ Client satisfaction: Faster onboarding = happier clients, faster project starts\n\n\n\nThe Lesson\n“The diagnosis matters more than the tool.”\nThis founder could have: - Hired ops people (expensive, didn’t solve the problem, just moved it) - Looked for “AI onboarding software” (overkill, didn’t fit his specific workflow) - Given up on scaling (kept revenue capped)\nInstead, she: 1. Asked the right diagnostic question: “Is this an AI problem? RPA? Workflow?” 2. Got honest answer: “It’s a hybrid (document extraction + workflow automation)” 3. Built a ₹2L solution instead of hiring ₹120L in new people 4. Scaled her company without new headcount\nThe framework = clarity. Clarity = right decision."
  },
  {
    "objectID": "case-studies/index.html#cta-module-4",
    "href": "case-studies/index.html#cta-module-4",
    "title": "Case Studies – Real Results",
    "section": "CTA MODULE #4",
    "text": "CTA MODULE #4\n\nYour Onboarding Might Look Different\nBut the principle is the same:\nDiagnose first. Then build.\nIf you’re scaling but manual processes are killing you, you need the framework.\nJanuary cohort: Learn to diagnose your own opportunities.\nReserve Your Spot – ₹24,978, 3 days, lifetime coaching included."
  },
  {
    "objectID": "case-studies/index.html#final-cta-section",
    "href": "case-studies/index.html#final-cta-section",
    "title": "Case Studies – Real Results",
    "section": "FINAL CTA SECTION",
    "text": "FINAL CTA SECTION\n\nFrom Confusion to Clarity in 3 Days\nYou’ve seen what clarity looks like:\n\nGrocery chain: From guessing to 91% accuracy\nFinance: From 12% errors to 99.2% accuracy\nAutomotive: From 65% forecast accuracy to 88%\nConsulting founder: From 3-week onboarding to 4-day onboarding\n\nAll started with the same diagnostic question:\n“Which problems in my business actually need AI?”\nThat’s not a question you answer in 6 months with consultants.\nThat’s a question you answer in 3 days with a framework.\n\n\n\nYour Next Move\nOption 1: Learn the framework (cohort) - 3 days live - Build your own roadmap - Lifetime access to coaching - ₹24,978\nSecure Your Seat – January 9\nOption 2: Explore a custom engagement - If you have a specific problem you want to solve fast - Limited consulting availability - ₹25,000/hour\nSchedule a Strategy Call\nOption 3: Just learn more - Subscribe to my newsletter for more frameworks and real-world examples - No pressure, no pitch\nSubscribe Here"
  },
  {
    "objectID": "case-studies/index.html#design-notes",
    "href": "case-studies/index.html#design-notes",
    "title": "Case Studies – Real Results",
    "section": "DESIGN NOTES",
    "text": "DESIGN NOTES\nCase Study Tile Template (Consistent Across All):\n┌─────────────────────────────────┐\n│ CASE STUDY HEADER               │\n│ Company | Industry | Challenge  │\n│ Timeline to Result              │\n└─────────────────────────────────┘\n\nTHE SNAPSHOT (4 lines, quick context)\n\nTHE PROBLEM (2-3 paragraphs, relatable)\n\nTHE CONSTRAINTS (3-4 bullets, show real barriers)\n\nWHAT WE DID (3 sections: Day 1/2/3 framework application)\n\nTHE RESULT (Before/after metrics, bold the wins)\n\nTHE LESSON (1-2 sentences, the key insight)\n\n[CTA MODULE between each case study]\nColor coding: - Headers: Dark blue (#0a0e27) or white on dark - Metrics: Cyan (#00B8D4) highlight - CTAs: Cyan button (consistent with homepage)"
  },
  {
    "objectID": "consulting/index.html",
    "href": "consulting/index.html",
    "title": "Work Directly With Me",
    "section": "",
    "text": "After 14 years architecting AI systems across automotive, retail, insurance, FMCG, and telecommunications, I’ve learned one thing:\nThe best solutions come from looking sideways.\nMost consultants stay within their industry expertise. I bring patterns from completely different domains to solve problems that seem intractable.\nHealthcare insights solving automotive challenges. Retail strategies transforming manufacturing operations. Mathematical frameworks from applied physics applied to business systems."
  },
  {
    "objectID": "consulting/index.html#the-approach",
    "href": "consulting/index.html#the-approach",
    "title": "Work Directly With Me",
    "section": "",
    "text": "After 14 years architecting AI systems across automotive, retail, insurance, FMCG, and telecommunications, I’ve learned one thing:\nThe best solutions come from looking sideways.\nMost consultants stay within their industry expertise. I bring patterns from completely different domains to solve problems that seem intractable.\nHealthcare insights solving automotive challenges. Retail strategies transforming manufacturing operations. Mathematical frameworks from applied physics applied to business systems."
  },
  {
    "objectID": "consulting/index.html#what-well-work-on",
    "href": "consulting/index.html#what-well-work-on",
    "title": "Work Directly With Me",
    "section": "What We’ll Work On",
    "text": "What We’ll Work On\n\n\nAI Strategy & Architecture\nMap your AI maturity. Design scalable systems. Build transformation roadmaps that actually deploy.\n\n\nCross-Industry Pattern Recognition\nApply insights from adjacent industries. Solve problems others can’t see because they’re looking straight ahead.\n\n\nExecutive Team Alignment\nGet your leadership on the same page about AI transformation. No vendor pitches. Just architecture.\n\n\nImplementation Support\nI don’t just hand you a deck and leave. I work alongside your team until the system is live and scaled."
  },
  {
    "objectID": "consulting/index.html#who-this-is-for",
    "href": "consulting/index.html#who-this-is-for",
    "title": "Work Directly With Me",
    "section": "Who This Is For",
    "text": "Who This Is For\nFortune 500 enterprises with real transformation budgets.\nLeaders who are tired of prescriptive AI and ready for predictive strategy.\nOrganizations ready to commit to systemic change, not point solutions.\nThis isn’t for everyone. But if you’re the right fit, the ROI is undeniable."
  },
  {
    "objectID": "consulting/index.html#current-engagement-model",
    "href": "consulting/index.html#current-engagement-model",
    "title": "Work Directly With Me",
    "section": "Current Engagement Model",
    "text": "Current Engagement Model\nI work with a limited number of clients at any given time, typically serving as Acting CTO or Strategic AI Advisor across multiple accounts.\nTypical engagement: 6-12 months, embedded with your leadership team.\nInvestment: Custom scoped based on organization size and transformation goals."
  },
  {
    "objectID": "consulting/index.html#apply-for-consulting",
    "href": "consulting/index.html#apply-for-consulting",
    "title": "Work Directly With Me",
    "section": "Apply for Consulting",
    "text": "Apply for Consulting\nCurrent waitlist: [Q2 2025]\nIf you’re exploring a major AI transformation and want to discuss whether we’re a fit, let’s talk.\nSchedule a Discovery Call\n\nOr email me directly at jitin@jitinkapila.com"
  },
  {
    "objectID": "faq.html",
    "href": "faq.html",
    "title": "Frequently Asked Questions (FAQs)",
    "section": "",
    "text": "Uh OH! :(\n\n\n\n\nDo I need to know how to code?\n\n\n\nNo. The AI Profit OS is designed for senior professionals, not engineers. Everything is built in Excel + simple tools you already use. If you can use Google Sheets, you can build these systems.\n\n\n\n\n\nI’m not technical. Will I be able to follow?\n\n\n\nAbsolutely. This training is designed for business leaders, not data scientists. I’ve trained CFOs, COOs, and Senior Directors with zero technical background. If you can manage a P&L, you can operationalize AI.\n\n\n\n\n\nWhat if I can’t attend all 3 days live?\n\n\n\nYou’ll get lifetime recording access. However, the live workshops where you build YOUR specific systems are where the real value comes from. We strongly recommend attending live.\n\n\n\n\n\nIs this just ChatGPT training?\n\n\n\nNo. ChatGPT is one tool in the AI Profit OS, but the system is much bigger. You’ll learn: (a) How to structure your workflow for AI leverage, (b) How to build repeatable systems (not one-off prompts), (c) How to measure ROI and scale what works.\n\n\n\n\n\nWhat industries does this work for?\n\n\n\nIf you’re a senior professional doing knowledge work, this works. I’ve deployed AI systems across automotive, retail, FMCG, telecommunications, finance, and healthcare. The frameworks are industry-agnostic.\n\n\n\n\n\nWhat’s the refund policy?\n\n\n\nThe “Unheard Of” Guarantee (see above). If you implement the system and don’t save 10+ hours/week within 30 days, refund + ₹5,000 paid to you. After 30 days, no refunds - but you can transfer your seat to a colleague.\n\n\n\n\n\nCan I bring my team?\n\n\n\nYes! We offer team pricing for 3+ participants from the same organization. Email jitin@jitinkapila.com for custom team rates. (Recommended: Bring 2-3 key leaders to accelerate adoption.)\n\n\n\n\n\nHow is this different from other AI courses?\n\n\n\nMost AI courses teach theory or simple prompting. The AI Profit OS is a complete operational system you’ll deploy in your workflow. You’re not learning about AI - you’re building leverage. By Day 3, you have working systems saving you time.\n\n\n\n\n\nWhat if AI changes? Will this become outdated?\n\n\n\nThe OLCD Protocol, AI-ART Matrix, and operational frameworks are model-agnostic. Whether it’s ChatGPT, Claude, Gemini, or whatever comes next, the system adapts. Plus, you get lifetime access to framework updates.\n\n\n\n\n\n\n\n\nI’m not sure this will work for my specific industry/role.\n\n\n\nThat’s exactly why Day 1 is focused on YOUR workflow and YOUR 90-day roadmap. The frameworks are industry-agnostic, but the implementation is personalized to your reality.\n\n\n\n\n\n₹7,999 feels expensive for a training.\n\n\n\nCompare it to: - (a) One hour of my consulting at ₹25,000/hour, -(b) The cost of being replaced by a 22-year-old who knows how to use AI, -(c) The ₹6.9Cr the grocery chain saved with this exact system. If you don’t save 10+ hours/week, I’ll refund you AND pay you ₹8,000.\n\n\n\n\n\nWhat if I can’t implement this alone?\n\n\n\nYou won’t be alone. You’ll join a private community of senior professionals building AI systems. Plus monthly office hours with me. And if you need hands-on implementation support, 1:1 consulting is available.\n\n\n\n\n\nHow do I know this isn’t just hype?\n\n\n\nLook at the results: ₹6.9Cr grocery chain savings. $11M automotive optimization. £80K/month retail revenue lift. These are real deployments at Mercedes, L’Oréal, Maruti, British Telecom. This isn’t theory - it’s operational leverage.\n\n\n\n\n\nWhat if AI replaces me anyway?\n\n\n\nAI will replace professionals who DON’T learn to operationalize it. Senior professionals who build AI systems become 10x more valuable. You’re not competing with AI - you’re building leverage ON TOP of your 10+ years of expertise.\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "policies.html#refund-policy",
    "href": "policies.html#refund-policy",
    "title": "Privacy",
    "section": "Refund Policy",
    "text": "Refund Policy"
  },
  {
    "objectID": "policies.html#terms-and-conditions",
    "href": "policies.html#terms-and-conditions",
    "title": "Privacy",
    "section": "Terms and Conditions",
    "text": "Terms and Conditions"
  }
]