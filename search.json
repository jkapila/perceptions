[
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "üìß Mail | üîó LinkedIn | üìç Bengaluru, India\n\n\n\nCross-industry problem solver with 14+ years applying mathematical thinking to business challenges across automotive, retail, FMCG, telecommunications, and insurance. Currently serving as Acting CTO for multiple Fortune 500 accounts while developing pattern recognition methodologies that transfer insights between industries.\n\n\n\n\n\n\nMulti-Account Strategic Leadership - Simultaneously managed technical strategy across 4+ Fortune 500 clients - Developed cross-industry pattern recognition framework for solution transfer - Built and mentored cross-functional teams of 15+ professionals - Currently mentoring 25+ professionals across multiple accounts\nBusiness Impact - Automotive: Reduced inventory excess by 45,000 units through predictive analytics - Retail: Optimized marketing allocation resulting in $500K annual budget efficiency - Insurance: Implemented claims analytics reducing costs by ¬•20M - FMCG: Post-pandemic inventory strategy eliminating $850K in holding costs\n\n\n\nEnterprise AI Strategy & Implementation - Led AI transformation initiatives for Fortune 500 companies - Applied machine learning solutions across automotive, FMCG, telecommunications - Consistently delivered 3-7x return on investments within 2-3 year timeframes - Pioneered cross-industry solution adaptation methodology\n\n\n\nFoundation Building & Technical Excellence - Built expertise in advanced analytics, machine learning, and statistical modeling - Gained experience across 6+ industries, developing pattern recognition abilities - Designed scalable AI systems for enterprise-level implementations\n\n\n\n\n\nStrategic Leadership - Cross-Industry Pattern Recognition & Innovation - Team Building & Talent Development - Executive Stakeholder Management\nTechnical Excellence - Advanced Analytics & Machine Learning Architecture - Predictive Modeling & Statistical Analysis - Enterprise AI System Design & Implementation\nIndustry Expertise - Automotive: Inventory optimization, supply chain analytics - Retail/FMCG: Customer analytics, marketing optimization, promotional planning - Insurance: Claims analytics, risk modeling, fraud detection - Telecommunications: Customer lifecycle management, network optimization\n\n\n\n\nMaster of Technolog | Data Sceince & Engineering | BITS Pilani\nBachelor of Engineering | Mechanical Engineering | BIT, Durg\n\n\n\n\nFractional Client AI-CTO across multiple Fortune 500 accounts, developing frameworks for cross-industry insight transfer while building the next generation of analytical leaders.\n\n\nAvailable for senior strategic roles and consulting engagements\nDownload PDF Resume | Get in Touch"
  },
  {
    "objectID": "resume.html#professional-summary",
    "href": "resume.html#professional-summary",
    "title": "Resume",
    "section": "",
    "text": "Cross-industry problem solver with 14+ years applying mathematical thinking to business challenges across automotive, retail, FMCG, telecommunications, and insurance. Currently serving as Acting CTO for multiple Fortune 500 accounts while developing pattern recognition methodologies that transfer insights between industries."
  },
  {
    "objectID": "resume.html#professional-experience",
    "href": "resume.html#professional-experience",
    "title": "Resume",
    "section": "",
    "text": "Multi-Account Strategic Leadership - Simultaneously managed technical strategy across 4+ Fortune 500 clients - Developed cross-industry pattern recognition framework for solution transfer - Built and mentored cross-functional teams of 15+ professionals - Currently mentoring 25+ professionals across multiple accounts\nBusiness Impact - Automotive: Reduced inventory excess by 45,000 units through predictive analytics - Retail: Optimized marketing allocation resulting in $500K annual budget efficiency - Insurance: Implemented claims analytics reducing costs by ¬•20M - FMCG: Post-pandemic inventory strategy eliminating $850K in holding costs\n\n\n\nEnterprise AI Strategy & Implementation - Led AI transformation initiatives for Fortune 500 companies - Applied machine learning solutions across automotive, FMCG, telecommunications - Consistently delivered 3-7x return on investments within 2-3 year timeframes - Pioneered cross-industry solution adaptation methodology\n\n\n\nFoundation Building & Technical Excellence - Built expertise in advanced analytics, machine learning, and statistical modeling - Gained experience across 6+ industries, developing pattern recognition abilities - Designed scalable AI systems for enterprise-level implementations"
  },
  {
    "objectID": "resume.html#core-competencies",
    "href": "resume.html#core-competencies",
    "title": "Resume",
    "section": "",
    "text": "Strategic Leadership - Cross-Industry Pattern Recognition & Innovation - Team Building & Talent Development - Executive Stakeholder Management\nTechnical Excellence - Advanced Analytics & Machine Learning Architecture - Predictive Modeling & Statistical Analysis - Enterprise AI System Design & Implementation\nIndustry Expertise - Automotive: Inventory optimization, supply chain analytics - Retail/FMCG: Customer analytics, marketing optimization, promotional planning - Insurance: Claims analytics, risk modeling, fraud detection - Telecommunications: Customer lifecycle management, network optimization"
  },
  {
    "objectID": "resume.html#education",
    "href": "resume.html#education",
    "title": "Resume",
    "section": "",
    "text": "Master of Technolog | Data Sceince & Engineering | BITS Pilani\nBachelor of Engineering | Mechanical Engineering | BIT, Durg"
  },
  {
    "objectID": "resume.html#current-focus",
    "href": "resume.html#current-focus",
    "title": "Resume",
    "section": "",
    "text": "Fractional Client AI-CTO across multiple Fortune 500 accounts, developing frameworks for cross-industry insight transfer while building the next generation of analytical leaders.\n\n\nAvailable for senior strategic roles and consulting engagements\nDownload PDF Resume | Get in Touch"
  },
  {
    "objectID": "posts/10_treeknn/index.html",
    "href": "posts/10_treeknn/index.html",
    "title": "TrieKNN: Unleashing KNN‚Äôs Power on Mixed Data Types",
    "section": "",
    "text": "Photo by Gelgas Airlangga"
  },
  {
    "objectID": "posts/10_treeknn/index.html#the-allure-and-limitation-of-knn",
    "href": "posts/10_treeknn/index.html#the-allure-and-limitation-of-knn",
    "title": "TrieKNN: Unleashing KNN‚Äôs Power on Mixed Data Types",
    "section": "The Allure and Limitation of KNN",
    "text": "The Allure and Limitation of KNN\nIn the realm of machine learning, the K-Nearest Neighbors (KNN) algorithm stands out for its intuitive nature and ease of implementation. Its principle is simple: classify a data point based on the majority class among its ‚Äòk‚Äô nearest neighbors in the feature space. This non-parametric approach makes no assumptions about the underlying data distribution, rendering it versatile for various applications. KNN is very popular, but it comes with some limitations.\nHowever, KNN‚Äôs Achilles‚Äô heel lies in its reliance on distance metrics, which are inherently designed for numerical data. Real-world datasets often contain a mix of numerical and categorical features, posing a significant challenge for KNN. How do you measure the distance between ‚Äòred‚Äô and ‚Äòblue,‚Äô or ‚Äòlarge‚Äô and ‚Äòsmall‚Äô?\n\nPrior Art\nSeveral strategies have been proposed to adapt KNN for mixed data:\n\nOne-Hot Encoding: Converts categorical features into numerical vectors, but can lead to high dimensionality.\nDistance Functions for Mixed Data: Develops and apply custom distance metrics that can handle both numerical and categorical features such as HEOM and many others.\nUsing mean/mode values: Replace the missing values with mean/mode.\n\nThese methods often involve compromises, either distorting the data‚Äôs inherent structure or adding computational overhead."
  },
  {
    "objectID": "posts/10_treeknn/index.html#enter-trieknn-a-novel-approach",
    "href": "posts/10_treeknn/index.html#enter-trieknn-a-novel-approach",
    "title": "TrieKNN: Unleashing KNN‚Äôs Power on Mixed Data Types",
    "section": "Enter TrieKNN: A Novel Approach",
    "text": "Enter TrieKNN: A Novel Approach\nWhat if we could cleverly sidestep the distance calculation problem for categorical features, while still leveraging KNN‚Äôs power? TrieKNN offers just that‚Äîa way to perform KNN on any mixed data!\nTrieKNN combines the strengths of Trie data structures and KNN to handle mixed data types gracefully. Here‚Äôs the core idea:\n\nTrie-Based Categorical Encoding: A Trie is used to store the categorical features of the data. Each node in the Trie represents a category.\nLeaf-Node KNN Models: At the leaf nodes of the Trie, where specific combinations of categorical features are found, we fit individual KNN models using only the numerical features.\nWeighted Prediction: To classify a new data point, we traverse the Trie based on its categorical features. At each level, we calculate a weighted distance based on available data, ending in a probability score in each leaf node.\n\n\nWhy This Works\n\nNo Direct Distance Calculation for Categorical Features: The Trie structure implicitly captures the relationships between categorical values.\nLocalized KNN Models: By fitting KNN models at the leaf nodes, we ensure that distance calculations are performed only on relevant numerical features.\nScalability: The Trie structure efficiently handles a large number of categorical features and values."
  },
  {
    "objectID": "posts/10_treeknn/index.html#building-a-trieknn-model",
    "href": "posts/10_treeknn/index.html#building-a-trieknn-model",
    "title": "TrieKNN: Unleashing KNN‚Äôs Power on Mixed Data Types",
    "section": "Building a TrieKNN Model",
    "text": "Building a TrieKNN Model\nLet‚Äôs dive into the implementation. We‚Äôll start with the TrieNode and Trie classes, then move on to the KNN model and the training/prediction process.\n\nTrie Implementation\n\n\nCode\nimport numpy as np\nfrom collections import Counter\n\nclass TrieNode:\n    def __init__(self):\n        self.children = {}  # Dictionary to store child nodes\n        self.is_end_of_word = False  # True if the node is the end of a word\n        self.count = 0  # Count of how many times a word has been inserted\n        self.class_counts = {}  # Class counts\n        self.class_weights = {}\n        self.model = None  # Model at leaf nodes\n        self.indexes = []  # Store data indexes belonging to this leaf\n        self.labels = []  # Store data indexes belonging to this leaf\n        self.node_weight = None\n\nclass Trie:\n    def __init__(self):\n        self.root = TrieNode()  # Root node of the Trie\n        self.data_index = 0  # Initialize data index\n\n    def insert(self, word_val, model):\n        current_node = self.root\n        word, val = word_val\n        current_node.count += 1\n\n        # Adding class counts\n        if val not in current_node.class_counts:\n            current_node.class_counts[val] = 0\n        current_node.class_counts[val] += 1\n\n        for char in word:\n            # If the character is not in children, add a new TrieNode\n            if char not in current_node.children:\n                current_node.children[char] = TrieNode()\n            current_node = current_node.children[char]\n\n            # Adding count of instances\n            current_node.count += 1\n\n            # adding class counts\n            if val not in current_node.class_counts:\n                current_node.class_counts[val] = 0\n            current_node.class_counts[val] += 1\n\n        # Mark the end of the word and increment count\n        current_node.is_end_of_word = True\n        current_node.indexes.append(self.data_index)  # Store the data index\n        current_node.labels.append(val)\n        current_node.model = model\n        self.data_index += 1  # Increment data index\n\n    def search(self, word):\n        current_node = self.root\n        for char in word:\n            # If the character doesn't exist in the children, the word doesn't exist\n            if char not in current_node.children:\n                return False\n            current_node = current_node.children[char]\n\n        # Return True if it's the end of a word and the word exists\n        return current_node.is_end_of_word\n\n    def count_word(self, word):\n        current_node = self.root\n        for char in word:\n            # If the character doesn't exist, the word doesn't exist\n            if char not in current_node.children:\n                return 0, current_node.class_counts  # Correctly return class_counts\n            current_node = current_node.children[char]\n\n        # Return the count of the word\n        return current_node.count, current_node.class_counts\n\n    def display(self):\n        # Recursively display the tree\n        def _display(node, word):\n            if node.is_end_of_word:\n                print(f\"Data: {word}, Count: {node.count}, Indexes: {len(node.indexes)} Classes :{node.class_counts} weights:{len(node.class_weights)}\")  # Display indexes too\n            for char, child in node.children.items():\n                _display(child, word + char)  # corrected the display\n\n        _display(self.root, \"\")\n\n    def apply(self, func):\n        \"\"\"\n        Applies a function to all models in the leaf nodes.\n        \"\"\"\n        def _apply(node):\n            if node.is_end_of_word and node.model is not None:\n                func(node)\n            for child in node.children.values():\n                _apply(child)\n\n        _apply(self.root)\n\n    def apply_weight_to_indexes(self, weight):\n        \"\"\"\n        Applies a weight to the indexes based on the percentage of data available.\n        \"\"\"\n        def _apply_weight_to_indexes(node):\n            if node.is_end_of_word:\n                total_count = sum(self.root.children[child].count for child in self.root.children)\n                percentage = node.count / total_count if total_count &gt; 0 else 0\n                weighted_indexes = [(index, weight * percentage) for index in node.indexes]\n                node.class_weights = weighted_indexes  # Corrected this line\n            for child in node.children.values():\n                _apply_weight_to_indexes(child)\n\n        _apply_weight_to_indexes(self.root)\n\n\n\n\nKNN Model\n\n\nCode\nclass KNNModel:\n    def __init__(self, k=5):\n        self.data = None\n        self.labels = []\n        self.k = k\n\n    def fit(self, data, indexes, labels):\n        # print(\"Fitting model with indexes:\", len(indexes), \"labels:\", len(labels))\n        self.data = data[indexes].astype(float)\n        self.labels = np.array(labels).astype(float)\n\n    def predict(self, data):\n        # print(\"Predicting with data:\", data)\n        dist_ind = np.sqrt(np.sum((self.data - data) ** 2, axis=1) ** 2)  # euclidean distance\n        main_arr = np.column_stack((self.labels, dist_ind))  # labels with distance\n        main = main_arr[main_arr[:, 1].argsort()]  # sorting based on distance\n        count = Counter(main[0:self.k, 0])  # counting labels\n        sums = np.array(list(count.values()))  # getting counts\n        return sums / np.sum(sums)  # prediction as probability\n\n\n\n\nTraining and Evaluation\nHere‚Äôs how we train and evaluate the TrieKNN model:\n\n\nCode\n# Sample data\nn = 10000\ndata = np.array((np.random.choice(['Anything ', 'By ','Chance '], p=[0.6,0.1,0.3],size=n),\n                 np.random.choice(['can', 'go', 'here','lets', 'see', \"it\"], p=[0.1, 0.1, 0.1, 0.2, 0.4, 0.1], size=n),\n                 np.random.normal(3, 1, size=n),\n                 np.random.normal(5, 2, size=n))).T\ny_label = np.random.choice([0,1], p=[0.7, 0.3], size=n)\n\n# Trie training\ntrie = Trie()\nfor X, y in zip(data, y_label):\n    trie.insert((X[:2], y),None)\n\n# Apply weights to indexes\ntrie.apply_weight_to_indexes(0.5)\n\n# Fit models of leaf nodes\ndef add_model(node, data):\n    node.model = KNNModel()\n    node.model.fit(data, node.indexes, node.labels)\n\ndef traverse_and_add_model(node, data):\n    if node.is_end_of_word:\n        add_model(node, data)  # Add model to leaf node\n    for child in node.children.values():\n        traverse_and_add_model(child, data)\n\ntraverse_and_add_model(trie.root, data[:, 2:])\n\n\n\n\nExplanation\n\nWe create sample data with mixed categorical and numerical features.\nWe insert each data point into the Trie, using the categorical features as the path.\nAfter the Trie is built, we traverse it and fit a KNN model to the data points stored at each leaf node.\nFinally, we can predict the class of new data points by traversing the Trie and using the KNN model at the corresponding leaf node."
  },
  {
    "objectID": "posts/10_treeknn/index.html#results-and-discussion",
    "href": "posts/10_treeknn/index.html#results-and-discussion",
    "title": "TrieKNN: Unleashing KNN‚Äôs Power on Mixed Data Types",
    "section": "Results and Discussion",
    "text": "Results and Discussion\nLet us display the trie.\n\n\n\n\nCode\ntrie.display()\n\n\nData: Anything see, Count: 2383, Indexes: 2383 Classes :{1: 698, 0: 1685} weights:2383\nData: Anything go, Count: 618, Indexes: 618 Classes :{0: 427, 1: 191} weights:618\nData: Anything lets, Count: 1259, Indexes: 1259 Classes :{1: 365, 0: 894} weights:1259\nData: Anything it, Count: 574, Indexes: 574 Classes :{1: 182, 0: 392} weights:574\nData: Anything can, Count: 592, Indexes: 592 Classes :{0: 411, 1: 181} weights:592\nData: Anything here, Count: 617, Indexes: 617 Classes :{0: 438, 1: 179} weights:617\nData: Chance see, Count: 1217, Indexes: 1217 Classes :{1: 358, 0: 859} weights:1217\nData: Chance it, Count: 300, Indexes: 300 Classes :{0: 214, 1: 86} weights:300\nData: Chance go, Count: 302, Indexes: 302 Classes :{0: 227, 1: 75} weights:302\nData: Chance lets, Count: 549, Indexes: 549 Classes :{1: 165, 0: 384} weights:549\nData: Chance here, Count: 284, Indexes: 284 Classes :{0: 188, 1: 96} weights:284\nData: Chance can, Count: 280, Indexes: 280 Classes :{0: 185, 1: 95} weights:280\nData: By see, Count: 421, Indexes: 421 Classes :{1: 112, 0: 309} weights:421\nData: By go, Count: 81, Indexes: 81 Classes :{0: 64, 1: 17} weights:81\nData: By can, Count: 107, Indexes: 107 Classes :{0: 80, 1: 27} weights:107\nData: By it, Count: 90, Indexes: 90 Classes :{0: 64, 1: 26} weights:90\nData: By here, Count: 107, Indexes: 107 Classes :{0: 75, 1: 32} weights:107\nData: By lets, Count: 219, Indexes: 219 Classes :{0: 149, 1: 70} weights:219\n\n\nThe model predicted the following values:\n\n\n\n\nCode\n# Prediction example\ndef predict_with_model(node):\n    predictions = node.model.predict(np.array([2,5]))\n    print(\"Predictions:\", predictions)\n\ntrie.apply(predict_with_model)\n\n\nPredictions: [0.8 0.2]\nPredictions: [0.6 0.4]\nPredictions: [0.6 0.4]\nPredictions: [0.6 0.4]\nPredictions: [0.4 0.6]\nPredictions: [0.6 0.4]\nPredictions: [0.8 0.2]\nPredictions: [0.4 0.6]\nPredictions: [1.]\nPredictions: [0.4 0.6]\nPredictions: [0.4 0.6]\nPredictions: [1.]\nPredictions: [0.6 0.4]\nPredictions: [0.6 0.4]\nPredictions: [0.4 0.6]\nPredictions: [0.2 0.8]\nPredictions: [0.4 0.6]\nPredictions: [0.4 0.6]\n\n\nThe predictions will vary on each run. From this we can see that we can use KNN on mixed data types."
  },
  {
    "objectID": "posts/10_treeknn/index.html#conclusion-a-promising-path-forward",
    "href": "posts/10_treeknn/index.html#conclusion-a-promising-path-forward",
    "title": "TrieKNN: Unleashing KNN‚Äôs Power on Mixed Data Types",
    "section": "Conclusion: A Promising Path Forward",
    "text": "Conclusion: A Promising Path Forward\nTrieKNN presents a compelling solution for extending the applicability of KNN to mixed data types. By leveraging the Trie data structure, it avoids direct distance calculations on categorical features, enabling the use of localized KNN models for numerical data.\nFurther research could explore:\n\nOptimizing the weighting scheme for combining predictions from different Trie levels.\nComparing TrieKNN‚Äôs performance against other mixed-data KNN approaches on benchmark datasets.\nExtending TrieKNN to handle missing data and noisy categorical features.\n\nTrieKNN opens up new possibilities for applying KNN in domains where mixed data types are prevalent, such as healthcare, e-commerce, and social science.\nResources and further reads:\n1. Nomclust R package\n2. An Improved kNN Based on Class Contribution and Feature Weighting\n3. An Improved Weighted KNN Algorithm for Imbalanced Data Classification\n4. A weighting approach for KNN classifier\n5. Unsupervised Outlier Detection for Mixed-Valued Dataset Based on the Adaptive k-Nearest Neighbor Global Network\n6. A hybrid approach based on k-nearest neighbors and decision tree for software fault prediction\n7. Analysis of Decision Tree and K-Nearest Neighbor Algorithm in the Classification of Breast Cancer"
  },
  {
    "objectID": "posts/03_crosstab_sparsity/index.html",
    "href": "posts/03_crosstab_sparsity/index.html",
    "title": "CrossTab Sparsity",
    "section": "",
    "text": "Cluster analysis has always fascinated me as a window into the hidden structures of data. During my collaboration with Kumarjit Pathak, we grappled with a persistent challenge in unsupervised learning: how to objectively evaluate clustering quality across different algorithms. Traditional metrics like the Silhouette Index or Bayesian Information Criterion felt restrictive‚Äîthey were siloed within specific methodologies, making cross-algorithm comparisons unreliable.\nThis frustration led us to develop a universal cluster evaluation metric, detailed in our paper ‚ÄúCross Comparison of Results from Different Clustering Approaches‚Äù. Our goal was to create a framework that transcends algorithmic biases, enabling:\n- Direct comparison of K-Means vs GMM vs DBSCAN vs PAM vs SOM vs Anything results\n- Identification of variables muddying cluster separation\n- Automated determination of optimal cluster counts\nIn this blog, I‚Äôll walk you through our journey‚Äîfrom conceptualization to real-world validation‚Äîand share insights that didn‚Äôt make it into the final paper."
  },
  {
    "objectID": "posts/03_crosstab_sparsity/index.html#our-aha-moment---crosstab-sparsity",
    "href": "posts/03_crosstab_sparsity/index.html#our-aha-moment---crosstab-sparsity",
    "title": "CrossTab Sparsity",
    "section": "Our ‚ÄúAha!‚Äù Moment - Crosstab Sparsity",
    "text": "Our ‚ÄúAha!‚Äù Moment - Crosstab Sparsity\n\n\n\n\nBest Cluster for K-means Using Crosstab sparsity\n\n\n\nWhile analyzing cross-tab matrices of variable distributions across clusters, we noticed a pattern: well-segregated clusters consistently showed higher frequencies along matrix diagonals. This inspired our two-part metric:\n\n\n\nSegregation Factor:\n# Simplified calculation from our codebase  \nmedian = np.median(cross_tab)  \nN_vk = np.sum(cross_tab &gt; median)  # Count \"well-segregated\" instances  \nExplanation Factor:\nexplanation = np.log(len(data) / (bins * clusters))  \n\n\nSegregation Factor: Measures how distinctly clusters separate data points. We used the median (not mean) to avoid skew from outlier-dominated matrices.\nExplanation Factor: Quantifies how well clusters capture data variability. The logarithmic term penalizes overfitting‚Äîa critical insight from debugging early over-segmented clusters.\n\nAnd the Final Formula:\nFor variable \\(v\\) with \\(k\\) clusters:\n\\[\nS_v^k = \\underbrace{\\frac{N_v^k}{\\max(l, k)}}_{\\text{Segregation}} \\times \\underbrace{\\ln\\left(\\frac{N_d}{l \\times k}\\right)}_{\\text{Explanation}}\n\\]\nwhere:\n- \\(N_v^k\\): Segregated instances (values above cross-tab matrix median)\n- \\(l\\): Number of value intervals for variable \\(v\\)\n- \\(N_d\\): Total observations\nThis formulation ensures algorithmic invariance, allowing comparison across methods like K-Means (distance-based) and GMM (probability-based). Also, now you can see from the formula two scenarios happens: 1. If each variable crosstab is too dense then their is no separation between classes 2. If each variable crosstab is too sparse then we loose on explanation.\nHence the curve reaches a maximum and then falls down giving use the separability that the cluster can produce:"
  },
  {
    "objectID": "posts/01_adaptive_regression/index.html",
    "href": "posts/01_adaptive_regression/index.html",
    "title": "Adaptive Regression",
    "section": "",
    "text": "Adapting path through mountains! Photo by Z√ºlf√º Demirüì∏\n\n\n\n\nIntroduction\nHere I am trying to express our logic to find such Observation. Lets dive in.\nThere are different value estimation technique like regression analysis and time-series analysis. Everyone of us has experimented on regression using OLS ,MLE, Ridge, LASSO, Robust etc., and also might have evaluated them using RMSE (Root Mean/Median Square Error), MAD (Mean/Median Absolute Deviation), MAE (Mean / Median Absolute Error) and MAPE (Mean/Median Absolute Percentage Error), etc‚Ä¶\nBut all of these gives a single point estimate that what is the overall error looks like. Just a different thought!! can we be sure that this single value of MAPE or MAE? How easy it is to infer that our trained model has fitted well across the distribution of dependent variable?\n\n\n\n\n\nPlot of Anscombe‚Äôs Quartet\n\n\n\n\n\nSome Descriptive Stats for Anscombe‚Äôs Quartet\n\n\nLet me give you a pretty small data-set to play with ‚ÄúThe Anscombe‚Äôs quartet‚Äù. This is a very famous data-set by Francis Anscombe. Please refer the plots below to understand the distribution of y1, y2, y3, y4. Isn‚Äôt it different?\nWould the measure of central tendency and disportion be same for this data? I am sure none of us would believe but to our utter surprise we see all the descriptive stats are kind of same. Don‚Äôt believe me !!! Please see the results below ( Source: Wikipedia ):\n\n\nSo what we do Now!\nAstonished !!! Don‚Äôt be. This is what has been hiding behind those numbers. And this is why we really won‚Äôt be able to cross certain performance level. Unless you change some features or even do a lot of hyper parameter tuning, your results won‚Äôt vary much.\nIf you look at the average value of MAPE in each decile you would see an interesting pattern. Let us show you what we see that pattern. One day while working on a business problem where I was using regression on a discussion with Kumarjit, we deviced a different way of model diagnosis. We worked together to give this a shape and build on it.\n\nAs you can see it is absolutely evident that either of the side in the distribution of MAPE values is going wild!!!!!!! Still overall MAPE is good (18%).\n\n\n\nSeeking Scope of Improvement\nWe worked together to build a different framework to address such issues on the go and reduce the MAPE deterioration on the edge of the distribution.\nThis problems gives rise to a concept we named as Distribution Assertive Regression (DAR).\nDAR is a framework that is based on cancelling the weakness of one point summaries by using the classical concepts of Reliability Engineering : The Bath Tub Curve.\n\n\n\n\n\nPlot for Classical Bath Tub Curve using a Hazard Function\n\n\nThe Specialty of this curve is that it gives you the likelihood which areas one tends to have high failure rates. In our experiments when we replace failure with MAPE value and the Time with sorted (ascending) value of target / dependent variable, we observe the same phenomenon. This is likely to happen because most of regression techniques assumes Normal (Gaussian) Distribution of data and fits itself towards the central tendency of this distribution.\nBecause of this tendency, any regression methods tends to learn less about data which are away from the central tendency of the target.\nLets look at BostonHousing data from ‚Äúmlbench‚Äù package in R.\n\n\n\nPlot for MAPE Bath Tub Curve for Decile Split ‚Äúmdev‚Äù from Data\n\n\nHere the MAPE is calculated for each decile split of ordered target variable. As you can observe it is following the bath tub curve. Hence the validates our hypothesis that the regression method is not able to understand much about the data at the either ends of the distribution.\n\n\nFinal Analysis\nNow the DAR framework essentially fixes this weakness of regression method and understands the behavior of data which is stable and can be tweak in a fashion that can be use in general practice.\nPlot of MAPE Bath Tub Curve after applying DAR Framework for Decile Split ‚Äúmdev‚Äù from Data\n\nHow this framework with same method reduced MAPEs so much and made model much more stable‚Ä¶?? Well here it is:\nThe DAR framework splits the data at either ends of the order target variable and performs regression on these ‚Äúsplit‚Äù data individually. This inherently reduces the so called ‚Äúnoise‚Äù part of the data and treat it as an individual data.\n\n\n\nScoring on New Data\nNow you might be thinking while applying regression this sounds good but how will one score this on new data. Well to answer that we used our most simple yet very effective friend ‚ÄúKNN‚Äù (Though any multiclass Classifier can be used here). So ideally scoring involves two step method :\n\nScore new value against each KNN / Multiclass Classifier model of the data\nBased on closeness we score it with the regression method used for that part of data.\n\nSo now we know how we can improve the prediction power of data for regression.\n\n\n\nCode and Flowchart\nIf things are simple lets keep it simple. Refer flowchart and code below for implementation of this framework. Paper here!\n\nR codePython codeHere is the Flow Chart\n\n\n\n\n\n\n\n\nTipClick to Expand\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipClick to Expand\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngraph TB\n    \n    subgraph Testing\n        p1(Finding bucket of model to choose)\n        p1 --&gt; p2([Making predictions &lt;br&gt; based on selected model for inference])\n        p2 --&gt; p3(Consolidate final score of prediction)\n    end\n\n    subgraph Training\n        md([Fitting a &lt;br&gt;Regression model])==&gt; di\n        di{Binning Data via &lt;br/&gt; evaluating Distribution &lt;br/&gt; MAPE values }\n        di --&gt; md2([Fitting a Buckteing model &lt;br/&gt; to Binned MAPE Buckets])\n        md2 --&gt; md3([Fitting Regression &lt;br&gt; Models on Binned Data])\n        md == Keeping main&lt;br/&gt;model ==&gt; ro        \n        md3 ==&gt; ro(Final Models &lt;br&gt; Binning Data Models + &lt;br&gt; Set of Regressoin Models)\n    end\n\n    \n    od([Data Input]) -- Training&lt;br&gt; Data--&gt; md\n    od -- Testing&lt;br&gt; Data--&gt; p1\n    ro -.-&gt; p1\n    ro -.-&gt; p2\n\n    classDef green fill:#9f6,stroke:#333,stroke-width:2px;\n    classDef yellow fill:#ff6,stroke:#333,stroke-width:2px;\n    classDef blue fill:#00f,stroke:#333,stroke-width:2px,color:#fff;\n    classDef orange fill:#f96,stroke:#333,stroke-width:4px;\n    class md,md2,md3 green\n    class di orange\n    class p1,p2 yellow\n    class ro,p3 blue\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jitin Kapila",
    "section": "",
    "text": "I solve complex AI and data science problems by finding patterns others miss across industries.\nSpecializing in AI strategy, machine learning implementation, and GenAI solutions that transform business outcomes through cross-industry pattern recognition."
  },
  {
    "objectID": "index.html#ai-strategy-data-science-expertise",
    "href": "index.html#ai-strategy-data-science-expertise",
    "title": "Jitin Kapila",
    "section": "AI Strategy & Data Science Expertise",
    "text": "AI Strategy & Data Science Expertise\nIn machine learning and AI strategy, there‚Äôs never just one solution to a problem. Some approaches are simply more elegant than others.\nThis drives how I approach complex data science challenges. After 14 years implementing AI solutions across automotive, retail, insurance, FMCG, and telecommunications, I‚Äôve learned that breakthrough ML insights often come when you apply data science methodologies from completely different industries."
  },
  {
    "objectID": "index.html#real-world-ai-implementation",
    "href": "index.html#real-world-ai-implementation",
    "title": "Jitin Kapila",
    "section": "Real-World AI Implementation",
    "text": "Real-World AI Implementation\nWhen I was leading an AI strategy project for automotive inventory optimization, traditional machine learning approaches weren‚Äôt delivering results. The data patterns were complex, requiring advanced analytics and GenAI techniques. Then I applied healthcare data science insights to the automotive challenge.\nThe AI solution became clear. We eliminated 45,000 excess units through end-to-end ML implementation. Not because I knew more about automotive than the experts, but because I applied cross-industry AI strategy thinking."
  },
  {
    "objectID": "index.html#what-i-do",
    "href": "index.html#what-i-do",
    "title": "Jitin Kapila",
    "section": "What I Do",
    "text": "What I Do\nI work with Fortune 500 companies implementing comprehensive AI strategy and data science solutions. My expertise spans machine learning architecture, GenAI implementation, predictive analytics, and end-to-end AI transformation.\nCurrently serving as Acting CTO across multiple accounts, developing AI strategies that deliver measurable business impact. I also mentor 25+ AI and data science professionals in advanced machine learning techniques and strategic AI thinking.\nMy background combines mechanical engineering, applied mathematics, and 14+ years of hands-on AI/ML implementation. The analytical frameworks that work for complex data science problems often apply beautifully to AI strategy across different business domains."
  },
  {
    "objectID": "index.html#ai-machine-learning-philosophy",
    "href": "index.html#ai-machine-learning-philosophy",
    "title": "Jitin Kapila",
    "section": "AI & Machine Learning Philosophy",
    "text": "AI & Machine Learning Philosophy\nPattern recognition across domains. Root cause analysis through data science. Elegant AI solutions over brute force machine learning approaches.\nWhen you‚Äôve solved similar ML challenges in healthcare, the path through retail AI becomes visible. When you understand the mathematics of one optimization problem, you start seeing the same data science structures everywhere.\nIt‚Äôs not about being the smartest AI expert in the room. It‚Äôs about being the data scientist who‚Äôs seen this pattern before, just wearing different industry clothes.\n\nWant to discuss your AI strategy or data science challenges?\nGet in touch"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I‚Äôve always been drawn to patterns. As a kid, I‚Äôd spend hours figuring out how things worked - taking apart radios, solving puzzles, trying to understand the underlying logic of systems.\nMy background is mechanical engineering and applied mathematics. But somewhere along the way, I discovered that the same analytical frameworks that work for understanding physical systems often apply beautifully to business problems."
  },
  {
    "objectID": "about.html#how-i-think",
    "href": "about.html#how-i-think",
    "title": "About",
    "section": "",
    "text": "I‚Äôve always been drawn to patterns. As a kid, I‚Äôd spend hours figuring out how things worked - taking apart radios, solving puzzles, trying to understand the underlying logic of systems.\nMy background is mechanical engineering and applied mathematics. But somewhere along the way, I discovered that the same analytical frameworks that work for understanding physical systems often apply beautifully to business problems."
  },
  {
    "objectID": "about.html#the-journey",
    "href": "about.html#the-journey",
    "title": "About",
    "section": "The Journey",
    "text": "The Journey\nFourteen years ago, I started working with data in the automotive industry. Traditional approaches weren‚Äôt cutting it for the problems we faced. So I began looking at business challenges the way I‚Äôd approach a mathematical proof - breaking them down, finding the underlying patterns, testing different approaches until something elegant emerged.\nThe breakthrough moment came when I realized that solutions from completely different industries could unlock problems that seemed intractable. Healthcare insights solving automotive challenges. Retail strategies transforming manufacturing operations. It sounds obvious now, but most experts stay within their domain expertise."
  },
  {
    "objectID": "about.html#what-drives-me",
    "href": "about.html#what-drives-me",
    "title": "About",
    "section": "What Drives Me",
    "text": "What Drives Me\nThere‚Äôs something deeply satisfying about finding the root cause of a complex problem. Even more satisfying when the solution is mathematically elegant - when all the pieces click into place and you realize there was a simpler way all along.\n‚ÄúIn mathematics, there‚Äôs never just one solution to a problem. Some roots are simply more beautiful than others.‚Äù\nThis philosophy extends beyond work. Whether I‚Äôm learning a new piece on guitar or exploring some remote mountain trail, I‚Äôm always looking for patterns, connections, the underlying structure that makes things work."
  },
  {
    "objectID": "about.html#current-focus",
    "href": "about.html#current-focus",
    "title": "About",
    "section": "Current Focus",
    "text": "Current Focus\nThese days, I work with companies across multiple industries simultaneously. It keeps the pattern recognition sharp - jumping from automotive inventory problems to retail marketing optimization to insurance claims analytics. Each domain informs the others.\nI also spend time mentoring other professionals who want to develop this kind of cross-industry thinking. Teaching someone to see patterns they couldn‚Äôt see before never gets old."
  },
  {
    "objectID": "about.html#the-personal-side",
    "href": "about.html#the-personal-side",
    "title": "About",
    "section": "The Personal Side",
    "text": "The Personal Side\nWhen I‚Äôm not solving problems, you‚Äôll find me playing guitar (badly but enthusiastically) or planning the next off-the-beaten-path adventure. There‚Äôs something about getting lost in unfamiliar places that keeps the mind flexible.\nI believe in approaching challenges with curiosity rather than assumptions. Sometimes the most interesting solutions come from the most unexpected directions.\n\nWant to discuss a problem you‚Äôre working on? Get in touch"
  },
  {
    "objectID": "insights.html",
    "href": "insights.html",
    "title": "Insights",
    "section": "",
    "text": "Examples of how cross-industry pattern recognition transforms complex challenges into elegant solutions."
  },
  {
    "objectID": "insights.html#mathematical-thinking-applied-to-business-problems",
    "href": "insights.html#mathematical-thinking-applied-to-business-problems",
    "title": "Insights",
    "section": "",
    "text": "Examples of how cross-industry pattern recognition transforms complex challenges into elegant solutions."
  },
  {
    "objectID": "posts/02_hypothesis_test/index.html",
    "href": "posts/02_hypothesis_test/index.html",
    "title": "A flow to Test Your Hypothesis in Python",
    "section": "",
    "text": "Hypothesis testing Photo by Tara Winstead\n\n\n\nOverview\nAll the practitioners of data science always hit one giant thing to do with data and you know it well its EDA -Exploratory Data Analysis. This word EDA1 was coined by Tukey himself in his seminal book published in 1983. But do you think that before that EDA doesn‚Äôt existed ?\n1¬†Emerson, J. D., & Hoaglin, D. C. (1983). Stem-and-leaf displays. In D. C. Hoaglin, F. Mosteller, & J. W. Tukey (Eds.) Understanding Robust and Exploratory Data Analysis, pp.¬†7‚Äì32. New York: Wiley. Book is here.Well glad you thought. Before that all were doing what is called as Hypothesis Testing. Yes, before this the race was majorly to fit the data and make most unbiased and robust estimate. But remember one thing when you talk about Hypothesis Testing it was always and majorly would be related to RCTs (Randomized Controlled Trials) a.k.a Randomized Clinical Trials and is Gold Standard of data.\n\n\n\n\n\n\nTipMore on RCTs and ODs\n\n\n\n\n\nNow let me now not hijack the discussion to what is RCTs and Observational Data (ODs) as it is more of Philosophical Reasoning rather than other quality of data, but essentially what we are trying to find is that can we by, using stats, identify interesting patterns in data.\nThe only thing happens wit RCT data is that we tend to believe these interesting patterns coincide with some sort of ‚ÄòCause-Effect‚Äô kind of relationship. But essentially due to bia nature of ODs, we certainly cant conclude this. And hence, can only find interesting patterns.\n\n\n\nLets move on. The big question is, for whatever reason you are doing HT , you are doing it for finding something intreating. And that something interesting is usually found by using Post-Hoc Tests. Now there are variety of Post-Hocs available but what is more know and hence easily found to be implemented in Tukey‚Äôs HSD.\nSo lets directly jump to how to follow this procedure. We‚Äôll be using bioinfokit for this, as it is much simpler wrapper around whats implemented in statsmodels.\n\n\nWhat are the results\nPheww‚Ä¶ Thats too much code right. But that would save a lot of your time in real life. So in real life you would write code as 3 steps below:\n\n\nCode\n# import libraries\nimport pandas as pd\n\n# Getting car data from UCI\ndf = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data',\n                 sep='\\s+',header=None,\n                 names=['mpg','cylinders','displacement','horsepower','weight',\n                 'acceleration','model_year','origin','car_name'])\ndf.head()\n\n# Syntax to do anove with validating the assumption, doing test and a post-hoc\nresults = do_anova_test(df=df, res_var='mpg',xfac_var='cylinders', \n                        anova_model='mpg ~ C(cylinders)+C(origin)+C(cylinders):C(origin)',\n                        ss_typ=3, result_full=True)\n\n\nResults form the do_anova_test\nLevens Test Result:\n                 Parameter    Value\n0      Test statistics (W)  14.5856\n1  Degrees of freedom (Df)   4.0000\n2                  p value   0.0000\n\nBartletts Test Result:\n                 Parameter    Value\n0      Test statistics (T)  61.2143\n1  Degrees of freedom (Df)   4.0000\n2                  p value   0.0000\n\nANOVA\\ANCOVA Test Result:\n                           df     sum_sq    mean_sq         F  PR(&gt;F)      n2\nIntercept                 1.0  6195.1701  6195.1701  296.3452  0.0000  0.2727\nC(cylinders)              4.0  7574.5864  1893.6466   90.5824  0.0000  0.3334\nC(origin)                 2.0   241.0703   120.5351    5.7658  0.0034  0.0106\nC(cylinders):C(origin)    8.0   577.4821    72.1853    3.4530  0.0046  0.0254\nResidual                389.0  8132.1404    20.9052       NaN     NaN     NaN\n\nTukey HSD Result:\n   group1  group2     Diff    Lower    Upper  q-value  p-value\n0       8       4  14.3237  12.8090  15.8383  36.6527   0.0010\n1       8       6   5.0226   3.1804   6.8648  10.5671   0.0010\n2       8       3   5.5869  -0.7990  11.9728   3.3909   0.1183\n3       8       5  12.4036   5.0643  19.7428   6.5503   0.0010\n4       4       6   9.3011   7.6765  10.9256  22.1910   0.0010\n5       4       3   8.7368   2.4102  15.0633   5.3524   0.0017\n6       4       5   1.9201  -5.3676   9.2078   1.0212   0.9000\n7       6       3   0.5643  -5.8486   6.9772   0.3410   0.9000\n8       6       5   7.3810   0.0182  14.7437   3.8854   0.0491\n9       3       5   6.8167  -2.7539  16.3873   2.7606   0.2919\nNice!!!\n\nAnd plotting is even easier\n\n\nCode\n# Numbers are clumsy for most. Making more interpretable plot on above results.\nplot_hsd(results.tukeyhsd.sort_values('Diff'), title=\"Tukey HSD resutls Anova of MPG ~ Cylinder\")\n\n\nResults form the plot_hsd\n\n\n\nTukey‚Äôs HSD comparison based on Anova Results\n\n\nPlots look good with ‚Äòp-values‚Äô.\n\n\nConclusion\nNow since we applied the above to a Non RCT we cannot conclude that Difference in mpg based on cylinder is huge specially as number of cylinders goes up. But this statement might not be as explicit as might be appearing from plot. Unless you have a strong believe that the data follows with rules and assumptions of RCTs, we should be only seeking interesting as in associated results and not cause-effect results.\n\n\nGive me ‚ÄúThe Code‚Äù\n\nPerforming AnovaPlotting Results\n\n\n\n\nAnova Test anova_test.py\nfrom bioinfokit import analys\n\nimport numpy as np\nfrom scipy import stats\n\nclass KeyResults:\n    \"\"\"\n    A basic class to hold all the results\n    \"\"\"\n    \n    def __init__(self,result_full):\n        self.keys = []\n        self.result_full = result_full\n    \n    def add_result(self,name,result):\n        if name == 'tukeyhsd':\n            self.keys.append(name)\n            setattr(self, name, result)\n        elif self.result_full:\n            self.keys.append(name)\n            setattr(self, name, result)\n\n\n# Anova test code\ndef do_anova_test(df, res_var, xfac_var, anova_model,ss_typ=3,\n                  effectsize='n2',result_full=False,add_res=False):\n    \"\"\"\n    Do all sequential anova tests\n    \n    Step 1) Leven's/ bartellet test for checking weather variance is homogenous or not\n    Step 2) Main ANOVA/ANCOVA test\n    Step 3) Tukey's HSD for individual combinations\n    \n    :param df: Pandas DataFrame holding all the columns\n    :param res_var: Variable for which we are checking ANOVA\n    :param xfac_var: Grouping Variables for which we want to do the comparisons\n    :param anova_model: SM formula for the model. This is life savour to make all things work\n    :param result_full: To provide all the results of intermediate steps\n    \n    \"\"\"\n\n    results = KeyResults(result_full)\n    \n    # initialize stat method\n    res = analys.stat()\n    \n    # doing levens test\n    res.levene(df=df, res_var=res_var,xfac_var=xfac_var)\n    print('\\nLeven\\'s Test Result:')\n    print(res.levene_summary)\n    results.add_result('levene',res.levene_summary)\n\n    # doing bartlett test\n    res.bartlett(df=df, res_var=res_var,xfac_var=xfac_var)\n    print('\\nBartlett\\'s Test Result:')\n    print(res.bartlett_summary)\n    results.add_result('bartlett',res.bartlett_summary)\n    \n    # doing anova / ancova\n    res.anova_stat(df=df, res_var=res_var, anova_model=anova_model,ss_typ=ss_typ)\n    aov_res = res.anova_summary\n    \n    # Add effect sizes\n    if effectsize == \"n2\":\n        all_effsize = (aov_res['sum_sq'] / aov_res['sum_sq'].sum()).to_numpy()\n        all_effsize[-1] = np.nan\n    else:\n        ss_resid = aov_res['sum_sq'].iloc[-1]\n        all_effsize = aov_res['sum_sq'].apply(lambda x: x / (x + ss_resid)).to_numpy()\n        all_effsize[-1] = np.nan\n    aov_res[effectsize] = all_effsize\n    #aov_res['bw_'] = res.anova_model_out.params.iloc[-1]\n    aov_res = aov_res.round(4)\n    \n    # printing results\n    print('\\nANOVA\\ANCOVA Test Result:')\n    print(aov_res)\n    results.add_result('anova',res.anova_summary.round(4))\n    results.add_result('anova_model',res.anova_model_out)\n    \n    # doing tukey's hsd top compare the groups\n    res.tukey_hsd(df=df, res_var=res_var,xfac_var=xfac_var, anova_model=anova_model,ss_typ=ss_typ)\n    print('\\nTukey HSD Result:')\n    print(res.tukey_summary.round(4))\n    results.add_result('tukeyhsd',res.tukey_summary.round(4))\n    \n    # add all result componets again if needed \n    if add_res:\n        results.add_result('allresult',res)\n    \n    return results\n\n\n\n\n\n\nPlotting results plot_hsd.py\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.style.use('seaborn-bright')\n\ndef plot_hsd(hsdres,p_cutoff=0.05,title=None,ax=None,figsize=(10,7)):\n     \"\"\"\n     Do plotting of tukeyhsd results\n    \n  \n    :param hsdres: 'tukeyhsd' result form the do_anova_test function\n    :param p_cutoff: Cutoff at which we get say a combination is significant\n    :param title: Title of the plot\n    :param ax: Define or get the matplotlib axes\n    :param figsize: Mention Figure size to draw\n    \n    \"\"\"\n\n    if ax is None:\n        fig,axp = plt.subplots(figsize=figsize)\n    else:\n        axp = ax\n    \n    # helper func\n    p_ind = lambda x : '' if x &gt; 0.1 else ('+' if x &gt; 0.05 else ('*' if x &gt; 0.01 else ('**' if x &gt;0.001 else '***')))\n    label_gen  = lambda x: f\"${x[0]} - {x[1]}\\ |\\ p:{x[2]:0.2f}{p_ind(x[2]):5s}$\"\n    \n    #setting values\n    mask = hsdres['p-value'] &lt;= p_cutoff\n    yticklabs = hsdres[['group1','group2','p-value']].apply(label_gen,axis=1).values\n    ys = np.arange(len(hsdres))\n    \n    # adding plot to axes\n    axp.errorbar(hsdres[~mask]['Diff'],ys[~mask],xerr=np.abs(hsdres[~mask][['Lower',\"Upper\"]]).values.T,\n                fmt='o', color='black', ecolor='lightgray', elinewidth=2, capsize=0)\n    axp.errorbar(hsdres[mask]['Diff'],ys[mask],xerr=np.abs(hsdres[mask][['Lower',\"Upper\"]]).values.T,\n                fmt='o', color='red', ecolor='pink', elinewidth=2, capsize=5)\n    axp.axvline(x=0,linestyle='--',c='skyblue')\n    axp.set_yticks([])\n    (l,u) = axp.get_xlim()\n    axp.set_xlim(l+1.5*l,u)\n    (l,u) = axp.get_xlim()\n    for idx,labs in enumerate(yticklabs):\n        axp.text(l-0.1*l,ys[idx],labs)\n    axp.set_yticklabels([])\n    \n    # finally doing what is needed\n    if ax is None:\n        plt.title('' if title is None else title,fontsize=14)\n        plt.show()\n    else:\n        return axp\n\n\n\n\n\nHope this give you kickstart to find you intresting patterns. Happy Learning!\n\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "posts/04_crosstab_sparsity_classification/index.html",
    "href": "posts/04_crosstab_sparsity_classification/index.html",
    "title": "CrossTab Sparsity for Classification",
    "section": "",
    "text": "Cross Roads where everyone meets!\n\n\n\nIntroduction: A Journey into Data\nPicture this: you‚Äôre standing on the icy shores of Antarctica, the wind whipping around you as you watch a colony of Palmer Penguins waddling about, oblivious to the data detective work you‚Äôre about to embark on. As a data science architect, you‚Äôre not just an observer; you‚Äôre a sleuth armed with algorithms and insights, ready to unravel the mysteries hidden within data. Today, we‚Äôll transform raw numbers into powerful narratives using CrossTab Sparsity as our guiding compass. This blog post will demonstrate how this metric can revolutionize classification tasks, shedding light on many fascinating datasets‚Äîthe charming Palmer Penguins and the serious Obesity, Credit cards data and many more.\n\n\nThe Power of CrossTab Sparsity\n\nWhat is CrossTab Sparsity?\nCrossTab Sparsity isn‚Äôt just a fancy term that sounds good at dinner parties; it‚Äôs a statistical measure that helps us peer into the intricate relationships between categorical variables. Imagine it as a magnifying glass that reveals how different categories interact within a contingency table. Understanding these interactions is crucial in classification tasks, where the right features can make or break your model (and your day).\nWhy Does It Matter?\nIn the world of data science, especially in classification, selecting relevant features is like picking the right ingredients for a gourmet meal‚Äîget it wrong, and you might end up with something unpalatable. CrossTab Sparsity helps us achieve this by:\n\nHighlighting Relationships: It‚Äôs like having a friend who always points out when two people are meant to be together‚Äîunderstanding how features interact with the target variable.\nStreamlining Models: Reducing complexity by focusing on significant features means less time spent untangling spaghetti code.\nEnhancing Interpretability: Making models easier to understand and explain to stakeholders is like translating tech jargon into plain English‚Äîeveryone appreciates that!\n\n\n\n\nData Overview: Our Data People at work here\n\nThe Datasets\nData 1: Estimation of Obesity Levels Based On Eating Habits and Physical Condition\nLittle bit about the data: This dataset, shared on 8/26/2019, looks at obesity levels in people from Mexico, Peru, and Colombia based on their eating habits and physical health. It includes 2,111 records with 16 features, and classifies individuals into different obesity levels, from insufficient weight to obesity type III. Most of the data (77%) was created using a tool, while the rest (23%) was collected directly from users online.\nData 2: Predict Students‚Äô Dropout and Academic Success\nLittle bit about the data: This dataset, shared on 12/12/2021, looks at factors like students‚Äô backgrounds, academic path, and socio-economic status to predict whether they‚Äôll drop out or succeed in their studies. With 4,424 records across 36 features, it covers students from different undergrad programs. The goal is to use machine learning to spot at-risk students early, so schools can offer support. The data has been cleaned and doesn‚Äôt have any missing values. It‚Äôs a classification task with three outcomes: dropout, still enrolled, or graduated\nKey Features:\n\nMulticlass: Both data set cater a multi class problems with NObeyesdad and Target columns\nMixed Data Type: A good mix of categorical and continuous variables are available for usage.\nSizeable: More than 2 K rows are available for testing.\n\n\n\n\nExploratory Data Analysis (EDA): Setting the Stage\nBefore we dive into model creation, let‚Äôs explore our dataset through some quick EDA. Think of this as getting to know your non-obese friends before inviting them to a party.\n\nEDA for Obesity Data\nHere‚Äôs a brief code snippet to perform essential EDA on the Obesity dataset:\n\n\nLoading data and generating basic descriptive\n# Load the Obesity data\nraw_df = pd.read_csv('ObesityDataSet_raw_and_data_sinthetic.csv')\ntarget = 'NObeyesdad'\n\n# Load Students data\n\n# Load Credit data\n# raw_data = sm.datasets.get_rdataset(\"credit_data\",'modeldata')\n# raw_df = raw_data.data\n# target = 'Status'\n\n# # Load Palmer penguins data\n# raw_data = sm.datasets.get_rdataset(\"penguins\",'palmerpenguins')\n# raw_df = raw_data.data\n# target = 'species'\n\n\n# # Load Credit data\n# raw_data = sm.datasets.get_rdataset(\"CreditCard\",'AER')\n# raw_df = raw_data.data\n# target = 'card'\n\n\n# setting things up for aal the next steps\nraw_df[target] = raw_df[target].astype('category') \nprint('No of data points available to work:',raw_df.shape)\ndisplay(raw_df.head())\n\n\n# Summary statistics\ndisplay(raw_df.describe())\n\n\nNo of data points available to work: (2111, 17)\n\n\n\n\n\n\n\n\n\nGender\nAge\nHeight\nWeight\nFamil_Hist_Owt\nFAVC\nFCVC\nNCP\nCAEC\nSMOKE\nCH2O\nSCC\nFAF\nTUE\nCALC\nMTRANS\nNObeyesdad\n\n\n\n\n0\nFemale\n21.0\n1.62\n64.0\nyes\nno\n2.0\n3.0\nSometimes\nno\n2.0\nno\n0.0\n1.0\nno\nPublic_Transportation\nNormal_Weight\n\n\n1\nFemale\n21.0\n1.52\n56.0\nyes\nno\n3.0\n3.0\nSometimes\nyes\n3.0\nyes\n3.0\n0.0\nSometimes\nPublic_Transportation\nNormal_Weight\n\n\n2\nMale\n23.0\n1.80\n77.0\nyes\nno\n2.0\n3.0\nSometimes\nno\n2.0\nno\n2.0\n1.0\nFrequently\nPublic_Transportation\nNormal_Weight\n\n\n3\nMale\n27.0\n1.80\n87.0\nno\nno\n3.0\n3.0\nSometimes\nno\n2.0\nno\n2.0\n0.0\nFrequently\nWalking\nOverweight_Level_I\n\n\n4\nMale\n22.0\n1.78\n89.8\nno\nno\n2.0\n1.0\nSometimes\nno\n2.0\nno\n0.0\n0.0\nSometimes\nPublic_Transportation\nOverweight_Level_II\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAge\nHeight\nWeight\nFCVC\nNCP\nCH2O\nFAF\nTUE\n\n\n\n\ncount\n2111.000000\n2111.000000\n2111.000000\n2111.000000\n2111.000000\n2111.000000\n2111.000000\n2111.000000\n\n\nmean\n24.312600\n1.701677\n86.586058\n2.419043\n2.685628\n2.008011\n1.010298\n0.657866\n\n\nstd\n6.345968\n0.093305\n26.191172\n0.533927\n0.778039\n0.612953\n0.850592\n0.608927\n\n\nmin\n14.000000\n1.450000\n39.000000\n1.000000\n1.000000\n1.000000\n0.000000\n0.000000\n\n\n25%\n19.947192\n1.630000\n65.473343\n2.000000\n2.658738\n1.584812\n0.124505\n0.000000\n\n\n50%\n22.777890\n1.700499\n83.000000\n2.385502\n3.000000\n2.000000\n1.000000\n0.625350\n\n\n75%\n26.000000\n1.768464\n107.430682\n3.000000\n3.000000\n2.477420\n1.666678\n1.000000\n\n\nmax\n61.000000\n1.980000\n173.000000\n3.000000\n4.000000\n3.000000\n3.000000\n2.000000\n\n\n\n\n\n\n\n\n\nTarget distribution\n\n\nTarget and Correlation\n# Visualize target data distribution\nplt.figure(figsize=(4, 3))\nsns.countplot(data=raw_df, x=target, hue=target, palette='Set2',)\nplt.title(f'Distribution of {target} levels')\nplt.xticks(rotation=45)\nplt.show()\n\n# Heatmap to check for correlations between numeric variables\ncorr = raw_df.corr('kendall',numeric_only=True)\nsns.heatmap(corr, annot=True, cmap='coolwarm')\nplt.title('Kendall Correlation Heatmap')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoneSome Mode EDA for the data\n\n\n\n\n\n\n\nEDA code\n# Visualize the distribution of numerical variables\nsns.pairplot(raw_df, hue=target, corner=True)\nplt.show()\n\n\n\n\n# Gettign Categorical data\ncategorical_columns = raw_df.select_dtypes(include='object').columns\n\n# Plot categorical variables with respect to the target variable\nfor col in categorical_columns:\n    plt.figure(figsize=(12, 5))\n    sns.countplot(data=raw_df,x=col, hue=target)\n    plt.title(f\"Countplot of {col} with respect to {target}\")\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Creation: Establishing a Baseline\nWith our exploratory analysis complete, we‚Äôre ready to create our baseline model using logistic regression with Statsmodels. This initial model will serve as our reference point‚Äîlike setting up a benchmark for your favorite video game.\n\n\nSplitting data and training a default Multinomila Logit model on our data\ndata_df = raw_df.dropna().reset_index(drop=True)\ndata_df[target] = data_df[target].cat.codes\n# X = data_df[['bill_length_mm','bill_depth_mm','flipper_length_mm','body_mass_g']] \n\ndata_df_test = data_df.sample(frac=0.1,random_state=3)\ndata_df_train = data_df.drop(data_df_test.index)\n\n# Using MN logistic regression model using formula API\n# This would essentially bold down to pair wise logsitic regression\nlogit_model = sm.MNLogit.from_formula(\n    f\"{target} ~ {' + '.join([col for col in data_df_train.columns if col != target])}\", \n    data=data_df_train\n).fit_regularized()\n\n\nOptimization terminated successfully    (Exit mode 0)\n            Current function value: 0.17113347578942742\n            Iterations: 489\n            Function evaluations: 670\n            Gradient evaluations: 489\n\n\n\n\n\n\n\n\n\n\nNoneBase model summary for geeks\n\n\n\n\n\n\n\nDisplay summary\ndisplay(logit_model.summary())\n\n\n\nMNLogit Regression Results\n\n\nDep. Variable:\nNObeyesdad\nNo. Observations:\n1900\n\n\nModel:\nMNLogit\nDf Residuals:\n1756\n\n\nMethod:\nMLE\nDf Model:\n138\n\n\nDate:\nThu, 27 Feb 2025\nPseudo R-squ.:\n0.9119\n\n\nTime:\n03:18:53\nLog-Likelihood:\n-325.15\n\n\nconverged:\nTrue\nLL-Null:\n-3691.8\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n0.000\n\n\n\n\n\n\n\n\nNObeyesdad=1\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-11.3439\n3.45e+05\n-3.29e-05\n1.000\n-6.76e+05\n6.76e+05\n\n\nGender[T.Male]\n-3.4606\n0.819\n-4.224\n0.000\n-5.066\n-1.855\n\n\nFamil_Hist_Owt[T.yes]\n-0.8874\n0.658\n-1.349\n0.177\n-2.177\n0.402\n\n\nFAVC[T.yes]\n0.2631\n0.782\n0.337\n0.736\n-1.269\n1.795\n\n\nCAEC[T.Frequently]\n-8.2219\n2.342\n-3.511\n0.000\n-12.811\n-3.632\n\n\nCAEC[T.Sometimes]\n-6.2475\n2.265\n-2.758\n0.006\n-10.687\n-1.808\n\n\nCAEC[T.no]\n-8.6341\n2.916\n-2.961\n0.003\n-14.349\n-2.919\n\n\nSMOKE[T.yes]\n4.5048\n3.105\n1.451\n0.147\n-1.582\n10.591\n\n\nSCC[T.yes]\n-0.7063\n1.458\n-0.484\n0.628\n-3.565\n2.152\n\n\nCALC[T.Frequently]\n-12.6173\n3.45e+05\n-3.66e-05\n1.000\n-6.76e+05\n6.76e+05\n\n\nCALC[T.Sometimes]\n-13.3244\n3.45e+05\n-3.86e-05\n1.000\n-6.76e+05\n6.76e+05\n\n\nCALC[T.no]\n-14.1980\n3.45e+05\n-4.12e-05\n1.000\n-6.76e+05\n6.76e+05\n\n\nMTRANS[T.Bike]\n15.8821\n2529.381\n0.006\n0.995\n-4941.614\n4973.378\n\n\nMTRANS[T.Motorbike]\n4.0050\n47.345\n0.085\n0.933\n-88.790\n96.800\n\n\nMTRANS[T.Public_Transportation]\n4.5116\n1.001\n4.505\n0.000\n2.549\n6.474\n\n\nMTRANS[T.Walking]\n4.3989\n1.507\n2.918\n0.004\n1.445\n7.353\n\n\nAge\n0.3779\n0.098\n3.858\n0.000\n0.186\n0.570\n\n\nHeight\n-14.4182\n4.123\n-3.497\n0.000\n-22.499\n-6.338\n\n\nWeight\n1.0784\n0.146\n7.384\n0.000\n0.792\n1.365\n\n\nFCVC\n-0.7676\n0.428\n-1.793\n0.073\n-1.607\n0.072\n\n\nNCP\n-1.7199\n0.489\n-3.516\n0.000\n-2.679\n-0.761\n\n\nCH2O\n-1.7255\n0.578\n-2.985\n0.003\n-2.859\n-0.592\n\n\nFAF\n-0.1753\n0.281\n-0.624\n0.533\n-0.726\n0.375\n\n\nTUE\n-0.9735\n0.458\n-2.124\n0.034\n-1.872\n-0.075\n\n\nNObeyesdad=2\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n17.3832\nnan\nnan\nnan\nnan\nnan\n\n\nGender[T.Male]\n-13.9964\n1.976\n-7.083\n0.000\n-17.869\n-10.123\n\n\nFamil_Hist_Owt[T.yes]\n2.0850\n1.721\n1.212\n0.226\n-1.288\n5.458\n\n\nFAVC[T.yes]\n1.0223\n1.765\n0.579\n0.562\n-2.437\n4.482\n\n\nCAEC[T.Frequently]\n-10.0658\n4.392\n-2.292\n0.022\n-18.674\n-1.458\n\n\nCAEC[T.Sometimes]\n-1.0233\n3.443\n-0.297\n0.766\n-7.771\n5.724\n\n\nCAEC[T.no]\n-0.4821\n977.119\n-0.000\n1.000\n-1915.601\n1914.637\n\n\nSMOKE[T.yes]\n8.1449\n4.011\n2.030\n0.042\n0.283\n16.007\n\n\nSCC[T.yes]\n-7.6939\n155.443\n-0.049\n0.961\n-312.356\n296.968\n\n\nCALC[T.Frequently]\n-2.4712\nnan\nnan\nnan\nnan\nnan\n\n\nCALC[T.Sometimes]\n-7.5357\nnan\nnan\nnan\nnan\nnan\n\n\nCALC[T.no]\n-7.2634\nnan\nnan\nnan\nnan\nnan\n\n\nMTRANS[T.Bike]\n-11.9360\n1.16e+08\n-1.03e-07\n1.000\n-2.27e+08\n2.27e+08\n\n\nMTRANS[T.Motorbike]\n10.9302\n48.258\n0.226\n0.821\n-83.653\n105.513\n\n\nMTRANS[T.Public_Transportation]\n11.2094\n1.756\n6.383\n0.000\n7.767\n14.651\n\n\nMTRANS[T.Walking]\n1.7141\n2.758\n0.622\n0.534\n-3.691\n7.119\n\n\nAge\n0.8105\n0.133\n6.108\n0.000\n0.550\n1.071\n\n\nHeight\n-184.0655\n14.785\n-12.450\n0.000\n-213.043\n-155.088\n\n\nWeight\n3.9430\n0.288\n13.681\n0.000\n3.378\n4.508\n\n\nFCVC\n0.8899\n1.009\n0.882\n0.378\n-1.088\n2.867\n\n\nNCP\n-1.1103\n0.710\n-1.564\n0.118\n-2.502\n0.281\n\n\nCH2O\n-1.5409\n0.877\n-1.757\n0.079\n-3.259\n0.178\n\n\nFAF\n-1.4599\n0.593\n-2.461\n0.014\n-2.622\n-0.297\n\n\nTUE\n-0.5909\n0.840\n-0.704\n0.482\n-2.237\n1.055\n\n\nNObeyesdad=3\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-138.5283\nnan\nnan\nnan\nnan\nnan\n\n\nGender[T.Male]\n-16.6646\n8.382\n-1.988\n0.047\n-33.094\n-0.235\n\n\nFamil_Hist_Owt[T.yes]\n2.3697\n11.592\n0.204\n0.838\n-20.350\n25.090\n\n\nFAVC[T.yes]\n-8.7847\n5.440\n-1.615\n0.106\n-19.447\n1.878\n\n\nCAEC[T.Frequently]\n-71.7139\n2.13e+08\n-3.37e-07\n1.000\n-4.17e+08\n4.17e+08\n\n\nCAEC[T.Sometimes]\n-3.9355\n4.749\n-0.829\n0.407\n-13.244\n5.373\n\n\nCAEC[T.no]\n7.7274\n977.625\n0.008\n0.994\n-1908.382\n1923.836\n\n\nSMOKE[T.yes]\n3.5336\n19.117\n0.185\n0.853\n-33.935\n41.002\n\n\nSCC[T.yes]\n-19.4881\n156.920\n-0.124\n0.901\n-327.046\n288.070\n\n\nCALC[T.Frequently]\n-43.6047\nnan\nnan\nnan\nnan\nnan\n\n\nCALC[T.Sometimes]\n-45.7392\nnan\nnan\nnan\nnan\nnan\n\n\nCALC[T.no]\n-28.2608\nnan\nnan\nnan\nnan\nnan\n\n\nMTRANS[T.Bike]\n0.0374\nnan\nnan\nnan\nnan\nnan\n\n\nMTRANS[T.Motorbike]\n-2.3922\n1.05e+11\n-2.28e-11\n1.000\n-2.05e+11\n2.05e+11\n\n\nMTRANS[T.Public_Transportation]\n22.6192\n6.634\n3.410\n0.001\n9.618\n35.621\n\n\nMTRANS[T.Walking]\n-5.3362\n34.114\n-0.156\n0.876\n-72.198\n61.526\n\n\nAge\n2.5098\n0.960\n2.615\n0.009\n0.629\n4.391\n\n\nHeight\n-278.8861\n44.172\n-6.314\n0.000\n-365.461\n-192.311\n\n\nWeight\n7.1526\n1.391\n5.141\n0.000\n4.426\n9.879\n\n\nFCVC\n4.1479\n3.269\n1.269\n0.204\n-2.258\n10.554\n\n\nNCP\n-1.5833\n2.388\n-0.663\n0.507\n-6.264\n3.098\n\n\nCH2O\n-13.3811\n5.527\n-2.421\n0.015\n-24.213\n-2.549\n\n\nFAF\n-9.8066\n4.355\n-2.252\n0.024\n-18.342\n-1.271\n\n\nTUE\n-5.7061\n3.289\n-1.735\n0.083\n-12.152\n0.739\n\n\nNObeyesdad=4\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-87.3253\n6.43e+07\n-1.36e-06\n1.000\n-1.26e+08\n1.26e+08\n\n\nGender[T.Male]\n-200.2957\n4.25e+07\n-4.71e-06\n1.000\n-8.33e+07\n8.33e+07\n\n\nFamil_Hist_Owt[T.yes]\n-30.9113\nnan\nnan\nnan\nnan\nnan\n\n\nFAVC[T.yes]\n-53.1787\nnan\nnan\nnan\nnan\nnan\n\n\nCAEC[T.Frequently]\n-28.5507\n2.16e+08\n-1.32e-07\n1.000\n-4.23e+08\n4.23e+08\n\n\nCAEC[T.Sometimes]\n-21.5727\n4.19e+07\n-5.15e-07\n1.000\n-8.21e+07\n8.21e+07\n\n\nCAEC[T.no]\n-2.1999\n1.31e+29\n-1.69e-29\n1.000\n-2.56e+29\n2.56e+29\n\n\nSMOKE[T.yes]\n-6.0935\n9.24e+08\n-6.59e-09\n1.000\n-1.81e+09\n1.81e+09\n\n\nSCC[T.yes]\n-12.3062\nnan\nnan\nnan\nnan\nnan\n\n\nCALC[T.Frequently]\n-6.2458\n1.59e+10\n-3.93e-10\n1.000\n-3.12e+10\n3.12e+10\n\n\nCALC[T.Sometimes]\n-37.1969\nnan\nnan\nnan\nnan\nnan\n\n\nCALC[T.no]\n-64.5072\nnan\nnan\nnan\nnan\nnan\n\n\nMTRANS[T.Bike]\n-0.2989\n1.92e+53\n-1.56e-54\n1.000\n-3.76e+53\n3.76e+53\n\n\nMTRANS[T.Motorbike]\n-0.2031\n3.86e+35\n-5.26e-37\n1.000\n-7.57e+35\n7.57e+35\n\n\nMTRANS[T.Public_Transportation]\n-57.6929\n5.78e+07\n-9.98e-07\n1.000\n-1.13e+08\n1.13e+08\n\n\nMTRANS[T.Walking]\n-7.4454\n2.11e+15\n-3.52e-15\n1.000\n-4.14e+15\n4.14e+15\n\n\nAge\n-9.3711\n100.732\n-0.093\n0.926\n-206.803\n188.061\n\n\nHeight\n-174.4791\n585.777\n-0.298\n0.766\n-1322.581\n973.623\n\n\nWeight\n8.7401\n34.352\n0.254\n0.799\n-58.588\n76.068\n\n\nFCVC\n49.0843\n3.05e+04\n0.002\n0.999\n-5.98e+04\n5.99e+04\n\n\nNCP\n2.3456\n4587.346\n0.001\n1.000\n-8988.688\n8993.379\n\n\nCH2O\n-18.5876\n33.678\n-0.552\n0.581\n-84.595\n47.420\n\n\nFAF\n-65.1863\n257.967\n-0.253\n0.801\n-570.792\n440.420\n\n\nTUE\n-44.3687\n279.949\n-0.158\n0.874\n-593.058\n504.321\n\n\nNObeyesdad=5\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-12.5582\n3.45e+05\n-3.64e-05\n1.000\n-6.76e+05\n6.76e+05\n\n\nGender[T.Male]\n-6.8927\n1.091\n-6.319\n0.000\n-9.031\n-4.755\n\n\nFamil_Hist_Owt[T.yes]\n-0.5826\n0.791\n-0.736\n0.462\n-2.134\n0.969\n\n\nFAVC[T.yes]\n2.6029\n0.975\n2.670\n0.008\n0.692\n4.514\n\n\nCAEC[T.Frequently]\n-7.2782\n2.533\n-2.873\n0.004\n-12.243\n-2.314\n\n\nCAEC[T.Sometimes]\n-2.8841\n2.442\n-1.181\n0.238\n-7.671\n1.903\n\n\nCAEC[T.no]\n-3.8084\n3.166\n-1.203\n0.229\n-10.013\n2.397\n\n\nSMOKE[T.yes]\n3.1147\n3.291\n0.947\n0.344\n-3.335\n9.565\n\n\nSCC[T.yes]\n2.1332\n1.626\n1.312\n0.190\n-1.054\n5.320\n\n\nCALC[T.Frequently]\n-9.0218\n3.45e+05\n-2.61e-05\n1.000\n-6.76e+05\n6.76e+05\n\n\nCALC[T.Sometimes]\n-9.1622\n3.45e+05\n-2.66e-05\n1.000\n-6.76e+05\n6.76e+05\n\n\nCALC[T.no]\n-10.7609\n3.45e+05\n-3.12e-05\n1.000\n-6.76e+05\n6.76e+05\n\n\nMTRANS[T.Bike]\n19.0539\n2529.381\n0.008\n0.994\n-4938.442\n4976.550\n\n\nMTRANS[T.Motorbike]\n1.6649\n47.401\n0.035\n0.972\n-91.240\n94.570\n\n\nMTRANS[T.Public_Transportation]\n6.0083\n1.212\n4.956\n0.000\n3.632\n8.385\n\n\nMTRANS[T.Walking]\n4.3751\n1.779\n2.460\n0.014\n0.889\n7.861\n\n\nAge\n0.4896\n0.107\n4.589\n0.000\n0.281\n0.699\n\n\nHeight\n-49.9784\n6.729\n-7.427\n0.000\n-63.167\n-36.790\n\n\nWeight\n1.7920\n0.168\n10.650\n0.000\n1.462\n2.122\n\n\nFCVC\n-0.8144\n0.599\n-1.359\n0.174\n-1.989\n0.360\n\n\nNCP\n-1.4253\n0.552\n-2.580\n0.010\n-2.508\n-0.343\n\n\nCH2O\n-1.8250\n0.678\n-2.690\n0.007\n-3.155\n-0.495\n\n\nFAF\n-0.5296\n0.375\n-1.412\n0.158\n-1.265\n0.206\n\n\nTUE\n-0.8409\n0.557\n-1.510\n0.131\n-1.932\n0.250\n\n\nNObeyesdad=6\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-2.1495\n1.51e+06\n-1.42e-06\n1.000\n-2.96e+06\n2.96e+06\n\n\nGender[T.Male]\n-6.7717\n1.213\n-5.583\n0.000\n-9.149\n-4.395\n\n\nFamil_Hist_Owt[T.yes]\n1.9277\n1.078\n1.789\n0.074\n-0.185\n4.040\n\n\nFAVC[T.yes]\n-0.4390\n1.141\n-0.385\n0.700\n-2.676\n1.798\n\n\nCAEC[T.Frequently]\n-5.4475\n3.295\n-1.653\n0.098\n-11.906\n1.011\n\n\nCAEC[T.Sometimes]\n0.8345\n3.075\n0.271\n0.786\n-5.192\n6.861\n\n\nCAEC[T.no]\n1.6818\n3.972\n0.423\n0.672\n-6.103\n9.466\n\n\nSMOKE[T.yes]\n7.0586\n3.567\n1.979\n0.048\n0.068\n14.049\n\n\nSCC[T.yes]\n1.3350\n2.021\n0.661\n0.509\n-2.625\n5.295\n\n\nCALC[T.Frequently]\n-2.1230\n1.51e+06\n-1.41e-06\n1.000\n-2.96e+06\n2.96e+06\n\n\nCALC[T.Sometimes]\n-4.6506\n1.51e+06\n-3.08e-06\n1.000\n-2.96e+06\n2.96e+06\n\n\nCALC[T.no]\n-4.1703\n1.51e+06\n-2.76e-06\n1.000\n-2.96e+06\n2.96e+06\n\n\nMTRANS[T.Bike]\n-21.8443\n6.4e+09\n-3.41e-09\n1.000\n-1.26e+10\n1.26e+10\n\n\nMTRANS[T.Motorbike]\n3.1683\n47.467\n0.067\n0.947\n-89.865\n96.202\n\n\nMTRANS[T.Public_Transportation]\n8.7749\n1.423\n6.165\n0.000\n5.985\n11.564\n\n\nMTRANS[T.Walking]\n1.2621\n2.258\n0.559\n0.576\n-3.163\n5.687\n\n\nAge\n0.6974\n0.116\n6.002\n0.000\n0.470\n0.925\n\n\nHeight\n-104.7093\n9.038\n-11.585\n0.000\n-122.424\n-86.995\n\n\nWeight\n2.6268\n0.190\n13.821\n0.000\n2.254\n2.999\n\n\nFCVC\n0.2192\n0.764\n0.287\n0.774\n-1.278\n1.716\n\n\nNCP\n-1.8144\n0.606\n-2.992\n0.003\n-3.003\n-0.626\n\n\nCH2O\n-1.9110\n0.757\n-2.525\n0.012\n-3.394\n-0.428\n\n\nFAF\n-0.9928\n0.439\n-2.264\n0.024\n-1.852\n-0.133\n\n\nTUE\n0.0701\n0.671\n0.104\n0.917\n-1.246\n1.386\n\n\n\n\n\n\n\n\n\nEvaluating Model Performance\nTo gauge our models‚Äô effectiveness, we‚Äôll employ various metrics such as accuracy, precision, recall, and F1-score. A confusion matrix will help visualize how well our models perform in classifying outcomes‚Äîthink of it as a report card for your model!\n\n\nEvaluating the Logit model\n# Predict on test data\nbase_preds = logit_model.predict(data_df_test).idxmax(axis=1)\ny_test = data_df_test[target]\n\n# Evaluate the model\naccuracy_orig = accuracy_score(y_test, base_preds)\nreport_orig = classification_report(y_test, base_preds)\n\nprint(\"Accuracy:\", accuracy_orig)\nprint(\"Classification Report:\")\nprint(report_orig)\n\n\nAccuracy: 0.909952606635071\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.93      0.86      0.89        29\n           1       0.86      0.83      0.84        29\n           2       0.95      0.91      0.93        45\n           3       0.94      0.97      0.95        31\n           4       1.00      0.96      0.98        27\n           5       0.83      0.90      0.86        21\n           6       0.84      0.93      0.89        29\n\n    accuracy                           0.91       211\n   macro avg       0.91      0.91      0.91       211\nweighted avg       0.91      0.91      0.91       211\n\n\n\n\n\n\nLooking for some Improvments!\n\nFeature Selection Using CrossTab Sparsity\nNow comes the exciting part‚Äîusing CrossTab Sparsity to refine our feature selection process! It‚Äôs like cleaning up your closet and only keeping the clothes that spark joy (thank you, Marie Kondo). 1\n1¬†This is based on work in Unique Metric for Health Analysis with Optimization of Clustering Activity and Cross Comparison of Results from Different Approach. Paper LinkCode is here!\n\n\nStandared Steps for Feature Selection\n\nCalculate CrossTab Sparsity: For each feature against the target variable.\nSelect Features: Based on sparsity scores that indicate significant interactions with the target variable.\nRecreate Models: Train new models using only the selected features‚Äîless is often more!\n\nHere we go!!!\n\n\n\nDoing what needs to Done Code ;)\nsns.set_style(\"white\")\nsns.set_context(\"paper\")\n# Calculating Crostab sparsity for each Column\nresults = crosstab_sparsity(data_df_train.iloc[:,:-1],data_df_train[target],numeric_bin='decile')\n\n# presenting results for consumption\ndf_long = pd.melt(results['scores'], id_vars=['Columns'], value_vars=['seggregation', 'explaination', 'metric'],\n                  var_name='Metric', value_name='values')\n\n# Adding jitter: small random noise to 'Columns' (x-axis)\n# df_long['values_jittered'] = df_long['Value'] + np.random.uniform(-0.1, 0.1, size=len(df_long))\n\n# Create a seaborn scatter plot with jitter, more professional color palette, and transparency\nplt.figure(figsize=(12, 5))\nsns.scatterplot(x='Columns', y='values', hue='Metric', style='Metric',\n        data=df_long, s=100, alpha=0.7, palette='deep')\n\n# Title and labels\nplt.title('Metrics by Columns', fontsize=16)\nplt.xticks(rotation=45) \nplt.xlabel('Columns', fontsize=10)\nplt.ylabel('Value', fontsize=10)\n\n# Display legend outside the plot for better readability\nplt.legend(title='Metric', loc='upper right', fancybox=True, framealpha=0.5)\n\n# Show the plot\nplt.tight_layout()\nplt.show()\n\n\nCSP calculated with decile for breaks!\n\nScores for 7 groups(s) is : 140.96057955229762\n\n\n\n\n\n\n\n\n\n\n\n\nAnd Drum Rolls pelase!!!\nUsing just top 5 varaibles we are getting almost similar or better overall accuracy. This amounts to greatly simplifing the models and clearly explain why some variable are not useful for modeling.\n\n\nAnd finally training and evaluating with drum rolls\nlogit_model_rev = sm.MNLogit.from_formula(f\"{target} ~ {' + '.join(results['scores'].loc[:5,'Columns'].values)}\", \n    data=data_df_train\n).fit_regularized()\n\n# Predict on test data\nchallenger_preds = logit_model_rev.predict(data_df_test).idxmax(axis=1)\ny_test = data_df_test[target]\n\n# Evaluate the model\naccuracy_new = accuracy_score(y_test, challenger_preds)\nreport_new = classification_report(y_test, challenger_preds)\n\nprint(\"Accuracy:\", accuracy_new)\nprint(\"Classification Report:\")\nprint(report_new)\n\n\nOptimization terminated successfully    (Exit mode 0)\n            Current function value: 0.174380345428068\n            Iterations: 417\n            Function evaluations: 662\n            Gradient evaluations: 417\nAccuracy: 0.9383886255924171\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.93      0.97      0.95        29\n           1       0.93      0.93      0.93        29\n           2       0.96      1.00      0.98        45\n           3       0.93      0.90      0.92        31\n           4       0.93      0.93      0.93        27\n           5       0.90      0.90      0.90        21\n           6       0.96      0.90      0.93        29\n\n    accuracy                           0.94       211\n   macro avg       0.94      0.93      0.93       211\nweighted avg       0.94      0.94      0.94       211\n\n\n\n\n\n\n\n\n\n\n\nNoneSummary of retrained model\n\n\n\n\n\n\n\nCode\ndisplay(logit_model_rev.summary())\n\n\n\nMNLogit Regression Results\n\n\nDep. Variable:\nNObeyesdad\nNo. Observations:\n1900\n\n\nModel:\nMNLogit\nDf Residuals:\n1858\n\n\nMethod:\nMLE\nDf Model:\n36\n\n\nDate:\nThu, 27 Feb 2025\nPseudo R-squ.:\n0.9103\n\n\nTime:\n03:18:54\nLog-Likelihood:\n-331.32\n\n\nconverged:\nTrue\nLL-Null:\n-3691.8\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n0.000\n\n\n\n\n\n\n\n\nNObeyesdad=1\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n58.1248\n9.361\n6.209\n0.000\n39.778\n76.472\n\n\nTUE\n0.1130\n0.445\n0.254\n0.799\n-0.759\n0.985\n\n\nCH2O\n-0.8634\n0.609\n-1.419\n0.156\n-2.056\n0.329\n\n\nFAF\n0.1425\n0.334\n0.426\n0.670\n-0.513\n0.798\n\n\nAge\n0.0579\n0.077\n0.754\n0.451\n-0.093\n0.208\n\n\nHeight\n-76.5735\n10.536\n-7.268\n0.000\n-97.224\n-55.923\n\n\nWeight\n1.3337\n0.176\n7.566\n0.000\n0.988\n1.679\n\n\nNObeyesdad=2\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n328.4616\n25.112\n13.080\n0.000\n279.242\n377.681\n\n\nTUE\n2.2275\n0.870\n2.560\n0.010\n0.522\n3.933\n\n\nCH2O\n-1.4150\n0.984\n-1.439\n0.150\n-3.343\n0.513\n\n\nFAF\n-1.3585\n0.747\n-1.820\n0.069\n-2.822\n0.105\n\n\nAge\n0.1537\n0.097\n1.591\n0.112\n-0.036\n0.343\n\n\nHeight\n-426.3945\n30.970\n-13.768\n0.000\n-487.095\n-365.694\n\n\nWeight\n5.3584\n0.372\n14.386\n0.000\n4.628\n6.088\n\n\nNObeyesdad=3\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n306.6447\n33.046\n9.279\n0.000\n241.876\n371.413\n\n\nTUE\n-7.8630\n5.691\n-1.382\n0.167\n-19.017\n3.291\n\n\nCH2O\n-21.0118\n11.508\n-1.826\n0.068\n-43.567\n1.543\n\n\nFAF\n-11.3624\n5.759\n-1.973\n0.048\n-22.650\n-0.075\n\n\nAge\n2.4017\n1.260\n1.905\n0.057\n-0.069\n4.872\n\n\nHeight\n-710.3867\n156.303\n-4.545\n0.000\n-1016.734\n-404.039\n\n\nWeight\n10.1072\n2.588\n3.905\n0.000\n5.034\n15.180\n\n\nNObeyesdad=4\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n352.4249\n33.573\n10.497\n0.000\n286.623\n418.227\n\n\nTUE\n-9.2469\n5.711\n-1.619\n0.105\n-20.440\n1.946\n\n\nCH2O\n-20.6780\n11.516\n-1.796\n0.073\n-43.250\n1.894\n\n\nFAF\n-14.7525\n5.794\n-2.546\n0.011\n-26.108\n-3.397\n\n\nAge\n2.1487\n1.262\n1.703\n0.089\n-0.325\n4.622\n\n\nHeight\n-758.2318\n156.401\n-4.848\n0.000\n-1064.772\n-451.692\n\n\nWeight\n10.5011\n2.589\n4.056\n0.000\n5.427\n15.575\n\n\nNObeyesdad=5\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n126.2892\n12.539\n10.072\n0.000\n101.713\n150.865\n\n\nTUE\n0.5832\n0.541\n1.077\n0.281\n-0.478\n1.645\n\n\nCH2O\n-0.8764\n0.706\n-1.242\n0.214\n-2.260\n0.507\n\n\nFAF\n-0.1920\n0.403\n-0.476\n0.634\n-0.983\n0.599\n\n\nAge\n0.0719\n0.082\n0.874\n0.382\n-0.089\n0.233\n\n\nHeight\n-160.2982\n14.026\n-11.429\n0.000\n-187.788\n-132.808\n\n\nWeight\n2.3663\n0.208\n11.397\n0.000\n1.959\n2.773\n\n\nNObeyesdad=6\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n207.3760\n15.374\n13.489\n0.000\n177.244\n237.508\n\n\nTUE\n1.6561\n0.646\n2.564\n0.010\n0.390\n2.922\n\n\nCH2O\n-0.6583\n0.773\n-0.851\n0.395\n-2.174\n0.857\n\n\nFAF\n-0.1243\n0.485\n-0.256\n0.798\n-1.076\n0.827\n\n\nAge\n0.1042\n0.087\n1.197\n0.231\n-0.066\n0.275\n\n\nHeight\n-266.6050\n17.598\n-15.150\n0.000\n-301.097\n-232.113\n\n\nWeight\n3.6160\n0.241\n15.026\n0.000\n3.144\n4.088\n\n\n\n\n\n\n\n\n\n\n\nImpact on Model Accuracy\nAfter applying feature selection based on CrossTab Sparsity, we‚Äôll compare the accuracy of our new models against our baseline models. This comparison will reveal how effectively CrossTab Sparsity enhances classification performance.\n\nResults and Discussion: Unveiling Insights\nModel Comparison Table\nAfter implementing CrossTab Sparsity in our feature selection process, let‚Äôs take a look at the results:\n\n\nComparision Code\nmetrics = {\n    \"Metric\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"],\n    \"Baseline Model with all Parameters\": [\n        accuracy_score(y_test, base_preds),\n        precision_score(y_test, base_preds, average='weighted'),\n        recall_score(y_test, base_preds, average='weighted'),\n        f1_score(y_test, base_preds, average='weighted'),\n    ],\n    \"Challenger Model with only 5 Variables\": [\n        accuracy_score(y_test, challenger_preds),\n        precision_score(y_test, challenger_preds, average='weighted'),\n        recall_score(y_test, challenger_preds, average='weighted'),\n        f1_score(y_test, challenger_preds, average='weighted'),\n    ]\n}\ndisplay(pd.DataFrame(metrics).round(4).set_index('Metric').T)\n\n\n\n\n\n\n\n\nMetric\nAccuracy\nPrecision\nRecall\nF1-Score\n\n\n\n\nBaseline Model with all Parameters\n0.9100\n0.9123\n0.9100\n0.9103\n\n\nChallenger Model with only 5 Variables\n0.9384\n0.9384\n0.9384\n0.9381\n\n\n\n\n\n\n\nInsights Gained\nThrough this analysis, several key insights emerge:\n\n\nReduction of similar accuracy from 16 to 5 i.e 68.75% reduction\n\n\n\nFeature Interactions Matter: The selected features based on CrossTab Sparsity significantly improved model accuracy‚Äîlike finding out which ingredients make your favorite dish even better!\nSimplicity is Key: By focusing on relevant features, we enhance accuracy while simplifying model interpretation‚Äîbecause nobody likes unnecessary complexity.\nReal-World Applications: These findings have practical implications in fields such as environmental science where classification plays a critical role‚Äîhelping us make better decisions for our planet.\n\n\n\n\nConclusion: The Road Ahead\nIn conclusion, this blog has illustrated how CrossTab Sparsity can be a game-changer in classification tasks using the Obesity dataset. By leveraging this metric for feature selection, we achieved notable improvements in model performance‚Äîproof that sometimes less really is more!\nFuture Work: Expanding Horizons\nAs we look ahead, there are exciting avenues to explore:\n\nInvestigating regression problems using CrossTab Sparsity.\nComparing its effectiveness with other feature selection methods such as Recursive Feature Elimination (RFE) or comparision with other feature selection mehtods.\n\nBy continuing this journey into data science, we not only enhance our technical skills but also contribute valuable insights that can drive meaningful change in various industries.\n\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "posts/06_saving_my_blog/index.html",
    "href": "posts/06_saving_my_blog/index.html",
    "title": "Journey back to Bloggin‚Äô with Quarto",
    "section": "",
    "text": "Journey that challenges you are the best! Image by Devraj Bajgain\n\n\n\nMy Blogging Evolution: From Jekyll‚Äôs Depths to Quarto‚Äôs Data Science Peak\nThe realm of blogging platforms is expansive and constantly evolving, offering solutions for every niche. As a data science architect, my journey through this landscape has been a search for the ideal tool‚Äîone that integrates with my workflow, supports my use of R and Python, and enables me to share insights efficiently. This quest has led me from Jekyll to Hugo, and finally, to the data science-focused environment of Quarto. Let me recount this evolution.\n\nThe Beginning: Jekyll‚Äôs Promise and Problems\nAround 2015, Jekyll was a leading choice for static site generation. It promised a simple, static blog, but the reality had its challenges. The setup was relatively straightforward, but as a data scientist, I encountered significant drawbacks. My workflow involved R and Python code, and integrating these into blog posts was cumbersome. I spent more time tweaking Ruby and HTMLthan writing, which wasn‚Äôt ideal.\nIt was an inefficient process that stifled creativity. I needed a platform that understood the needs of data scientists, allowing seamless integration of code, narrative, and visuals. Despite its initial appeal, Jekyll proved more of a hindrance üò•\n\n\nA Glimmer of Hope: Hugo and Blogdown\nNext, I discovered Hugo, along with the blogdown package in R. This combination allowed me to write blog posts directly in R Markdown, which was revolutionary. I created shortcodes that suited my style, and writing in R Markdown was a game-changer.\nHowever, my enthusiasm was short-lived. As I shifted more towards Python for development, I faced the same old problem: juggling between environments. The dream of a unified workflow remained out of reach.\n\n\nThe Ideal: Quarto\nThen came Quarto. This has been transformative. Quarto is designed for researchers and data scientists, supporting both R and Python seamlessly. I started migrating my site in early 2022, and the transition was smooth. I built my site, deployed it on Netlify, and finally had a natural and intuitive workflow.\nQuarto allowed me to focus on content. I could write, code, and visualize without leaving my data science environment. Its support for multiple output formats made adapting content for different channels easy.\n\n\nThe Hiatus and the Return\nLife intervened, and blogging took a backseat during 2023 and early 2024.\nNow, I‚Äôm back. The goal is to blog better, creating content that sparks conversations and pushes data science communication. I aim to leverage Quarto‚Äôs full capabilities, building an engaging and informative blog, dashboards and whatever is possible. The plan is to distribute content across multiple channels and build a community. Fingers crossed, I won‚Äôt stop this time. ‚úåÔ∏è\n\n\n\nLessons Learned and Resources\nMy blogging journey has taught me valuable lessons:\n\nChoose the Right Tool: The platform impacts your productivity. Select one that aligns with your workflow.\nEmbrace Simplicity: Focus on creating content and let the platform handle the rest.\nStay Consistent: Consistency is key. Maintain a realistic schedule.\nEngage with Your Audience: Interact with readers and build a community.\n\nMy recent inspiration is from :\n\n%&gt;% dreams Blog by Isabella Vel√°squez.\nNotes from a data witch A blog by Danielle Navarro.\nAlbert Rapp His personal Blog.\nRandom Realizations A great work by Matt Bowers especially for python users.\nBits of Analytics A good simple site!\nAwesome Quarto A great place to find best things on Quarto!!!\n\nTo enhance my blog, I‚Äôm referring to these resources:\n\nAster Hu‚Äôs experience migrating from Jekyll to Quarto\nRasmus Nes‚Äôs Blog on Hugo vs Quarto\nDiscussion on lightweight personal blogging platforms on Hacker News\nDan MacKinlay‚Äôs notes on static websites\nAnother Hacker News discussion on the best way to author blogs in 2024\nCan quarto work with Jekyll?\nSwesome Static Generators\n\nHere‚Äôs to the next phase of my blogging evolution. May it be filled with insights and innovation!\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research Work",
    "section": "",
    "text": "Unique Metric for Health Analysis with Optimization of Clustering Activity and Cross Comparison of Results from Different Approach. Paper Link\n\n\n\nA method for estimating clusters holistically. The metric dose not depend on any assumptions about the Approach to Cluster the Data. All that matters is the Data what it fits on and what are the Cluster Segments observed.\n\n\n\n\n\nReinforcement Evolutionary Learning Method for Self-Learning (RELM).Paper Link\n\n\nRELM is a take on using Reinforcement Learning on Quantitative Data where learning happens via Evolutionary Algorithms (GA, ES, NES, NSGA-II, etc). This method takes on the challenge age old problem of Concept Drift.\n\n\nFuturistic Classification with Dynamic Reference Frame Strategy.Paper Link\n\n\nAs a Data science practitioner, one of the classical problem is to understand The Churn. This paper takes a perspective on data that enables to identify what actually distinguishes Churn from Other.\n\n\nPersonalized Influence Estimation Technique(PIE). Paper Link\n\n\nWe have lot of methods to access the global influence of a variable in data for most of ML Algorithms. But to make a justifiable point estimate about a datum might be a key source of information in many cases of Anomaly detection, Churn Analysis or Employee Attrition Reduction, etc.. Individual PIE values (point estimates) can be used to trigger a certain action. In this paper we estimate PIE values for Linear and Non Linear models.\n\n\nDistribution Assertive Regression (DAR). Paper Link and Blog Post\n\n\nDAR is regression analysis in which we understand how a fit of regression varies over it‚Äôs value. In this paper we resolve the Regression by Parts and estimate the new data based on heuristic methods like K-nearest Neighbor and Regression Equation."
  }
]