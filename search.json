[
  {
    "objectID": "webinar/index.html",
    "href": "webinar/index.html",
    "title": "Webinar",
    "section": "",
    "text": "Webinar Content\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Don‚Äôt Learn AI. Operationalize It.",
    "section": "",
    "text": "üè¢\n15+ Years\nSystems Architecture & AI Strategy\n\n\nüíº\n$80M+ Deployed\nFortune 500 AI Projects\n\n\nüë•\n50+ Trained\nC-Suite Leaders\n\n\n‚ö°\n10 hrs/week Saved\nProven In 30 Days"
  },
  {
    "objectID": "index.html#why-leaders-trust-this",
    "href": "index.html#why-leaders-trust-this",
    "title": "Don‚Äôt Learn AI. Operationalize It.",
    "section": "",
    "text": "üè¢\n15+ Years\nSystems Architecture & AI Strategy\n\n\nüíº\n$80M+ Deployed\nFortune 500 AI Projects\n\n\nüë•\n50+ Trained\nC-Suite Leaders\n\n\n‚ö°\n10 hrs/week Saved\nProven In 30 Days"
  },
  {
    "objectID": "index.html#the-problem",
    "href": "index.html#the-problem",
    "title": "Don‚Äôt Learn AI. Operationalize It.",
    "section": "The Problem",
    "text": "The Problem\nYou‚Äôre under pressure. Your CEO is asking about AI. Your competition is moving faster.\nEveryone‚Äôs selling you tools. Nobody‚Äôs teaching you architecture.\nThe gap isn‚Äôt tools. It‚Äôs strategy.\nSenior professionals aren‚Äôt being replaced by AI. They‚Äôre being replaced by junior professionals who know how to use it."
  },
  {
    "objectID": "index.html#this-is-for-you-if",
    "href": "index.html#this-is-for-you-if",
    "title": "Don‚Äôt Learn AI. Operationalize It.",
    "section": "This Is For You If:",
    "text": "This Is For You If:\n‚Üí C-suite, VP, or department head with decision-making authority\n‚Üí 12+ years of professional experience\n‚Üí Managing teams, budgets, or strategic initiatives\n‚Üí You‚Äôve watched competitors move faster and asked: ‚ÄúWhat do we do first?‚Äù\n\nNot For You If:\nYou‚Äôre looking for ‚ÄúAI tools‚Äù or ‚ÄúChatGPT shortcuts.‚Äù We teach systems thinking, not prompts."
  },
  {
    "objectID": "index.html#what-you-build-the-ai-profit-os",
    "href": "index.html#what-you-build-the-ai-profit-os",
    "title": "Don‚Äôt Learn AI. Operationalize It.",
    "section": "What You Build: The AI Profit OS‚Ñ¢",
    "text": "What You Build: The AI Profit OS‚Ñ¢\n3 days. 3 deliverables.\n\n1. Strategy Framework\nA decision tree for any AI opportunity in your business:\nCan AI solve this? Should we? What‚Äôs the cost?\n\n\n2. Execution Roadmap\nStep-by-step plan for your first AI use case.\nYou design. Your team builds. No consultant needed.\n\n\n3. ROI Model\nThe justification leadership wants to see.\nBefore/after impact. Timeline. Break-even. Confidence level."
  },
  {
    "objectID": "index.html#what-happens-next",
    "href": "index.html#what-happens-next",
    "title": "Don‚Äôt Learn AI. Operationalize It.",
    "section": "What Happens Next",
    "text": "What Happens Next\nDays 1‚Äì3: You attend live. You design your first use case.\nDays 4‚Äì30: Your team executes the roadmap. You ship something real.\nDay 31+: You measure results. You architect the next use case.\nLifetime: Access to 2x/month coaching sessions + private community."
  },
  {
    "objectID": "index.html#your-track-record",
    "href": "index.html#your-track-record",
    "title": "Don‚Äôt Learn AI. Operationalize It.",
    "section": "Your Track Record",
    "text": "Your Track Record\n\n\n\nExperience\nAI Strategist. Ex AI-CTO. 15+ years.\n\n\nDeployed\n$80M+ in Fortune 500 AI projects.\n\n\nTrained\n50+ Fortune 500 executives.\n\n\nOutcome\nAverage: 10 hours/week reclaimed within 30 days."
  },
  {
    "objectID": "index.html#investment-24999",
    "href": "index.html#investment-24999",
    "title": "Don‚Äôt Learn AI. Operationalize It.",
    "section": "Investment: ‚Çπ24,999",
    "text": "Investment: ‚Çπ24,999\nIncludes: - 3 days live instruction - Recordings if you miss a session - Daily assignments + worksheets - Private community access - Lifetime group coaching (2x/month)\nFormat: Live Virtual\nDates: Jan 9‚Äì11, 2026\nSeats: Limited"
  },
  {
    "objectID": "index.html#the-guarantee",
    "href": "index.html#the-guarantee",
    "title": "Don‚Äôt Learn AI. Operationalize It.",
    "section": "The Guarantee",
    "text": "The Guarantee\nAttend all 3 days. Complete the work. If you don‚Äôt map at least one viable AI use case + create a 30‚Äì90 day roadmap, I‚Äôll refund 100% + pay you ‚Çπ2,500.\nThis is a performance guarantee, not a refund policy. Your success is my incentive."
  },
  {
    "objectID": "index.html#ready",
    "href": "index.html#ready",
    "title": "Don‚Äôt Learn AI. Operationalize It.",
    "section": "Ready?",
    "text": "Ready?\n\nSecure Your Seat ‚Äì ‚Çπ24,999\n\nFounding rate. Limited seats. Next cohort: ‚Çπ36,999."
  },
  {
    "objectID": "index.html#where-this-works",
    "href": "index.html#where-this-works",
    "title": "Don‚Äôt Learn AI. Operationalize It.",
    "section": "Where This Works",
    "text": "Where This Works\n\nFortune 500 Ops Team: 8 Weeks to Shipped\nProblem: 60+ hours/week of manual planning. Consultants wanted ‚Çπ100K+ and 12 weeks.\nWhat Changed: Team attended cohort. Built architecture in 3 days. Shipped MVP in 8 weeks.\nOutcome: 30+ hours/week saved. ‚Çπ100K in consulting fees avoided.\nLesson: Non-technical operators can architect AI when they have a framework.\nRead the full case study ‚Üí"
  },
  {
    "objectID": "index.html#common-questions",
    "href": "index.html#common-questions",
    "title": "Don‚Äôt Learn AI. Operationalize It.",
    "section": "Common Questions",
    "text": "Common Questions\nQ: I don‚Äôt have a technical background. Can I do this?\nA: Yes. You‚Äôre learning to architect like an executive, not code. That‚Äôs your job already.\n\nQ: What if my team doesn‚Äôt execute?\nA: Then it‚Äôs on you (leadership). But the roadmap is specific enough that your CTO can build it in 30 days.\n\nQ: Can I scale this to multiple departments?\nA: You architect one use case at a time. After the first ships, you can architect the next 3.\n\nQ: How is this different from hiring a consultant?\nA: A consultant does the work for you. This teaches you to do it. Plus, ‚Çπ24,999 vs.¬†‚Çπ100K."
  },
  {
    "objectID": "index.html#stop-guessing.-start-architecting.",
    "href": "index.html#stop-guessing.-start-architecting.",
    "title": "Don‚Äôt Learn AI. Operationalize It.",
    "section": "Stop Guessing. Start Architecting.",
    "text": "Stop Guessing. Start Architecting.\nJanuary cohort starts in [X days]. Seats are limited.\n\nSecure Your Seat ‚Äì ‚Çπ24,999"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Get in Touch",
    "section": "",
    "text": "Contact me for AI Consulting and Strategy services, or to discuss how AI can transform your business.\nFor information about courses and training programs, please visit our Courses page.\n   \n\n\n\n\n\n\nFull Name \nEmail Address \nMessage\n\nSend message\n\n\n\nSubscribe to Updates \nStay informed about AI trends, insights, and new content. Subscribe to receive updates directly to your inbox.\n\n\n\n\n\n\n  Email Address\n  \n  \n  \n\n\n\n\n\n\n\n  \n\n&lt;button type=\"submit\" class=\"btn\"&gt;Subscribe&lt;/button&gt;\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "cohort/index.html",
    "href": "cohort/index.html",
    "title": "Build Your Personal  AI Operating System",
    "section": "",
    "text": "NEXT COHORT\nJanuary 09-11, 2026 7 PM‚Äì10 PM IST)\n\n\n\n\nFORMAT\nLive Virtual (3 consecutive evenings)\n\n\n\n\nInvestment\n‚Çπ24,999 (Founding rate. Next cohort: ‚Çπ36,999)\n\n\n\n\nSeats limited to 50 per cohort. January filling fast. Click above and get enrolled."
  },
  {
    "objectID": "cohort/index.html#for-senior-professionals-who",
    "href": "cohort/index.html#for-senior-professionals-who",
    "title": "Build Your Personal  AI Operating System",
    "section": "FOR SENIOR PROFESSIONALS WHO‚Ä¶",
    "text": "FOR SENIOR PROFESSIONALS WHO‚Ä¶\n\n\n\n\n\n\nAre you a VP, C-suite executive, department head, or founder who:\n‚úì Manage teams or portfolios with real P&L responsibility\n‚úì Have 10+ years of professional experience (you‚Äôve seen fads come and go)\n‚úì Know AI is a competitive advantage, but you‚Äôre unsure what to implement or how\n‚úì Are confident as a strategist, but feel lost on AI specifically\n‚úì Want to lead with AI without becoming a data scientist\n\n\n\nIf this describes you, this cohort is designed for you.\nYou‚Äôre not looking for a ‚Äúlearn ChatGPT‚Äù tutorial. You‚Äôre looking for framework‚Äîa system to evaluate any AI opportunity, quantify the ROI, and hand it off to your team for execution.\nThat‚Äôs exactly what this 3-day intensive delivers."
  },
  {
    "objectID": "cohort/index.html#this-is-not-for-you-if",
    "href": "cohort/index.html#this-is-not-for-you-if",
    "title": "Build Your Personal  AI Operating System",
    "section": "THIS IS NOT FOR YOU IF‚Ä¶",
    "text": "THIS IS NOT FOR YOU IF‚Ä¶\n\nSkip this cohort if you:\n‚úó Are new to your field (less than 5 years experience) ‚Äî you‚Äôll get more value from foundational AI courses\n‚úó Want to learn how to code or prompt-engineer ‚Äî this is strategy, not technical implementation\n‚úó Expect AI to do 100% of your job without you learning anything ‚Äî that‚Äôs not how this works\n‚úó Are looking for quick wins or ‚Äúside hustle‚Äù income ‚Äî we focus on sustainable competitive advantage\n‚úó Won‚Äôt invest 2‚Äì3 hours/week for 30 days after the cohort ‚Äî ongoing execution is required\n‚úó Want theory and certifications instead of practical operational systems ‚Äî we don‚Äôt do that\n\nNo hard feelings if this isn‚Äôt for you. There are great foundational courses out there. This is for builders who are ready to architect."
  },
  {
    "objectID": "cohort/index.html#what-youll-build-in-3-days",
    "href": "cohort/index.html#what-youll-build-in-3-days",
    "title": "Build Your Personal  AI Operating System",
    "section": "WHAT YOU‚ÄôLL BUILD IN 3 DAYS",
    "text": "WHAT YOU‚ÄôLL BUILD IN 3 DAYS\n\nBy the end of 3 days, you walk away with 3 tangible, executable assets:"
  },
  {
    "objectID": "cohort/index.html#your-3-day-roadmap",
    "href": "cohort/index.html#your-3-day-roadmap",
    "title": "Build Your Personal  AI Operating System",
    "section": "YOUR 3-DAY ROADMAP",
    "text": "YOUR 3-DAY ROADMAP\n\n\nDay 1: AI ART Matrix‚Ñ¢ ‚Äì Strategic Foundation\nWhat you‚Äôll learn: How to evaluate ANY business problem and determine if AI is the right solution.\nThe framework: AI ART Matrix (Alignment, Readiness, and Transformation)\n\n1 hour teaching + practice: We walk through the framework with live examples. You‚Äôll see how to ask the right questions about your own business.\n1 hour group breakout: You‚Äôll work in small groups on a real use case. We‚Äôll refine your thinking in real time.\n30 min Q&A: Ask me anything about your specific problems. We‚Äôll help you think through them.\n\nBy end of Day 1: You‚Äôll have evaluated 3‚Äì5 of your own business problems and ranked them by AI fit + strategic impact.\n\n\nDay 2: O.L.C.D Framework‚Ñ¢ ‚Äì Diagnostic Mapping\nWhat you‚Äôll learn: How to map problems to actual AI use cases and understand what you‚Äôd need to execute.\nThe framework: O.L.C.D (Outcome, Logic, Capability, Data)\n\n1 hour teaching + practice: We walk through real client examples. You‚Äôll see how different problems map to different AI solutions (some are ML, some are automation, some are pure workflow).\n1 hour group breakout: You‚Äôll take your top use case from Day 1 and map it using O.L.C.D. We‚Äôll uncover what data you have, what you‚Äôre missing, and what‚Äôs actually feasible.\n30 min Q&A: Get clarity on roadblocks, data gaps, or execution challenges.\n\nBy end of Day 2: You‚Äôll have a diagnostic blueprint for your top 1‚Äì2 use cases (what to build, what data you need, who should own it).\n\n\nDay 3: ROI Mapping ‚Äì Execution & Measurement\nWhat you‚Äôll learn: How to quantify the business impact of your AI initiative and lock in stakeholder buy-in.\nThe methodology: Before/After ROI Model (time saved, cost reduced, revenue enabled, risk mitigated)\n\n1 hour teaching + practice: We walk through real ROI scenarios. You‚Äôll see how to translate ‚Äúsaves time‚Äù into ‚Äúsaves ‚ÇπX‚Äù and how to build confidence intervals for your CFO.\n1 hour group breakout: You‚Äôll build the ROI model for your top use case. What‚Äôs the before state? What‚Äôs the after state? What‚Äôs the timeline? What‚Äôs the investment required?\n1 hour Q&A + Roadmap finalization: We‚Äôll lock in your 30‚Äì90 day execution plan. You‚Äôll leave with a dated, accountable roadmap.\n\nBy end of Day 3: You‚Äôll have a ROI-backed business case and a 90-day roadmap you can hand to your team or present to leadership."
  },
  {
    "objectID": "cohort/index.html#proof-this-actually-works",
    "href": "cohort/index.html#proof-this-actually-works",
    "title": "Build Your Personal  AI Operating System",
    "section": "PROOF: THIS ACTUALLY WORKS",
    "text": "PROOF: THIS ACTUALLY WORKS\n\nYou might be thinking: ‚ÄúThis sounds good in theory. Does it actually work?‚Äù\nHere are 3 real examples of what happened when senior professionals applied this framework:"
  },
  {
    "objectID": "cohort/index.html#what-past-leaders-say",
    "href": "cohort/index.html#what-past-leaders-say",
    "title": "Build Your Personal  AI Operating System",
    "section": "WHAT PAST LEADERS SAY",
    "text": "WHAT PAST LEADERS SAY\n\n\n\n‚ÄúI was skeptical that a 3-day course could teach me strategy. But the frameworks‚Äîthe AI ART matrix and O.L.C.D‚Äîthey actually changed how I think about AI in our business. Within 30 days, I‚Äôd identified 3 legitimate use cases and got my team aligned on the roadmap. This isn‚Äôt a course. It‚Äôs a thinking system.‚Äù\n\n‚Äî [VP of Operations, Fortune 500 Tech Company]\n\n\n‚ÄúEvery consultant we talked to quoted ‚Çπ100K+ and 6 months. The frameworks I learned here, I could apply immediately. I didn‚Äôt need to hire anyone. My team and I built the first use case in 8 weeks. This is a cheat code for non-technical leaders.‚Äù\n\n‚Äî [Chief Financial Officer, Mid-Market Services]\n\n\n‚ÄúI was drowning in AI jargon. This cut through all of it. Instead of learning tools, I learned how to think about problems strategically. The ROI framework is what sold my leadership team. Worth every rupee.‚Äù\n\n‚Äî [Head of Product, VC-Backed SaaS]\n\nMore testimonials coming after January cohort concludes. Previous 1-on-1 students available for reference upon request.\n\n\nSecure Your Seat ‚Äì ‚Çπ24,999"
  },
  {
    "objectID": "cohort/index.html#your-investment-whats-included",
    "href": "cohort/index.html#your-investment-whats-included",
    "title": "Build Your Personal  AI Operating System",
    "section": "YOUR INVESTMENT & WHAT‚ÄôS INCLUDED",
    "text": "YOUR INVESTMENT & WHAT‚ÄôS INCLUDED\n\n\n\nThe Investment: ‚Çπ24,999 (January Cohort ‚Äì Founding Rate)\nNext cohort: ‚Çπ36,999. Price increases as demand grows.\n\n\n\nHere‚Äôs What‚Äôs Included:\nThe Live Experience:\n\n3 consecutive evening sessions (7 - 10 PM IST January 09-11, 2026)\n\nLive instruction, group breakouts, live Q&A\n\nAccess to recordings for 2 weeks post-cohort (lifetime access available if preferred)\n\nYour Assets (You Take These With You):\n\nReady-to-use Prompt Library (50+ prompts for AI problem evaluation)\n\nStrategy Framework Worksheets (AI ART matrix, O.L.C.D, ROI templates)\n\nLive Excel Tracker (ROI measurement sheet + use case roadmap template)\n\nDiagnostic Playbook (step-by-step decision framework)\n\nOngoing Access:\n\nPrivate Community (ask me questions anytime; get peer feedback)\n\n2x/month Group Sessions (forever) ‚Äì Check in on your progress, get clarity, refine strategy\n\nEmail Support (direct access for challenges you hit during implementation)\n\nNewsletter Subscription (stay updated on AI trends, new use cases, framework updates)\n\n\n\nYou‚Äôre not buying a course. You‚Äôre buying a thinking system + ongoing access to me for the next 90 days (and 2x/month after that)."
  },
  {
    "objectID": "cohort/index.html#the-guarantee-roi-or-your-money-back",
    "href": "cohort/index.html#the-guarantee-roi-or-your-money-back",
    "title": "Build Your Personal  AI Operating System",
    "section": "THE GUARANTEE: ROI OR YOUR MONEY BACK",
    "text": "THE GUARANTEE: ROI OR YOUR MONEY BACK\n\nMy Reputation Is Built On ROI, Not Views\n\nHere‚Äôs the deal:\nIf you complete the cohort and follow the system, and after 30 days you haven‚Äôt:\n\nMapped at least one viable AI use case with clear ROI (before/after scenario modeled), AND\nCreated a 30‚Äì90 day roadmap you can hand to your team or leadership,\n\nI‚Äôll refund you 100% of your investment AND pay you ‚Çπ2,599 directly for your time."
  },
  {
    "objectID": "cohort/index.html#common-questions",
    "href": "cohort/index.html#common-questions",
    "title": "Build Your Personal  AI Operating System",
    "section": "COMMON QUESTIONS",
    "text": "COMMON QUESTIONS\n\n\n\n\nDo I need to know how to code?\n\n\n\nNo.¬†The AI Profit OS is designed for senior professionals, not engineers. Everything is built in Excel + simple tools you already use. If you can use Google Sheets, you can build these systems.\n\n\n\n\n\nI‚Äôm not technical. Will I be able to follow?\n\n\n\nAbsolutely. This training is designed for business leaders, not data scientists. I‚Äôve trained CFOs, COOs, and Senior Directors with zero technical background. If you can manage a P&L, you can operationalize AI.\n\n\n\n\n\nWhat if I can‚Äôt attend all 3 days live?\n\n\n\nYou‚Äôll get lifetime recording access. However, the live workshops where you build YOUR specific systems are where the real value comes from. We strongly recommend attending live.\n\n\n\n\n\nIs this just ChatGPT training?\n\n\n\nNo.¬†ChatGPT is one tool in the AI Profit OS, but the system is much bigger. You‚Äôll learn: (a) How to structure your workflow for AI leverage, (b) How to build repeatable systems (not one-off prompts), (c) How to measure ROI and scale what works.\n\n\n\n\n\nWhat industries does this work for?\n\n\n\nIf you‚Äôre a senior professional doing knowledge work, this works. I‚Äôve deployed AI systems across automotive, retail, FMCG, telecommunications, finance, and healthcare. The frameworks are industry-agnostic.\n\n\n\n\n\nWhat‚Äôs the refund policy?\n\n\n\nThe ‚ÄúUnheard Of‚Äù Guarantee (see above). If you implement the system and don‚Äôt save 10+ hours/week within 30 days, refund + ‚Çπ5,000 paid to you. After 30 days, no refunds - but you can transfer your seat to a colleague.\n\n\n\n\n\nCan I bring my team?\n\n\n\nYes! We offer team pricing for 3+ participants from the same organization. Email jitin@jitinkapila.com for custom team rates. (Recommended: Bring 2-3 key leaders to accelerate adoption.)\n\n\n\n\n\nHow is this different from other AI courses?\n\n\n\nMost AI courses teach theory or simple prompting. The AI Profit OS is a complete operational system you‚Äôll deploy in your workflow. You‚Äôre not learning about AI - you‚Äôre building leverage. By Day 3, you have working systems saving you time.\n\n\n\n\n\nWhat if AI changes? Will this become outdated?\n\n\n\nThe OLCD Protocol, AI-ART Matrix, and operational frameworks are model-agnostic. Whether it‚Äôs ChatGPT, Claude, Gemini, or whatever comes next, the system adapts. Plus, you get lifetime access to framework updates.\n\n\n\n\n\nDo I need to be technical to do this?\n\n\n\nNo.¬†This entire cohort is designed for non-technical people. You won‚Äôt code. You won‚Äôt write prompts. You‚Äôll think strategically about where AI fits in your business and why.   The frameworks work whether you have a technical background or not. Your job is to architect. Your engineer‚Äôs job is to build.\n\n\n\n\n\nCan I attend if I‚Äôm new to AI?\n\n\n\nYes, as long as you have 10+ years of professional experience. You don‚Äôt need to know what ChatGPT is or how transformers work. You need to understand your business, your problems, and what good execution looks like.\n\n\n\n\n\nWhat if I can‚Äôt attend live? Can I watch recordings?\n\n\n\nPartially. Recordings are available for 2 weeks post-cohort so you can catch up on lectures. But the breakout room work and live Q&A are the most valuable part‚Äîthose happen during live sessions only.   If live attendance is an issue, reach out before you enroll. We can discuss options.\n\n\n\n\n\nCan I bring my team?\n\n\n\nNo.¬†This is designed for individual leaders. You become the AI strategist. You take this back to your team.   (If your team needs deep technical training on implementation, that‚Äôs a separate conversation‚Äîwe have resources for that.)\n\n\n\n\n\nIs this just ChatGPT and prompts?\n\n\n\nNo.¬†We don‚Äôt teach tools. We teach thinking. ChatGPT might be part of your solution, or it might not be. That depends on your use case.   What we do teach: How to think about problems strategically, how to evaluate which AI approaches fit, how to quantify ROI, and how to execute without hiring consultants.\n\n\n\n\n\nWhat happens after the 3 days?\n\n\n\nYou get: * Access to our community (ask questions, peer learning) * 2x/month group sessions with me (forever) ‚Äì we review your progress, refine strategy, unblock challenges * Email access (for urgent questions) You‚Äôre responsible for execution with your team. I‚Äôm your strategic advisor, not your implementer.\n\n\n\n\n\nWhat if my AI implementation doesn‚Äôt work?\n\n\n\nThat‚Äôs not a refund trigger. The refund is if you can‚Äôt map a viable use case or create a roadmap.   If you map something and it doesn‚Äôt work in execution, that‚Äôs implementation, not strategy. In that case, you have 2x/month sessions to debug. We pivot and adjust.\n\n\n\n\n\nHow is this different from other AI courses?\n\n\n\nMost courses teach you tools: Here‚Äôs ChatGPT. Here‚Äôs how to write prompts. Here‚Äôs how to use AI tools.   We teach you thinking: Here‚Äôs how to evaluate any problem. Here‚Äôs how to map it to a solution. Here‚Äôs how to quantify ROI so your CFO approves it.   Most courses are for beginners. This is for leaders who already understand business‚Äîyou just need the AI framework.\n\n\n\n\n\nIs this industry-agnostic?\n\n\n\nYes. AI is industry agnostic and we‚Äôve worked with leaders in retail, finance, consulting, healthcare, tech, manufacturing, logistics. The frameworks work everywhere because they‚Äôre based on problems, not industries.\n\n\n\n\n\nWhat if I implement something and it doesn‚Äôt hit the ROI I modeled?\n\n\n\nThat‚Äôs real life. Models are approximations. But here‚Äôs what matters: You‚Äôll have a diagnostic framework. If results are worse than expected, you‚Äôll know how to ask the right questions to debug it.   That‚Äôs what the 2x/month sessions are for.\n\n\n\n\n\nDo I get a certificate?\n\n\n\nNo.¬†Certificates are for validation. We focus on competence. After this cohort, you‚Äôll be able to architect AI use cases for your business. That‚Äôs the real credential.\n\n\n\n\n\nWhat if I decide this isn‚Äôt right for me after I enroll?\n\n\n\nYou have a 7-day money-back guarantee. If you enroll and change your mind, get a full refund within 7 days. No questions asked.   (The ROI guarantee kicks in after you complete the cohort, as detailed above.)."
  },
  {
    "objectID": "cohort/index.html#ready-to-architect-your-ai-strategy",
    "href": "cohort/index.html#ready-to-architect-your-ai-strategy",
    "title": "Build Your Personal  AI Operating System",
    "section": "READY TO ARCHITECT YOUR AI STRATEGY?",
    "text": "READY TO ARCHITECT YOUR AI STRATEGY?\n\nIn 3 days, you‚Äôll know: - Which AI opportunities matter in your business - How to evaluate them rigorously - What the ROI actually looks like - Who should own execution - How to present it to leadership\nIn 30‚Äì90 days, your team will ship the first one.\nNo consultant. No waiting. No confusion.\nJanuary 09-11, 2026. Limited to 50 leaders per cohort.\n\nSecure Your Seat ‚Äì ‚Çπ24,999\nNext cohort: ‚Çπ36,999. Invest early."
  },
  {
    "objectID": "cohort/index.html#still-on-the-fence",
    "href": "cohort/index.html#still-on-the-fence",
    "title": "Build Your Personal  AI Operating System",
    "section": "Still On The Fence?",
    "text": "Still On The Fence?\nHave a question before you enroll?\n\n\nBook a 15-min call with me | Email me directly\n\nWe‚Äôll make sure this is the right fit for you.\n\n\n\n\nI‚Äôm not sure this will work for my specific industry/role.\n\n\n\nThat‚Äôs exactly why Day 1 is focused on YOUR workflow and YOUR 90-day roadmap. The frameworks are industry-agnostic, but the implementation is personalized to your reality.\n\n\n\n\n\n feels expensive for a training.\n\n\n\nCompare it to: - (a) One hour of my consulting at ‚Çπ25,000/hour, -(b) The cost of being replaced by a 22-year-old who knows how to use AI, -(c) The ‚Çπ6.9Cr the grocery chain saved with this exact system. If you don‚Äôt save 10+ hours/week, I‚Äôll refund you AND pay you ‚Çπ8,000.\n\n\n\n\n\nWhat if I can‚Äôt implement this alone?\n\n\n\nYou won‚Äôt be alone. You‚Äôll join a private community of senior professionals building AI systems. Plus monthly office hours with me. And if you need hands-on implementation support, 1:1 consulting is available.\n\n\n\n\n\nHow do I know this isn‚Äôt just hype?\n\n\n\nLook at the results: ‚Çπ6.9Cr grocery chain savings. $11M automotive optimization. ¬£80K/month retail revenue lift. These are real deployments at Mercedes, L‚ÄôOr√©al, Maruti, British Telecom. This isn‚Äôt theory - it‚Äôs operational leverage.\n\n\n\n\n\nWhat if AI replaces me anyway?\n\n\n\nAI will replace professionals who DON‚ÄôT learn to operationalize it. Senior professionals who build AI systems become 10x more valuable. You‚Äôre not competing with AI - you‚Äôre building leverage ON TOP of your 10+ years of expertise."
  },
  {
    "objectID": "blog/strategy/decision-first-ai/index.html",
    "href": "blog/strategy/decision-first-ai/index.html",
    "title": "Decision-First AI: Why Data Should Follow, Not Lead",
    "section": "",
    "text": "Start here: don‚Äôt open a notebook until you know the decision you want to change.\nSounds obvious. But most AI projects don‚Äôt start there. They start with a dataset, or with a ‚Äúlet‚Äôs try this model,‚Äù or with a platform demo that looks great in the cloud. And then weeks later the obvious question appears: ‚ÄúOkay ‚Äî what decision does this support?‚Äù People shrug. The project stalls. The models are good. The business impact is vague.\nThis is the dataset-first trap. It wastes time, money, and faith. It also gives AI a bad name.\nI‚Äôve seen the opposite work ‚Äî a lot. Start with the decision. Map the decision. Then pick the simplest data and model that make the decision better. The result? Faster pilots, clearer ROI, and systems that actually get used.\nThat approach is not just a management neat-idea. There‚Äôs a real body of research showing that aligning models to downstream decision goals yields better decisions than optimizing prediction accuracy alone. And there are real, practical wins ‚Äî from telecom fault detection to inventory systems ‚Äî when you flip the order. arXiv+1"
  },
  {
    "objectID": "blog/strategy/decision-first-ai/index.html#the-dataset-first-trap-what-it-looks-like-and-why-it-hurts",
    "href": "blog/strategy/decision-first-ai/index.html#the-dataset-first-trap-what-it-looks-like-and-why-it-hurts",
    "title": "Decision-First AI: Why Data Should Follow, Not Lead",
    "section": "The ‚Äúdataset-first‚Äù trap ‚Äî what it looks like, and why it hurts",
    "text": "The ‚Äúdataset-first‚Äù trap ‚Äî what it looks like, and why it hurts\nHere‚Äôs the typical playbook I see in companies:\n\nSomeone discovers a new dataset.\nThey build dashboards, then a model, then a fine model, then a fancier model.\nThey show a demo. The demo gets applause. Then the work hits integration, governance, and the messy reality of people who must make decisions every day. The model‚Äôs outputs don‚Äôt map to a decision process. So adoption fails.\n\nWhy? Because the project optimized the wrong thing. It optimized prediction metrics ‚Äî error, F1, AUC, MAPE. And those are useful. But they‚Äôre not the measure of business impact. A model with better accuracy can still be useless if it doesn‚Äôt change what someone does.\nHarvard Business Review captured this idea well: decisions don‚Äôt start with data. They start with a problem, a role, a process, and a behavior. If your analytics don‚Äôt connect to that reality, you get slides and disappointment. Harvard Business Review\nThere are more subtle costs too. Dataset-first projects often create models that are brittle in production: they overfit to historical quirks, they require constant data wrangling, and they produce numbers no one trusts. That kills scale."
  },
  {
    "objectID": "blog/strategy/decision-first-ai/index.html#decision-first-in-research-not-new-but-finally-practical",
    "href": "blog/strategy/decision-first-ai/index.html#decision-first-in-research-not-new-but-finally-practical",
    "title": "Decision-First AI: Why Data Should Follow, Not Lead",
    "section": "Decision-first in research: not new, but finally practical",
    "text": "Decision-first in research: not new, but finally practical\nThere‚Äôs academic grounding for starting with decisions. In the machine-learning community this shows up as ‚Äúdecision-focused learning‚Äù or ‚Äúsmart predict-then-optimize.‚Äù The idea: train predictive models not for pure accuracy, but to minimize the loss that matters to the downstream optimization or decision task. When you optimize directly for the decision loss, you often get better business outcomes ‚Äî even with ‚Äúworse‚Äù prediction metrics. arXiv+1\nRecent papers and reviews show both the theory and practical methods: surrogate losses that reflect decision outcomes, techniques to differentiate through optimization, and heuristics for discrete problems. The takeaway: the math supports the intuition. If you want a model to help choose inventory levels, price points, or routing, train it with that decision in mind ‚Äî not just with RMSE. Optimization Online+1\nThat doesn‚Äôt mean every model must be complex. Often the opposite. Framing the decision reduces model complexity because you only model what matters. This is your Occam‚Äôs Razor"
  },
  {
    "objectID": "blog/strategy/decision-first-ai/index.html#the-decision-map-simple-framework-you-can-use-today",
    "href": "blog/strategy/decision-first-ai/index.html#the-decision-map-simple-framework-you-can-use-today",
    "title": "Decision-First AI: Why Data Should Follow, Not Lead",
    "section": "The Decision Map: simple framework you can use today",
    "text": "The Decision Map: simple framework you can use today\nIf you want to flip to decision-first, start with a small, disciplined tool I call a Decision Map. It‚Äôs a one-page artefact. Build it before you touch data.\nHere‚Äôs the Decision Map ‚Äî six steps. Do them in order.\n\nName the decision. Who decides, how often, and what options do they choose? Example: ‚ÄúField-ops decides whether to dispatch a technician to a suspected DSL fault.‚Äù Be specific. Frequency matters ‚Äî hourly, daily, weekly change what you can do.\nDefine the decision metric(s).What counts as success? Lower cost? Faster response time? Increase in net revenue? Pick one primary metric and one secondary. If you can‚Äôt name it in a single measurable sentence, you don‚Äôt have a decision.\nMap the current process. Where is the decision made today? Which people and tools are involved? Where does data enter? Where do delays happen? This step exposes the friction you must remove.\nIdentify the minimal action the model must trigger. The model doesn‚Äôt need to be perfect. It needs to change behavior. If the model‚Äôs output is a probability, what threshold triggers action? Who gets the alert? What‚Äôs the handoff?\nList the minimal signals (data) needed. Only include data that directly reduces uncertainty for the decision. You‚Äôll be surprised how small this list often is. Think: signal ‚Üí action. Not ‚Äúall the data.‚Äù\nPlan the feedback loop. How will you measure the decision metric after deployment? How will you collect labels and iterate? Decide that upfront.\n\nIf you complete this map, you‚Äôll have done 80% of the work most teams skip. It forces alignment, and it reveals whether the project is worth doing."
  },
  {
    "objectID": "blog/strategy/decision-first-ai/index.html#a-telecom-example-mapped-end-to-end-real-story",
    "href": "blog/strategy/decision-first-ai/index.html#a-telecom-example-mapped-end-to-end-real-story",
    "title": "Decision-First AI: Why Data Should Follow, Not Lead",
    "section": "A telecom example, mapped end-to-end (real story)",
    "text": "A telecom example, mapped end-to-end (real story)\nA brief, real example matters. I built a real-time anomaly detection system for a European telecom client early in my career. The project didn‚Äôt begin with ‚Äúwe have logs.‚Äù It began with this decision map:\n\nDecision: Should operations dispatch a field technician proactively for a suspected DSL fault?\nMetric: Reduce customer reported faults and improve Net Promoter Score (NPS) by reducing time-to-detect. Also: monthly cost savings from fewer reactive truck rolls and more planned truck rolls.\nProcess: Operations received the customer call, created a ticket, and dispatched if needed ‚Äî often hours or days later. That was slow and expensive.\n\nAction: If the system flags an anomaly with high confidence\nRed : Very high probability in next 48 hours,\nAmber: probability in next 2-15 days\nGreen: No visibility of error in next 15 days This creates a high-priority ticket and dispatch a remote check or technician and was planned in region wise manner.\n\nSignals: DSL line metrics, error rates, device telemetry, event logs ‚Äî a handful of streams, not every log.\nFeedback: Compare flagged incidents to customer complaints and adjust thresholds.\n\nBecause the decision was so clear, we could measure value before full scale. The pilot cut detection time from days to hours, raised customer satisfaction significantly ( We sent messages to possible signal disruption early), and saved ~¬£80K per month ( because reducing complete breakdown to DSL by heat or so and by planning the route than going everywhere) . It wasn‚Äôt an exotic model (or may be it was, tech details some other day); it was a tightly scoped system that informed a clear action.\nNotice how this maps to the Decision-First steps. The model existed to change a single operational choice. That focus made deployment possible and measurable fast."
  },
  {
    "objectID": "blog/strategy/decision-first-ai/index.html#another-quick-example-inventory-decisions",
    "href": "blog/strategy/decision-first-ai/index.html#another-quick-example-inventory-decisions",
    "title": "Decision-First AI: Why Data Should Follow, Not Lead",
    "section": "Another quick example: inventory decisions",
    "text": "Another quick example: inventory decisions\nInventory forecasting is a classic area where decision-first matters. You can chase lower MAPE and never change stocking policy. Or you can ask: what decision do merchandisers make with this forecast? When you frame it as ‚Äúwhich SKUs do we order for next week, and what reorder points trigger expedited shipments,‚Äù you design the forecast differently: shorter horizons, bias for understock on fast movers, and direct constraints on reorder costs.\nI‚Äôve led projects that delivered $11M in inventory optimization by building forecasts and decision rules that match merchant behavior and supply constraints. The trick was not better models ‚Äî it was framing forecasts so the merchandisers could act with confidence."
  },
  {
    "objectID": "blog/strategy/decision-first-ai/index.html#practical-tips-for-teams-do-this-in-week-one",
    "href": "blog/strategy/decision-first-ai/index.html#practical-tips-for-teams-do-this-in-week-one",
    "title": "Decision-First AI: Why Data Should Follow, Not Lead",
    "section": "Practical tips for teams (do this in week one)",
    "text": "Practical tips for teams (do this in week one)\n\nRun a one-hour Decision Map workshop. Invite the decision owner, one operator, one engineer, and one product owner. Build the one-page map. If the owner can‚Äôt commit to a metric, pause the project.\nStart with a simple rule baseline. Before modeling, define a rule that will be your baseline (e.g., ‚Äúif X &gt; T, create ticket‚Äù). If the model can‚Äôt beat that rule in decision impact, scrap it.\nMeasure decision impact, not model accuracy. Your dashboard should show business metric delta ‚Äî not just RMSE. If you show the board a change in cost or conversion, you‚Äôll get attention.\nPrioritize deployment constraints. Decide telemetry, latency, and handoff requirements first. Models that can‚Äôt meet latency or trust constraints are useless no matter how accurate.\nIterate with real feedback. Don‚Äôt wait for ‚Äúperfect.‚Äù Ship an MVP that can be measured, then refine. Real decisions provide labels and operational learning."
  },
  {
    "objectID": "blog/strategy/decision-first-ai/index.html#common-objections-and-how-to-handle-them",
    "href": "blog/strategy/decision-first-ai/index.html#common-objections-and-how-to-handle-them",
    "title": "Decision-First AI: Why Data Should Follow, Not Lead",
    "section": "Common objections and how to handle them",
    "text": "Common objections and how to handle them\n‚ÄúBut we don‚Äôt have a clear decision owner.‚Äù Then don‚Äôt build a model. Decisions live in roles. Pull the right owner in early, or you‚Äôll build for nobody.\n‚ÄúOur data is messy.‚Äù Fine. If you can define the minimal signals, you can often create a proxy or start with manual labels. Messy data is easier to handle when you only need a few signals for a specific decision.\n‚ÄúWe need predictions for many uses.‚Äù Build a simple decision-first pilot first. Use its success to fund broader platform work. Pilots create proof that unlocks investment.\n‚ÄúDecision-focused methods are academic ‚Äî too hard.‚Äù There‚Äôs truth and myth here. The academic techniques show big wins when decision loss can be written down. But you don‚Äôt need complicated differentiable optimization to start. Use a decision map, simple thresholds, A/B tests, and iterative measurement. Graduate to decision-focused training once you have a stable objective. The research just tells us ‚Äî unsurprisingly ‚Äî that when you train with the decision in mind, outcomes improve. Optimization Online+1"
  },
  {
    "objectID": "blog/strategy/decision-first-ai/index.html#one-page-checklist-copy-this",
    "href": "blog/strategy/decision-first-ai/index.html#one-page-checklist-copy-this",
    "title": "Decision-First AI: Why Data Should Follow, Not Lead",
    "section": "One-page checklist (copy this)",
    "text": "One-page checklist (copy this)\n\nDecision name (The GOTO problem statement): ____________________\nPrimary / Secondary metric (which can accounted for ROI calculation later ): _____\nDecision owner (I don‚Äôt want to debate on this): ________________________\nFrequency of predictions: real-time / hourly / daily / weekly / monthly\nAction / interventions which can be triggered by model: ____________________\nMinimal signals required( The core Data to begin with): ______________________\nBaseline rule (your fail-safe if everything goes wrong): ____________________\nFeedback source ( might be tricky, but you should have one): __________________\n\nIf you can fill this in, you‚Äôre set for a pilot."
  },
  {
    "objectID": "blog/strategy/decision-first-ai/index.html#final-note-start-small-measure-fast-then-scale-with-discipline",
    "href": "blog/strategy/decision-first-ai/index.html#final-note-start-small-measure-fast-then-scale-with-discipline",
    "title": "Decision-First AI: Why Data Should Follow, Not Lead",
    "section": "Final note ‚Äî start small, measure fast, then scale with discipline",
    "text": "Final note ‚Äî start small, measure fast, then scale with discipline\nThe decision-first approach is simple because business problems are simple when stated well. The hard part is discipline: saying no to shiny demos and yes to measurable change. Start with one decision that matters. Map it. Ship a small system that changes behavior. Measure the business metric. Iterate.\nResearch supports this: models trained with the decision in mind perform better on the actual outcomes you care about. And in practice, teams that flip the order ‚Äî decision first, data second ‚Äî get to value faster. arXiv+1\nIf you want help mapping a decision in your company, send me one line describing the decision and the current process. I‚Äôll reply with the Decision Map template you can use in a one-hour workshop. Or if you want we can connect too.\nIf this post help you or think help someone in need, please do share it. Thanks!!!"
  },
  {
    "objectID": "blog/strategy/decision-first-ai/index.html#key-sources-further-reading",
    "href": "blog/strategy/decision-first-ai/index.html#key-sources-further-reading",
    "title": "Decision-First AI: Why Data Should Follow, Not Lead",
    "section": "Key sources & further reading",
    "text": "Key sources & further reading\n\nElmachtoub, A.N., & Grigas, P. ‚Äî Smart ‚ÄúPredict, then Optimize‚Äù (foundational paper on decision-focused loss and SPO). arXiv\nReviews and recent work on decision-focused learning (predict-and-optimize / decision-focused methods). arXiv+1\nHarvard Business Review ‚Äî Decisions Don‚Äôt Start with Data ‚Äî on why framing the decision matters. Harvard Business Review\n\n(And ‚Äî the telecom and inventory examples referenced above are from projects on my profile/resume: Ask, if you want more details !! )"
  },
  {
    "objectID": "blog/engineering/10_treeknn/index.html",
    "href": "blog/engineering/10_treeknn/index.html",
    "title": "TrieKNN: Unleashing KNN‚Äôs Power on Mixed Data Types",
    "section": "",
    "text": "Photo by Gelgas Airlangga"
  },
  {
    "objectID": "blog/engineering/10_treeknn/index.html#the-allure-and-limitation-of-knn",
    "href": "blog/engineering/10_treeknn/index.html#the-allure-and-limitation-of-knn",
    "title": "TrieKNN: Unleashing KNN‚Äôs Power on Mixed Data Types",
    "section": "The Allure and Limitation of KNN",
    "text": "The Allure and Limitation of KNN\nIn the realm of machine learning, the K-Nearest Neighbors (KNN) algorithm stands out for its intuitive nature and ease of implementation. Its principle is simple: classify a data point based on the majority class among its ‚Äòk‚Äô nearest neighbors in the feature space. This non-parametric approach makes no assumptions about the underlying data distribution, rendering it versatile for various applications. KNN is very popular, but it comes with some limitations.\nHowever, KNN‚Äôs Achilles‚Äô heel lies in its reliance on distance metrics, which are inherently designed for numerical data. Real-world datasets often contain a mix of numerical and categorical features, posing a significant challenge for KNN. How do you measure the distance between ‚Äòred‚Äô and ‚Äòblue,‚Äô or ‚Äòlarge‚Äô and ‚Äòsmall‚Äô?\n\nPrior Art\nSeveral strategies have been proposed to adapt KNN for mixed data:\n\nOne-Hot Encoding: Converts categorical features into numerical vectors, but can lead to high dimensionality.\nDistance Functions for Mixed Data: Develops and apply custom distance metrics that can handle both numerical and categorical features such as HEOM and many others.\nUsing mean/mode values: Replace the missing values with mean/mode.\n\nThese methods often involve compromises, either distorting the data‚Äôs inherent structure or adding computational overhead."
  },
  {
    "objectID": "blog/engineering/10_treeknn/index.html#enter-trieknn-a-novel-approach",
    "href": "blog/engineering/10_treeknn/index.html#enter-trieknn-a-novel-approach",
    "title": "TrieKNN: Unleashing KNN‚Äôs Power on Mixed Data Types",
    "section": "Enter TrieKNN: A Novel Approach",
    "text": "Enter TrieKNN: A Novel Approach\nWhat if we could cleverly sidestep the distance calculation problem for categorical features, while still leveraging KNN‚Äôs power? TrieKNN offers just that‚Äîa way to perform KNN on any mixed data!\nTrieKNN combines the strengths of Trie data structures and KNN to handle mixed data types gracefully. Here‚Äôs the core idea:\n\nTrie-Based Categorical Encoding: A Trie is used to store the categorical features of the data. Each node in the Trie represents a category.\nLeaf-Node KNN Models: At the leaf nodes of the Trie, where specific combinations of categorical features are found, we fit individual KNN models using only the numerical features.\nWeighted Prediction: To classify a new data point, we traverse the Trie based on its categorical features. At each level, we calculate a weighted distance based on available data, ending in a probability score in each leaf node.\n\n\nWhy This Works\n\nNo Direct Distance Calculation for Categorical Features: The Trie structure implicitly captures the relationships between categorical values.\nLocalized KNN Models: By fitting KNN models at the leaf nodes, we ensure that distance calculations are performed only on relevant numerical features.\nScalability: The Trie structure efficiently handles a large number of categorical features and values."
  },
  {
    "objectID": "blog/engineering/10_treeknn/index.html#building-a-trieknn-model",
    "href": "blog/engineering/10_treeknn/index.html#building-a-trieknn-model",
    "title": "TrieKNN: Unleashing KNN‚Äôs Power on Mixed Data Types",
    "section": "Building a TrieKNN Model",
    "text": "Building a TrieKNN Model\nLet‚Äôs dive into the implementation. We‚Äôll start with the TrieNode and Trie classes, then move on to the KNN model and the training/prediction process.\n\nTrie Implementation\n\n\nCode\nimport numpy as np\nfrom collections import Counter\n\nclass TrieNode:\n    def __init__(self):\n        self.children = {}  # Dictionary to store child nodes\n        self.is_end_of_word = False  # True if the node is the end of a word\n        self.count = 0  # Count of how many times a word has been inserted\n        self.class_counts = {}  # Class counts\n        self.class_weights = {}\n        self.model = None  # Model at leaf nodes\n        self.indexes = []  # Store data indexes belonging to this leaf\n        self.labels = []  # Store data indexes belonging to this leaf\n        self.node_weight = None\n\nclass Trie:\n    def __init__(self):\n        self.root = TrieNode()  # Root node of the Trie\n        self.data_index = 0  # Initialize data index\n\n    def insert(self, word_val, model):\n        current_node = self.root\n        word, val = word_val\n        current_node.count += 1\n\n        # Adding class counts\n        if val not in current_node.class_counts:\n            current_node.class_counts[val] = 0\n        current_node.class_counts[val] += 1\n\n        for char in word:\n            # If the character is not in children, add a new TrieNode\n            if char not in current_node.children:\n                current_node.children[char] = TrieNode()\n            current_node = current_node.children[char]\n\n            # Adding count of instances\n            current_node.count += 1\n\n            # adding class counts\n            if val not in current_node.class_counts:\n                current_node.class_counts[val] = 0\n            current_node.class_counts[val] += 1\n\n        # Mark the end of the word and increment count\n        current_node.is_end_of_word = True\n        current_node.indexes.append(self.data_index)  # Store the data index\n        current_node.labels.append(val)\n        current_node.model = model\n        self.data_index += 1  # Increment data index\n\n    def search(self, word):\n        current_node = self.root\n        for char in word:\n            # If the character doesn't exist in the children, the word doesn't exist\n            if char not in current_node.children:\n                return False\n            current_node = current_node.children[char]\n\n        # Return True if it's the end of a word and the word exists\n        return current_node.is_end_of_word\n\n    def count_word(self, word):\n        current_node = self.root\n        for char in word:\n            # If the character doesn't exist, the word doesn't exist\n            if char not in current_node.children:\n                return 0, current_node.class_counts  # Correctly return class_counts\n            current_node = current_node.children[char]\n\n        # Return the count of the word\n        return current_node.count, current_node.class_counts\n\n    def display(self):\n        # Recursively display the tree\n        def _display(node, word):\n            if node.is_end_of_word:\n                print(f\"Data: {word}, Count: {node.count}, Indexes: {len(node.indexes)} Classes :{node.class_counts} weights:{len(node.class_weights)}\")  # Display indexes too\n            for char, child in node.children.items():\n                _display(child, word + char)  # corrected the display\n\n        _display(self.root, \"\")\n\n    def apply(self, func):\n        \"\"\"\n        Applies a function to all models in the leaf nodes.\n        \"\"\"\n        def _apply(node):\n            if node.is_end_of_word and node.model is not None:\n                func(node)\n            for child in node.children.values():\n                _apply(child)\n\n        _apply(self.root)\n\n    def apply_weight_to_indexes(self, weight):\n        \"\"\"\n        Applies a weight to the indexes based on the percentage of data available.\n        \"\"\"\n        def _apply_weight_to_indexes(node):\n            if node.is_end_of_word:\n                total_count = sum(self.root.children[child].count for child in self.root.children)\n                percentage = node.count / total_count if total_count &gt; 0 else 0\n                weighted_indexes = [(index, weight * percentage) for index in node.indexes]\n                node.class_weights = weighted_indexes  # Corrected this line\n            for child in node.children.values():\n                _apply_weight_to_indexes(child)\n\n        _apply_weight_to_indexes(self.root)\n\n\n\n\nKNN Model\n\n\nCode\nclass KNNModel:\n    def __init__(self, k=5):\n        self.data = None\n        self.labels = []\n        self.k = k\n\n    def fit(self, data, indexes, labels):\n        # print(\"Fitting model with indexes:\", len(indexes), \"labels:\", len(labels))\n        self.data = data[indexes].astype(float)\n        self.labels = np.array(labels).astype(float)\n\n    def predict(self, data):\n        # print(\"Predicting with data:\", data)\n        dist_ind = np.sqrt(np.sum((self.data - data) ** 2, axis=1) ** 2)  # euclidean distance\n        main_arr = np.column_stack((self.labels, dist_ind))  # labels with distance\n        main = main_arr[main_arr[:, 1].argsort()]  # sorting based on distance\n        count = Counter(main[0:self.k, 0])  # counting labels\n        sums = np.array(list(count.values()))  # getting counts\n        return sums / np.sum(sums)  # prediction as probability\n\n\n\n\nTraining and Evaluation\nHere‚Äôs how we train and evaluate the TrieKNN model:\n\n\nCode\n# Sample data\nn = 10000\ndata = np.array((np.random.choice(['Anything ', 'By ','Chance '], p=[0.6,0.1,0.3],size=n),\n                 np.random.choice(['can', 'go', 'here','lets', 'see', \"it\"], p=[0.1, 0.1, 0.1, 0.2, 0.4, 0.1], size=n),\n                 np.random.normal(3, 1, size=n),\n                 np.random.normal(5, 2, size=n))).T\ny_label = np.random.choice([0,1], p=[0.7, 0.3], size=n)\n\n# Trie training\ntrie = Trie()\nfor X, y in zip(data, y_label):\n    trie.insert((X[:2], y),None)\n\n# Apply weights to indexes\ntrie.apply_weight_to_indexes(0.5)\n\n# Fit models of leaf nodes\ndef add_model(node, data):\n    node.model = KNNModel()\n    node.model.fit(data, node.indexes, node.labels)\n\ndef traverse_and_add_model(node, data):\n    if node.is_end_of_word:\n        add_model(node, data)  # Add model to leaf node\n    for child in node.children.values():\n        traverse_and_add_model(child, data)\n\ntraverse_and_add_model(trie.root, data[:, 2:])\n\n\n\n\nExplanation\n\nWe create sample data with mixed categorical and numerical features.\nWe insert each data point into the Trie, using the categorical features as the path.\nAfter the Trie is built, we traverse it and fit a KNN model to the data points stored at each leaf node.\nFinally, we can predict the class of new data points by traversing the Trie and using the KNN model at the corresponding leaf node."
  },
  {
    "objectID": "blog/engineering/10_treeknn/index.html#results-and-discussion",
    "href": "blog/engineering/10_treeknn/index.html#results-and-discussion",
    "title": "TrieKNN: Unleashing KNN‚Äôs Power on Mixed Data Types",
    "section": "Results and Discussion",
    "text": "Results and Discussion\nLet us display the trie.\n\n\n\n\nCode\ntrie.display()\n\n\nData: Anything lets, Count: 1209, Indexes: 1209 Classes :{np.int64(0): 832, np.int64(1): 377} weights:1209\nData: Anything here, Count: 636, Indexes: 636 Classes :{np.int64(0): 432, np.int64(1): 204} weights:636\nData: Anything can, Count: 666, Indexes: 666 Classes :{np.int64(0): 455, np.int64(1): 211} weights:666\nData: Anything go, Count: 585, Indexes: 585 Classes :{np.int64(0): 407, np.int64(1): 178} weights:585\nData: Anything see, Count: 2363, Indexes: 2363 Classes :{np.int64(0): 1637, np.int64(1): 726} weights:2363\nData: Anything it, Count: 571, Indexes: 571 Classes :{np.int64(0): 410, np.int64(1): 161} weights:571\nData: Chance see, Count: 1210, Indexes: 1210 Classes :{np.int64(1): 372, np.int64(0): 838} weights:1210\nData: Chance lets, Count: 599, Indexes: 599 Classes :{np.int64(1): 187, np.int64(0): 412} weights:599\nData: Chance go, Count: 270, Indexes: 270 Classes :{np.int64(0): 176, np.int64(1): 94} weights:270\nData: Chance here, Count: 278, Indexes: 278 Classes :{np.int64(0): 200, np.int64(1): 78} weights:278\nData: Chance it, Count: 316, Indexes: 316 Classes :{np.int64(0): 213, np.int64(1): 103} weights:316\nData: Chance can, Count: 309, Indexes: 309 Classes :{np.int64(0): 228, np.int64(1): 81} weights:309\nData: By can, Count: 76, Indexes: 76 Classes :{np.int64(0): 45, np.int64(1): 31} weights:76\nData: By see, Count: 397, Indexes: 397 Classes :{np.int64(1): 128, np.int64(0): 269} weights:397\nData: By here, Count: 107, Indexes: 107 Classes :{np.int64(1): 43, np.int64(0): 64} weights:107\nData: By lets, Count: 226, Indexes: 226 Classes :{np.int64(1): 77, np.int64(0): 149} weights:226\nData: By it, Count: 89, Indexes: 89 Classes :{np.int64(0): 55, np.int64(1): 34} weights:89\nData: By go, Count: 93, Indexes: 93 Classes :{np.int64(0): 64, np.int64(1): 29} weights:93\n\n\nThe model predicted the following values:\n\n\n\n\nCode\n# Prediction example\ndef predict_with_model(node):\n    predictions = node.model.predict(np.array([2,5]))\n    print(\"Predictions:\", predictions)\n\ntrie.apply(predict_with_model)\n\n\nPredictions: [0.4 0.6]\nPredictions: [0.6 0.4]\nPredictions: [0.4 0.6]\nPredictions: [0.8 0.2]\nPredictions: [0.4 0.6]\nPredictions: [0.8 0.2]\nPredictions: [0.4 0.6]\nPredictions: [0.6 0.4]\nPredictions: [0.2 0.8]\nPredictions: [0.4 0.6]\nPredictions: [0.8 0.2]\nPredictions: [0.4 0.6]\nPredictions: [0.4 0.6]\nPredictions: [0.6 0.4]\nPredictions: [1.]\nPredictions: [0.4 0.6]\nPredictions: [0.6 0.4]\nPredictions: [0.6 0.4]\n\n\nThe predictions will vary on each run. From this we can see that we can use KNN on mixed data types."
  },
  {
    "objectID": "blog/engineering/10_treeknn/index.html#conclusion-a-promising-path-forward",
    "href": "blog/engineering/10_treeknn/index.html#conclusion-a-promising-path-forward",
    "title": "TrieKNN: Unleashing KNN‚Äôs Power on Mixed Data Types",
    "section": "Conclusion: A Promising Path Forward",
    "text": "Conclusion: A Promising Path Forward\nTrieKNN presents a compelling solution for extending the applicability of KNN to mixed data types. By leveraging the Trie data structure, it avoids direct distance calculations on categorical features, enabling the use of localized KNN models for numerical data.\nFurther research could explore:\n\nOptimizing the weighting scheme for combining predictions from different Trie levels.\nComparing TrieKNN‚Äôs performance against other mixed-data KNN approaches on benchmark datasets.\nExtending TrieKNN to handle missing data and noisy categorical features.\n\nTrieKNN opens up new possibilities for applying KNN in domains where mixed data types are prevalent, such as healthcare, e-commerce, and social science.\nResources and further reads:\n1. Nomclust R package\n2. An Improved kNN Based on Class Contribution and Feature Weighting\n3. An Improved Weighted KNN Algorithm for Imbalanced Data Classification\n4. A weighting approach for KNN classifier\n5. Unsupervised Outlier Detection for Mixed-Valued Dataset Based on the Adaptive k-Nearest Neighbor Global Network\n6. A hybrid approach based on k-nearest neighbors and decision tree for software fault prediction\n7. Analysis of Decision Tree and K-Nearest Neighbor Algorithm in the Classification of Breast Cancer"
  },
  {
    "objectID": "blog/engineering/03_crosstab_sparsity/index.html",
    "href": "blog/engineering/03_crosstab_sparsity/index.html",
    "title": "CrossTab Sparsity",
    "section": "",
    "text": "Cluster analysis has always fascinated me as a window into the hidden structures of data. During my collaboration with Kumarjit Pathak, we grappled with a persistent challenge in unsupervised learning: how to objectively evaluate clustering quality across different algorithms. Traditional metrics like the Silhouette Index or Bayesian Information Criterion felt restrictive‚Äîthey were siloed within specific methodologies, making cross-algorithm comparisons unreliable.\nThis frustration led us to develop a universal cluster evaluation metric, detailed in our paper ‚ÄúCross Comparison of Results from Different Clustering Approaches‚Äù. Our goal was to create a framework that transcends algorithmic biases, enabling:\n- Direct comparison of K-Means vs GMM vs DBSCAN vs PAM vs SOM vs Anything results\n- Identification of variables muddying cluster separation\n- Automated determination of optimal cluster counts\nIn this blog, I‚Äôll walk you through our journey‚Äîfrom conceptualization to real-world validation‚Äîand share insights that didn‚Äôt make it into the final paper."
  },
  {
    "objectID": "blog/engineering/03_crosstab_sparsity/index.html#introduction",
    "href": "blog/engineering/03_crosstab_sparsity/index.html#introduction",
    "title": "CrossTab Sparsity",
    "section": "",
    "text": "Cluster analysis has always fascinated me as a window into the hidden structures of data. During my collaboration with Kumarjit Pathak, we grappled with a persistent challenge in unsupervised learning: how to objectively evaluate clustering quality across different algorithms. Traditional metrics like the Silhouette Index or Bayesian Information Criterion felt restrictive‚Äîthey were siloed within specific methodologies, making cross-algorithm comparisons unreliable.\nThis frustration led us to develop a universal cluster evaluation metric, detailed in our paper ‚ÄúCross Comparison of Results from Different Clustering Approaches‚Äù. Our goal was to create a framework that transcends algorithmic biases, enabling:\n- Direct comparison of K-Means vs GMM vs DBSCAN vs PAM vs SOM vs Anything results\n- Identification of variables muddying cluster separation\n- Automated determination of optimal cluster counts\nIn this blog, I‚Äôll walk you through our journey‚Äîfrom conceptualization to real-world validation‚Äîand share insights that didn‚Äôt make it into the final paper."
  },
  {
    "objectID": "blog/engineering/03_crosstab_sparsity/index.html#the-birth-of-the-metric-a-first-person-perspective",
    "href": "blog/engineering/03_crosstab_sparsity/index.html#the-birth-of-the-metric-a-first-person-perspective",
    "title": "CrossTab Sparsity",
    "section": "The Birth of the Metric: A First-Person Perspective",
    "text": "The Birth of the Metric: A First-Person Perspective\nWhy Existing Methods Fell Short\nEarly in our research, we cataloged limitations of popular evaluation techniques:\n\nMethod Dependency\n\n\n\nSilhouette scores worked beautifully for K-Means but faltered with Gaussian Mixture Models (GMM).\n\nProbability-based metrics like BIC couldn‚Äôt handle distance-based clusters.\n\n\nNoise Blindness\nNoisy variables often contaminated clusters, but traditional methods required manual outlier detection.\nSubjective Optimization* Elbow plots and dendrograms left too much room for human interpretation.\n\n\nOur ‚ÄúAha!‚Äù Moment - Crosstab Sparsity\n\n\n\n\nBest Cluster for K-means Using Crosstab sparsity\n\n\n\nWhile analyzing cross-tab matrices of variable distributions across clusters, we noticed a pattern: well-segregated clusters consistently showed higher frequencies along matrix diagonals. This inspired our two-part metric:\n\n\n\nSegregation Factor:\n# Simplified calculation from our codebase  \nmedian = np.median(cross_tab)  \nN_vk = np.sum(cross_tab &gt; median)  # Count \"well-segregated\" instances  \nExplanation Factor:\nexplanation = np.log(len(data) / (bins * clusters))  \n\n\nSegregation Factor: Measures how distinctly clusters separate data points. We used the median (not mean) to avoid skew from outlier-dominated matrices.\nExplanation Factor: Quantifies how well clusters capture data variability. The logarithmic term penalizes overfitting‚Äîa critical insight from debugging early over-segmented clusters.\n\nAnd the Final Formula:\nFor variable \\(v\\) with \\(k\\) clusters:\n\\[\nS_v^k = \\underbrace{\\frac{N_v^k}{\\max(l, k)}}_{\\text{Segregation}} \\times \\underbrace{\\ln\\left(\\frac{N_d}{l \\times k}\\right)}_{\\text{Explanation}}\n\\]\nwhere:\n- \\(N_v^k\\): Segregated instances (values above cross-tab matrix median)\n- \\(l\\): Number of value intervals for variable \\(v\\)\n- \\(N_d\\): Total observations\nThis formulation ensures algorithmic invariance, allowing comparison across methods like K-Means (distance-based) and GMM (probability-based). Also, now you can see from the formula two scenarios happens: 1. If each variable crosstab is too dense then their is no separation between classes 2. If each variable crosstab is too sparse then we loose on explanation.\nHence the curve reaches a maximum and then falls down giving use the separability that the cluster can produce:"
  },
  {
    "objectID": "blog/engineering/03_crosstab_sparsity/index.html#case-study-vehicle-silhouettes-through-my-eyes",
    "href": "blog/engineering/03_crosstab_sparsity/index.html#case-study-vehicle-silhouettes-through-my-eyes",
    "title": "CrossTab Sparsity",
    "section": "Case Study: Vehicle Silhouettes (Through My Eyes)",
    "text": "Case Study: Vehicle Silhouettes (Through My Eyes)\n\nThe Dataset That Almost Broke Us\nWe tested our metric on a vehicle silhouette dataset with 18 shape-related features (e.g., compactness, circularity). Initially, inconsistent results plagued us‚Äîuntil we realized our binning strategy for continuous variables was flawed.\n\n\nKey Adjustments:\n- Switched from equal-width to quantile-based binning (10 bins per variable).\n- For categorical variables, retained native levels instead of coercing bins.\n\n\nThe Breakthrough\nAfter refining the preprocessing:\n\nOptimal Clusters: Our metric plateaued at \\(k=6\\) , aligning perfectly with known vehicle categories (sedans, trucks, etc.).\nNoise Detection: Variables like Max.LWR (length-width ratio) scored poorly, revealing inconsistent clustering. We later found this was due to manufacturers‚Äô design variances.\n\nFinding best cluster for K-Means alone:\n\n\n\n\nBest Cluster for PAM method Using Crosstab sparsity\n\n\n\nComparing all cluster methods and find the optimal one:\n\n\n\n\nOptimal Cluster for many methods\n\n\n\nThe chunkiest part : Understanding your variable for separateness. This gives direct insight of what variable in your data is most critical separator.\n\n\n\n\nAll kind of variable scored against Metrics"
  },
  {
    "objectID": "blog/engineering/03_crosstab_sparsity/index.html#comparative-advantages-and-creativity-at-work",
    "href": "blog/engineering/03_crosstab_sparsity/index.html#comparative-advantages-and-creativity-at-work",
    "title": "CrossTab Sparsity",
    "section": "Comparative Advantages and Creativity at Work",
    "text": "Comparative Advantages and Creativity at Work\nComparative Advantage Over Traditional Metrics\n\n\n\n\n\n\n\n\n\nFeature/Scenario\nSilhouette Index\nDavies-Bouldin\nCrosstab Sparsity\n\n\n\n\nAlgorithm Agnostic\n‚ùå (Distance-based only)\n‚ùå\n‚úîÔ∏è\n\n\nHandles Mixed Data\n‚ùå\n‚ùå\n‚úîÔ∏è\n\n\nIdentifies Noisy Vars\n‚ùå\n‚ùå\n‚úîÔ∏è\n\n\nOptimal Cluster Detection\nManual elbow analysis\nManual analysis\nAutomated plateau detection\n\n\nMixed Algorithms\nFailed (GMM vs K-Means)\nFailed (needs numerical data)\nAchieved 92% consistency[1]\n\n\nNoisy Variables\nManual outlier removal\nManual outlier removal\nAuto-detected (e.g., Max.LWR)\n\n\nOptimal Cluster Detection\nSubjective elbow plots\nSubjective to Elbow plots\nObjective plateau detection\n\n\n\n\nOur creativity yielding boons. We wanted a simple metric to judge different kind of cluster, but we got much more from our experiments and work on this metric:\n\nVariable-Level Diagnostics: Low \\(S_v^k\\) scores pinpoint variables muddying cluster separation.\n\nCross-Method Benchmarking: Compare K-Means (distance) vs GMM (probability) vs hierarchical vs partition clustering fairly using a unified score.\n\nScale Invariance: Logarithmic term makes scores comparable across datasets of varying sizes.\n\nDebug Cluster Quality: Identify and remove noisy variables preemptively\n\nAutomate Model Selection: Objectively choose between K-Means, GMM, PAM, Agglomerative."
  },
  {
    "objectID": "blog/engineering/03_crosstab_sparsity/index.html#lessons-learned-and-future-vision",
    "href": "blog/engineering/03_crosstab_sparsity/index.html#lessons-learned-and-future-vision",
    "title": "CrossTab Sparsity",
    "section": "Lessons Learned and Future Vision",
    "text": "Lessons Learned and Future Vision\nFew take away from these experiments\n1. Binning Sensitivity: Quantile-based binning was transformation. Equal-width bins distorted scores for skewed variables.\n2. Categorical Handling: Native levels for categorical outperformed frequency-based grouping.\n3. Non-Parametric Approach: This approach allowed us to make sense of data without being tied down by assumptions. We have seen how this metric can be a game-changer for statisticians, providing insights not just into cluster behavior but also into rare event modeling.\nThe plots from these experiments not only clarify how clusters behave but also offer valuable insights for identifying outliers. I believe there‚Äôs exciting potential to extend this metric into classification and value estimation modeling. Imagine using it as a loss function in both linear and non-linear methods to achieve better data segmentation! Thing for another blog someday!\n\nA Personal Reflection\nDeveloping this metric taught me that simplicity often masks depth. A two-component formula now underpins clustering decisions in industries we never imagined‚Äîfrom fraud detection to genomics. Yet, I‚Äôm most proud of how it democratizes cluster analysis: business analysts at our partner firms now optimize clusters without PhD-level stats.\nYou can find implementation of python code here\nThis blog synthesizes findings from our original paper, available here. For a deeper dive into the math, check Section 3 of the paper.\nTo my readers: Have you tried implementing cross-algorithm clustering? Share your war stories in the comments‚ÄîI‚Äôd love to troubleshoot together!"
  },
  {
    "objectID": "blog/engineering/01_adaptive_regression/index.html",
    "href": "blog/engineering/01_adaptive_regression/index.html",
    "title": "Adaptive Regression",
    "section": "",
    "text": "Here I am trying to express our logic to find such Observation. Lets dive in.\nThere are different value estimation technique like regression analysis and time-series analysis. Everyone of us has experimented on regression using OLS ,MLE, Ridge, LASSO, Robust etc., and also might have evaluated them using RMSE (Root Mean/Median Square Error), MAD (Mean/Median Absolute Deviation), MAE (Mean / Median Absolute Error) and MAPE (Mean/Median Absolute Percentage Error), etc‚Ä¶\nBut all of these gives a single point estimate that what is the overall error looks like. Just a different thought!! can we be sure that this single value of MAPE or MAE? How easy it is to infer that our trained model has fitted well across the distribution of dependent variable?\n\n\n\n\n\nPlot of Anscombe‚Äôs Quartet\n\n\n\n\n\nSome Descriptive Stats for Anscombe‚Äôs Quartet\n\n\nLet me give you a pretty small data-set to play with ‚ÄúThe Anscombe‚Äôs quartet‚Äù. This is a very famous data-set by Francis Anscombe. Please refer the plots below to understand the distribution of y1, y2, y3, y4. Isn‚Äôt it different?\nWould the measure of central tendency and disportion be same for this data? I am sure none of us would believe but to our utter surprise we see all the descriptive stats are kind of same. Don‚Äôt believe me !!! Please see the results below ( Source: Wikipedia ):"
  },
  {
    "objectID": "blog/engineering/01_adaptive_regression/index.html#introduction",
    "href": "blog/engineering/01_adaptive_regression/index.html#introduction",
    "title": "Adaptive Regression",
    "section": "",
    "text": "Here I am trying to express our logic to find such Observation. Lets dive in.\nThere are different value estimation technique like regression analysis and time-series analysis. Everyone of us has experimented on regression using OLS ,MLE, Ridge, LASSO, Robust etc., and also might have evaluated them using RMSE (Root Mean/Median Square Error), MAD (Mean/Median Absolute Deviation), MAE (Mean / Median Absolute Error) and MAPE (Mean/Median Absolute Percentage Error), etc‚Ä¶\nBut all of these gives a single point estimate that what is the overall error looks like. Just a different thought!! can we be sure that this single value of MAPE or MAE? How easy it is to infer that our trained model has fitted well across the distribution of dependent variable?\n\n\n\n\n\nPlot of Anscombe‚Äôs Quartet\n\n\n\n\n\nSome Descriptive Stats for Anscombe‚Äôs Quartet\n\n\nLet me give you a pretty small data-set to play with ‚ÄúThe Anscombe‚Äôs quartet‚Äù. This is a very famous data-set by Francis Anscombe. Please refer the plots below to understand the distribution of y1, y2, y3, y4. Isn‚Äôt it different?\nWould the measure of central tendency and disportion be same for this data? I am sure none of us would believe but to our utter surprise we see all the descriptive stats are kind of same. Don‚Äôt believe me !!! Please see the results below ( Source: Wikipedia ):"
  },
  {
    "objectID": "blog/engineering/01_adaptive_regression/index.html#so-what-we-do-now",
    "href": "blog/engineering/01_adaptive_regression/index.html#so-what-we-do-now",
    "title": "Adaptive Regression",
    "section": "So what we do Now!",
    "text": "So what we do Now!\nAstonished !!! Don‚Äôt be. This is what has been hiding behind those numbers. And this is why we really won‚Äôt be able to cross certain performance level. Unless you change some features or even do a lot of hyper parameter tuning, your results won‚Äôt vary much.\nIf you look at the average value of MAPE in each decile you would see an interesting pattern. Let us show you what we see that pattern. One day while working on a business problem where I was using regression on a discussion with Kumarjit, we deviced a different way of model diagnosis. We worked together to give this a shape and build on it.\n\nAs you can see it is absolutely evident that either of the side in the distribution of MAPE values is going wild!!!!!!! Still overall MAPE is good (18%)."
  },
  {
    "objectID": "blog/engineering/01_adaptive_regression/index.html#seeking-scope-of-improvement",
    "href": "blog/engineering/01_adaptive_regression/index.html#seeking-scope-of-improvement",
    "title": "Adaptive Regression",
    "section": "Seeking Scope of Improvement",
    "text": "Seeking Scope of Improvement\nWe worked together to build a different framework to address such issues on the go and reduce the MAPE deterioration on the edge of the distribution.\nThis problems gives rise to a concept we named as Distribution Assertive Regression (DAR).\nDAR is a framework that is based on cancelling the weakness of one point summaries by using the classical concepts of Reliability Engineering : The Bath Tub Curve.\n\n\n\n\n\nPlot for Classical Bath Tub Curve using a Hazard Function\n\n\nThe Specialty of this curve is that it gives you the likelihood which areas one tends to have high failure rates. In our experiments when we replace failure with MAPE value and the Time with sorted (ascending) value of target / dependent variable, we observe the same phenomenon. This is likely to happen because most of regression techniques assumes Normal (Gaussian) Distribution of data and fits itself towards the central tendency of this distribution.\nBecause of this tendency, any regression methods tends to learn less about data which are away from the central tendency of the target.\nLets look at BostonHousing data from ‚Äúmlbench‚Äù package in R.\n\n\n\nPlot for MAPE Bath Tub Curve for Decile Split ‚Äúmdev‚Äù from Data\n\n\nHere the MAPE is calculated for each decile split of ordered target variable. As you can observe it is following the bath tub curve. Hence the validates our hypothesis that the regression method is not able to understand much about the data at the either ends of the distribution."
  },
  {
    "objectID": "blog/engineering/01_adaptive_regression/index.html#final-analysis",
    "href": "blog/engineering/01_adaptive_regression/index.html#final-analysis",
    "title": "Adaptive Regression",
    "section": "Final Analysis",
    "text": "Final Analysis\nNow the DAR framework essentially fixes this weakness of regression method and understands the behavior of data which is stable and can be tweak in a fashion that can be use in general practice.\nPlot of MAPE Bath Tub Curve after applying DAR Framework for Decile Split ‚Äúmdev‚Äù from Data\n\nHow this framework with same method reduced MAPEs so much and made model much more stable‚Ä¶?? Well here it is:\nThe DAR framework splits the data at either ends of the order target variable and performs regression on these ‚Äúsplit‚Äù data individually. This inherently reduces the so called ‚Äúnoise‚Äù part of the data and treat it as an individual data."
  },
  {
    "objectID": "blog/engineering/01_adaptive_regression/index.html#scoring-on-new-data",
    "href": "blog/engineering/01_adaptive_regression/index.html#scoring-on-new-data",
    "title": "Adaptive Regression",
    "section": "Scoring on New Data",
    "text": "Scoring on New Data\nNow you might be thinking while applying regression this sounds good but how will one score this on new data. Well to answer that we used our most simple yet very effective friend ‚ÄúKNN‚Äù (Though any multiclass Classifier can be used here). So ideally scoring involves two step method :\n\nScore new value against each KNN / Multiclass Classifier model of the data\nBased on closeness we score it with the regression method used for that part of data.\n\nSo now we know how we can improve the prediction power of data for regression."
  },
  {
    "objectID": "blog/engineering/01_adaptive_regression/index.html#code-and-flowchart",
    "href": "blog/engineering/01_adaptive_regression/index.html#code-and-flowchart",
    "title": "Adaptive Regression",
    "section": "Code and Flowchart",
    "text": "Code and Flowchart\nIf things are simple lets keep it simple. Refer flowchart and code below for implementation of this framework. Paper here!\n\nR codePython codeHere is the Flow Chart\n\n\n\n\n\n\n\n\nTipClick to Expand\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipClick to Expand\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngraph TB\n    \n    subgraph Testing\n        p1(Finding bucket of model to choose)\n        p1 --&gt; p2([Making predictions &lt;br&gt; based on selected model for inference])\n        p2 --&gt; p3(Consolidate final score of prediction)\n    end\n\n    subgraph Training\n        md([Fitting a &lt;br&gt;Regression model])==&gt; di\n        di{Binning Data via &lt;br/&gt; evaluating Distribution &lt;br/&gt; MAPE values }\n        di --&gt; md2([Fitting a Buckteing model &lt;br/&gt; to Binned MAPE Buckets])\n        md2 --&gt; md3([Fitting Regression &lt;br&gt; Models on Binned Data])\n        md == Keeping main&lt;br/&gt;model ==&gt; ro        \n        md3 ==&gt; ro(Final Models &lt;br&gt; Binning Data Models + &lt;br&gt; Set of Regressoin Models)\n    end\n\n    \n    od([Data Input]) -- Training&lt;br&gt; Data--&gt; md\n    od -- Testing&lt;br&gt; Data--&gt; p1\n    ro -.-&gt; p1\n    ro -.-&gt; p2\n\n    classDef green fill:#9f6,stroke:#333,stroke-width:2px;\n    classDef yellow fill:#ff6,stroke:#333,stroke-width:2px;\n    classDef blue fill:#00f,stroke:#333,stroke-width:2px,color:#fff;\n    classDef orange fill:#f96,stroke:#333,stroke-width:4px;\n    class md,md2,md3 green\n    class di orange\n    class p1,p2 yellow\n    class ro,p3 blue"
  },
  {
    "objectID": "about.html#the-origin-story",
    "href": "about.html#the-origin-story",
    "title": "Hi - I am Jitin! I help business leverage AI for Profits",
    "section": "The Origin Story",
    "text": "The Origin Story\nI didn‚Äôt set out to become ‚ÄúThe AI Architect.‚Äù\nI started as a mechanical engineer with a background in applied mathematics. Early in my career, I was hired to solve automotive inventory problems that traditional approaches couldn‚Äôt crack.\nThe patterns were too complex. The variables too numerous. The standard playbooks weren‚Äôt working.\nThen I had a realization:\nWhat if I stopped thinking about this like an automotive problem and started thinking about it like a healthcare system managing patient flow?\nSuddenly, the solution became obvious. We eliminated 45,000 excess units.\nNot because I knew more about automotive than the automotive experts.\nBecause I was looking at it sideways."
  },
  {
    "objectID": "about.html#the-pattern-emerges",
    "href": "about.html#the-pattern-emerges",
    "title": "Hi - I am Jitin! I help business leverage AI for Profits",
    "section": "The Pattern Emerges",
    "text": "The Pattern Emerges\nOver 15 years architecting AI systems across automotive, retail, insurance, FMCG, and telecommunications for Fortune 500s.\nAs a mechanical engineer with applied mathematics background, I kept seeing the same thing:\nThe breakthrough comes from applying insights from completely different domains.\nHealthcare solving automotive. Retail transforming manufacturing. Mathematical frameworks from physics applied to business systems.\nMost experts stay within their industry expertise. They know their domain deeply, but they‚Äôre looking straight ahead.\nI learned to look sideways."
  },
  {
    "objectID": "about.html#the-mathematical-mindset",
    "href": "about.html#the-mathematical-mindset",
    "title": "Hi - I am Jitin! I help business leverage AI for Profits",
    "section": "The Mathematical Mindset",
    "text": "The Mathematical Mindset\nIn mathematics, there‚Äôs never just one solution to a problem. Some roots are simply more beautiful than others.\nThis philosophy drives how I approach enterprise AI:\n\nPattern recognition across domains instead of single-industry expertise\nRoot cause analysis instead of surface-level fixes\nElegant solutions over brute force approaches\nSystemic thinking instead of point solutions\n\nWhen you‚Äôve solved similar problems in healthcare, the path through retail becomes visible. When you understand the mathematics of one optimization challenge, you start seeing the same structures everywhere."
  },
  {
    "objectID": "about.html#current-work",
    "href": "about.html#current-work",
    "title": "Hi - I am Jitin! I help business leverage AI for Profits",
    "section": "Current Work",
    "text": "Current Work\nI serve as AI Strategist (Ex-AI CTO) across multiple Fortune 500 accounts while mentoring a team of 30+ professionals.\nThe work spans:\n\nPredictive analytics and AI strategy architecture\nCross-industry system optimization\nExecutive team transformation and strategic planning\nImplementation frameworks that actually scale\n\nI also run The AI Profit OS ‚Äî a 3-day intensive that teaches enterprise leaders to think like architects instead of tool users."
  },
  {
    "objectID": "about.html#beyond-the-work",
    "href": "about.html#beyond-the-work",
    "title": "Hi - I am Jitin! I help business leverage AI for Profits",
    "section": "Beyond the Work",
    "text": "Beyond the Work\nWhen I‚Äôm not architecting AI systems, you‚Äôll find me:\n\nPlaying guitar (badly but enthusiastically)\nPlanning the next off-the-beaten-path adventure\nReading mathematical proofs for fun (yes, really)\n\nGetting lost in unfamiliar places keeps the mind flexible. Some of my best insights come from being completely outside my comfort zone."
  },
  {
    "objectID": "about.html#want-to-work-together",
    "href": "about.html#want-to-work-together",
    "title": "Hi - I am Jitin! I help business leverage AI for Profits",
    "section": "Want to Work Together?",
    "text": "Want to Work Together?\nI take on a limited number of consulting engagements and run cohort-based training for enterprise AI leaders.\nIf you‚Äôre exploring a major AI transformation, let‚Äôs talk.\n\n\nExplore Consulting Join The Protocol\n\nOr just email me to discuss your challenge."
  },
  {
    "objectID": "blog/engineering/02_hypothesis_test/index.html",
    "href": "blog/engineering/02_hypothesis_test/index.html",
    "title": "A flow to Test Your Hypothesis in Python",
    "section": "",
    "text": "Hypothesis testing Photo by Tara Winstead"
  },
  {
    "objectID": "blog/engineering/02_hypothesis_test/index.html#overview",
    "href": "blog/engineering/02_hypothesis_test/index.html#overview",
    "title": "A flow to Test Your Hypothesis in Python",
    "section": "Overview",
    "text": "Overview\nAll the practitioners of data science always hit one giant thing to do with data and you know it well its EDA -Exploratory Data Analysis. This word EDA1 was coined by Tukey himself in his seminal book published in 1983. But do you think that before that EDA doesn‚Äôt existed ?\n1¬†Emerson, J. D., & Hoaglin, D. C. (1983). Stem-and-leaf displays. In D. C. Hoaglin, F. Mosteller, & J. W. Tukey (Eds.) Understanding Robust and Exploratory Data Analysis, pp.¬†7‚Äì32. New York: Wiley. Book is here.Well glad you thought. Before that all were doing what is called as Hypothesis Testing. Yes, before this the race was majorly to fit the data and make most unbiased and robust estimate. But remember one thing when you talk about Hypothesis Testing it was always and majorly would be related to RCTs (Randomized Controlled Trials) a.k.a Randomized Clinical Trials and is Gold Standard of data.\n\n\n\n\n\n\nTipMore on RCTs and ODs\n\n\n\n\n\nNow let me now not hijack the discussion to what is RCTs and Observational Data (ODs) as it is more of Philosophical Reasoning rather than other quality of data, but essentially what we are trying to find is that can we by, using stats, identify interesting patterns in data.\nThe only thing happens wit RCT data is that we tend to believe these interesting patterns coincide with some sort of ‚ÄòCause-Effect‚Äô kind of relationship. But essentially due to bia nature of ODs, we certainly cant conclude this. And hence, can only find interesting patterns.\n\n\n\nLets move on. The big question is, for whatever reason you are doing HT , you are doing it for finding something intreating. And that something interesting is usually found by using Post-Hoc Tests. Now there are variety of Post-Hocs available but what is more know and hence easily found to be implemented in Tukey‚Äôs HSD.\nSo lets directly jump to how to follow this procedure. We‚Äôll be using bioinfokit for this, as it is much simpler wrapper around whats implemented in statsmodels."
  },
  {
    "objectID": "blog/engineering/02_hypothesis_test/index.html#what-are-the-results",
    "href": "blog/engineering/02_hypothesis_test/index.html#what-are-the-results",
    "title": "A flow to Test Your Hypothesis in Python",
    "section": "What are the results",
    "text": "What are the results\nPheww‚Ä¶ Thats too much code right. But that would save a lot of your time in real life. So in real life you would write code as 3 steps below:\n\n\nCode\n# import libraries\nimport pandas as pd\n\n# Getting car data from UCI\ndf = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data',\n                 sep='\\s+',header=None,\n                 names=['mpg','cylinders','displacement','horsepower','weight',\n                 'acceleration','model_year','origin','car_name'])\ndf.head()\n\n# Syntax to do anove with validating the assumption, doing test and a post-hoc\nresults = do_anova_test(df=df, res_var='mpg',xfac_var='cylinders', \n                        anova_model='mpg ~ C(cylinders)+C(origin)+C(cylinders):C(origin)',\n                        ss_typ=3, result_full=True)\n\n\nResults form the do_anova_test\nLevens Test Result:\n                 Parameter    Value\n0      Test statistics (W)  14.5856\n1  Degrees of freedom (Df)   4.0000\n2                  p value   0.0000\n\nBartletts Test Result:\n                 Parameter    Value\n0      Test statistics (T)  61.2143\n1  Degrees of freedom (Df)   4.0000\n2                  p value   0.0000\n\nANOVA\\ANCOVA Test Result:\n                           df     sum_sq    mean_sq         F  PR(&gt;F)      n2\nIntercept                 1.0  6195.1701  6195.1701  296.3452  0.0000  0.2727\nC(cylinders)              4.0  7574.5864  1893.6466   90.5824  0.0000  0.3334\nC(origin)                 2.0   241.0703   120.5351    5.7658  0.0034  0.0106\nC(cylinders):C(origin)    8.0   577.4821    72.1853    3.4530  0.0046  0.0254\nResidual                389.0  8132.1404    20.9052       NaN     NaN     NaN\n\nTukey HSD Result:\n   group1  group2     Diff    Lower    Upper  q-value  p-value\n0       8       4  14.3237  12.8090  15.8383  36.6527   0.0010\n1       8       6   5.0226   3.1804   6.8648  10.5671   0.0010\n2       8       3   5.5869  -0.7990  11.9728   3.3909   0.1183\n3       8       5  12.4036   5.0643  19.7428   6.5503   0.0010\n4       4       6   9.3011   7.6765  10.9256  22.1910   0.0010\n5       4       3   8.7368   2.4102  15.0633   5.3524   0.0017\n6       4       5   1.9201  -5.3676   9.2078   1.0212   0.9000\n7       6       3   0.5643  -5.8486   6.9772   0.3410   0.9000\n8       6       5   7.3810   0.0182  14.7437   3.8854   0.0491\n9       3       5   6.8167  -2.7539  16.3873   2.7606   0.2919\nNice!!!\n\nAnd plotting is even easier\n\n\nCode\n# Numbers are clumsy for most. Making more interpretable plot on above results.\nplot_hsd(results.tukeyhsd.sort_values('Diff'), title=\"Tukey HSD resutls Anova of MPG ~ Cylinder\")\n\n\nResults form the plot_hsd\n\n\n\nTukey‚Äôs HSD comparison based on Anova Results\n\n\nPlots look good with ‚Äòp-values‚Äô."
  },
  {
    "objectID": "blog/engineering/02_hypothesis_test/index.html#conclusion",
    "href": "blog/engineering/02_hypothesis_test/index.html#conclusion",
    "title": "A flow to Test Your Hypothesis in Python",
    "section": "Conclusion",
    "text": "Conclusion\nNow since we applied the above to a Non RCT we cannot conclude that Difference in mpg based on cylinder is huge specially as number of cylinders goes up. But this statement might not be as explicit as might be appearing from plot. Unless you have a strong believe that the data follows with rules and assumptions of RCTs, we should be only seeking interesting as in associated results and not cause-effect results."
  },
  {
    "objectID": "blog/engineering/02_hypothesis_test/index.html#give-me-the-code",
    "href": "blog/engineering/02_hypothesis_test/index.html#give-me-the-code",
    "title": "A flow to Test Your Hypothesis in Python",
    "section": "Give me ‚ÄúThe Code‚Äù",
    "text": "Give me ‚ÄúThe Code‚Äù\n\nPerforming AnovaPlotting Results\n\n\n\n\nAnova Test anova_test.py\nfrom bioinfokit import analys\n\nimport numpy as np\nfrom scipy import stats\n\nclass KeyResults:\n    \"\"\"\n    A basic class to hold all the results\n    \"\"\"\n    \n    def __init__(self,result_full):\n        self.keys = []\n        self.result_full = result_full\n    \n    def add_result(self,name,result):\n        if name == 'tukeyhsd':\n            self.keys.append(name)\n            setattr(self, name, result)\n        elif self.result_full:\n            self.keys.append(name)\n            setattr(self, name, result)\n\n\n# Anova test code\ndef do_anova_test(df, res_var, xfac_var, anova_model,ss_typ=3,\n                  effectsize='n2',result_full=False,add_res=False):\n    \"\"\"\n    Do all sequential anova tests\n    \n    Step 1) Leven's/ bartellet test for checking weather variance is homogenous or not\n    Step 2) Main ANOVA/ANCOVA test\n    Step 3) Tukey's HSD for individual combinations\n    \n    :param df: Pandas DataFrame holding all the columns\n    :param res_var: Variable for which we are checking ANOVA\n    :param xfac_var: Grouping Variables for which we want to do the comparisons\n    :param anova_model: SM formula for the model. This is life savour to make all things work\n    :param result_full: To provide all the results of intermediate steps\n    \n    \"\"\"\n\n    results = KeyResults(result_full)\n    \n    # initialize stat method\n    res = analys.stat()\n    \n    # doing levens test\n    res.levene(df=df, res_var=res_var,xfac_var=xfac_var)\n    print('\\nLeven\\'s Test Result:')\n    print(res.levene_summary)\n    results.add_result('levene',res.levene_summary)\n\n    # doing bartlett test\n    res.bartlett(df=df, res_var=res_var,xfac_var=xfac_var)\n    print('\\nBartlett\\'s Test Result:')\n    print(res.bartlett_summary)\n    results.add_result('bartlett',res.bartlett_summary)\n    \n    # doing anova / ancova\n    res.anova_stat(df=df, res_var=res_var, anova_model=anova_model,ss_typ=ss_typ)\n    aov_res = res.anova_summary\n    \n    # Add effect sizes\n    if effectsize == \"n2\":\n        all_effsize = (aov_res['sum_sq'] / aov_res['sum_sq'].sum()).to_numpy()\n        all_effsize[-1] = np.nan\n    else:\n        ss_resid = aov_res['sum_sq'].iloc[-1]\n        all_effsize = aov_res['sum_sq'].apply(lambda x: x / (x + ss_resid)).to_numpy()\n        all_effsize[-1] = np.nan\n    aov_res[effectsize] = all_effsize\n    #aov_res['bw_'] = res.anova_model_out.params.iloc[-1]\n    aov_res = aov_res.round(4)\n    \n    # printing results\n    print('\\nANOVA\\ANCOVA Test Result:')\n    print(aov_res)\n    results.add_result('anova',res.anova_summary.round(4))\n    results.add_result('anova_model',res.anova_model_out)\n    \n    # doing tukey's hsd top compare the groups\n    res.tukey_hsd(df=df, res_var=res_var,xfac_var=xfac_var, anova_model=anova_model,ss_typ=ss_typ)\n    print('\\nTukey HSD Result:')\n    print(res.tukey_summary.round(4))\n    results.add_result('tukeyhsd',res.tukey_summary.round(4))\n    \n    # add all result componets again if needed \n    if add_res:\n        results.add_result('allresult',res)\n    \n    return results\n\n\n\n\n\n\nPlotting results plot_hsd.py\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.style.use('seaborn-bright')\n\ndef plot_hsd(hsdres,p_cutoff=0.05,title=None,ax=None,figsize=(10,7)):\n     \"\"\"\n     Do plotting of tukeyhsd results\n    \n  \n    :param hsdres: 'tukeyhsd' result form the do_anova_test function\n    :param p_cutoff: Cutoff at which we get say a combination is significant\n    :param title: Title of the plot\n    :param ax: Define or get the matplotlib axes\n    :param figsize: Mention Figure size to draw\n    \n    \"\"\"\n\n    if ax is None:\n        fig,axp = plt.subplots(figsize=figsize)\n    else:\n        axp = ax\n    \n    # helper func\n    p_ind = lambda x : '' if x &gt; 0.1 else ('+' if x &gt; 0.05 else ('*' if x &gt; 0.01 else ('**' if x &gt;0.001 else '***')))\n    label_gen  = lambda x: f\"${x[0]} - {x[1]}\\ |\\ p:{x[2]:0.2f}{p_ind(x[2]):5s}$\"\n    \n    #setting values\n    mask = hsdres['p-value'] &lt;= p_cutoff\n    yticklabs = hsdres[['group1','group2','p-value']].apply(label_gen,axis=1).values\n    ys = np.arange(len(hsdres))\n    \n    # adding plot to axes\n    axp.errorbar(hsdres[~mask]['Diff'],ys[~mask],xerr=np.abs(hsdres[~mask][['Lower',\"Upper\"]]).values.T,\n                fmt='o', color='black', ecolor='lightgray', elinewidth=2, capsize=0)\n    axp.errorbar(hsdres[mask]['Diff'],ys[mask],xerr=np.abs(hsdres[mask][['Lower',\"Upper\"]]).values.T,\n                fmt='o', color='red', ecolor='pink', elinewidth=2, capsize=5)\n    axp.axvline(x=0,linestyle='--',c='skyblue')\n    axp.set_yticks([])\n    (l,u) = axp.get_xlim()\n    axp.set_xlim(l+1.5*l,u)\n    (l,u) = axp.get_xlim()\n    for idx,labs in enumerate(yticklabs):\n        axp.text(l-0.1*l,ys[idx],labs)\n    axp.set_yticklabels([])\n    \n    # finally doing what is needed\n    if ax is None:\n        plt.title('' if title is None else title,fontsize=14)\n        plt.show()\n    else:\n        return axp\n\n\n\n\n\nHope this give you kickstart to find you intresting patterns. Happy Learning!"
  },
  {
    "objectID": "blog/engineering/04_crosstab_sparsity_classification/index.html",
    "href": "blog/engineering/04_crosstab_sparsity_classification/index.html",
    "title": "CrossTab Sparsity for Classification",
    "section": "",
    "text": "Cross Roads where everyone meets!\n\n\n\nIntroduction: A Journey into Data\nPicture this: you‚Äôre standing on the icy shores of Antarctica, the wind whipping around you as you watch a colony of Palmer Penguins waddling about, oblivious to the data detective work you‚Äôre about to embark on. As a data science architect, you‚Äôre not just an observer; you‚Äôre a sleuth armed with algorithms and insights, ready to unravel the mysteries hidden within data. Today, we‚Äôll transform raw numbers into powerful narratives using CrossTab Sparsity as our guiding compass. This blog post will demonstrate how this metric can revolutionize classification tasks, shedding light on many fascinating datasets‚Äîthe charming Palmer Penguins and the serious Obesity, Credit cards data and many more.\n\n\nThe Power of CrossTab Sparsity\n\nWhat is CrossTab Sparsity?\nCrossTab Sparsity isn‚Äôt just a fancy term that sounds good at dinner parties; it‚Äôs a statistical measure that helps us peer into the intricate relationships between categorical variables. Imagine it as a magnifying glass that reveals how different categories interact within a contingency table. Understanding these interactions is crucial in classification tasks, where the right features can make or break your model (and your day).\nWhy Does It Matter?\nIn the world of data science, especially in classification, selecting relevant features is like picking the right ingredients for a gourmet meal‚Äîget it wrong, and you might end up with something unpalatable. CrossTab Sparsity helps us achieve this by:\n\nHighlighting Relationships: It‚Äôs like having a friend who always points out when two people are meant to be together‚Äîunderstanding how features interact with the target variable.\nStreamlining Models: Reducing complexity by focusing on significant features means less time spent untangling spaghetti code.\nEnhancing Interpretability: Making models easier to understand and explain to stakeholders is like translating tech jargon into plain English‚Äîeveryone appreciates that!\n\n\n\n\nData Overview: Our Data People at work here\n\nThe Datasets\nData 1: Estimation of Obesity Levels Based On Eating Habits and Physical Condition\nLittle bit about the data: This dataset, shared on 8/26/2019, looks at obesity levels in people from Mexico, Peru, and Colombia based on their eating habits and physical health. It includes 2,111 records with 16 features, and classifies individuals into different obesity levels, from insufficient weight to obesity type III. Most of the data (77%) was created using a tool, while the rest (23%) was collected directly from users online.\nData 2: Predict Students‚Äô Dropout and Academic Success\nLittle bit about the data: This dataset, shared on 12/12/2021, looks at factors like students‚Äô backgrounds, academic path, and socio-economic status to predict whether they‚Äôll drop out or succeed in their studies. With 4,424 records across 36 features, it covers students from different undergrad programs. The goal is to use machine learning to spot at-risk students early, so schools can offer support. The data has been cleaned and doesn‚Äôt have any missing values. It‚Äôs a classification task with three outcomes: dropout, still enrolled, or graduated\nKey Features:\n\nMulticlass: Both data set cater a multi class problems with NObeyesdad and Target columns\nMixed Data Type: A good mix of categorical and continuous variables are available for usage.\nSizeable: More than 2 K rows are available for testing.\n\n\n\n\nExploratory Data Analysis (EDA): Setting the Stage\nBefore we dive into model creation, let‚Äôs explore our dataset through some quick EDA. Think of this as getting to know your non-obese friends before inviting them to a party.\n\nEDA for Obesity Data\nHere‚Äôs a brief code snippet to perform essential EDA on the Obesity dataset:\n\n\nLoading data and generating basic descriptive\n# Load the Obesity data\nraw_df = pd.read_csv('ObesityDataSet_raw_and_data_sinthetic.csv')\ntarget = 'NObeyesdad'\n\n# Load Students data\n\n# Load Credit data\n# raw_data = sm.datasets.get_rdataset(\"credit_data\",'modeldata')\n# raw_df = raw_data.data\n# target = 'Status'\n\n# # Load Palmer penguins data\n# raw_data = sm.datasets.get_rdataset(\"penguins\",'palmerpenguins')\n# raw_df = raw_data.data\n# target = 'species'\n\n\n# # Load Credit data\n# raw_data = sm.datasets.get_rdataset(\"CreditCard\",'AER')\n# raw_df = raw_data.data\n# target = 'card'\n\n\n# setting things up for aal the next steps\nraw_df[target] = raw_df[target].astype('category') \nprint('No of data points available to work:',raw_df.shape)\ndisplay(raw_df.head())\n\n\n# Summary statistics\ndisplay(raw_df.describe())\n\n\nNo of data points available to work: (2111, 17)\n\n\n\n\n\n\n\n\n\nGender\nAge\nHeight\nWeight\nFamil_Hist_Owt\nFAVC\nFCVC\nNCP\nCAEC\nSMOKE\nCH2O\nSCC\nFAF\nTUE\nCALC\nMTRANS\nNObeyesdad\n\n\n\n\n0\nFemale\n21.0\n1.62\n64.0\nyes\nno\n2.0\n3.0\nSometimes\nno\n2.0\nno\n0.0\n1.0\nno\nPublic_Transportation\nNormal_Weight\n\n\n1\nFemale\n21.0\n1.52\n56.0\nyes\nno\n3.0\n3.0\nSometimes\nyes\n3.0\nyes\n3.0\n0.0\nSometimes\nPublic_Transportation\nNormal_Weight\n\n\n2\nMale\n23.0\n1.80\n77.0\nyes\nno\n2.0\n3.0\nSometimes\nno\n2.0\nno\n2.0\n1.0\nFrequently\nPublic_Transportation\nNormal_Weight\n\n\n3\nMale\n27.0\n1.80\n87.0\nno\nno\n3.0\n3.0\nSometimes\nno\n2.0\nno\n2.0\n0.0\nFrequently\nWalking\nOverweight_Level_I\n\n\n4\nMale\n22.0\n1.78\n89.8\nno\nno\n2.0\n1.0\nSometimes\nno\n2.0\nno\n0.0\n0.0\nSometimes\nPublic_Transportation\nOverweight_Level_II\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAge\nHeight\nWeight\nFCVC\nNCP\nCH2O\nFAF\nTUE\n\n\n\n\ncount\n2111.000000\n2111.000000\n2111.000000\n2111.000000\n2111.000000\n2111.000000\n2111.000000\n2111.000000\n\n\nmean\n24.312600\n1.701677\n86.586058\n2.419043\n2.685628\n2.008011\n1.010298\n0.657866\n\n\nstd\n6.345968\n0.093305\n26.191172\n0.533927\n0.778039\n0.612953\n0.850592\n0.608927\n\n\nmin\n14.000000\n1.450000\n39.000000\n1.000000\n1.000000\n1.000000\n0.000000\n0.000000\n\n\n25%\n19.947192\n1.630000\n65.473343\n2.000000\n2.658738\n1.584812\n0.124505\n0.000000\n\n\n50%\n22.777890\n1.700499\n83.000000\n2.385502\n3.000000\n2.000000\n1.000000\n0.625350\n\n\n75%\n26.000000\n1.768464\n107.430682\n3.000000\n3.000000\n2.477420\n1.666678\n1.000000\n\n\nmax\n61.000000\n1.980000\n173.000000\n3.000000\n4.000000\n3.000000\n3.000000\n2.000000\n\n\n\n\n\n\n\n\n\nTarget distribution\n\n\nTarget and Correlation\n# Visualize target data distribution\nplt.figure(figsize=(4, 3))\nsns.countplot(data=raw_df, x=target, hue=target, palette='Set2',)\nplt.title(f'Distribution of {target} levels')\nplt.xticks(rotation=45)\nplt.show()\n\n# Heatmap to check for correlations between numeric variables\ncorr = raw_df.corr('kendall',numeric_only=True)\nsns.heatmap(corr, annot=True, cmap='coolwarm')\nplt.title('Kendall Correlation Heatmap')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoneSome Mode EDA for the data\n\n\n\n\n\n\n\nEDA code\n# Visualize the distribution of numerical variables\nsns.pairplot(raw_df, hue=target, corner=True)\nplt.show()\n\n\n\n\n# Gettign Categorical data\ncategorical_columns = raw_df.select_dtypes(include='object').columns\n\n# Plot categorical variables with respect to the target variable\nfor col in categorical_columns:\n    plt.figure(figsize=(12, 5))\n    sns.countplot(data=raw_df,x=col, hue=target)\n    plt.title(f\"Countplot of {col} with respect to {target}\")\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Creation: Establishing a Baseline\nWith our exploratory analysis complete, we‚Äôre ready to create our baseline model using logistic regression with Statsmodels. This initial model will serve as our reference point‚Äîlike setting up a benchmark for your favorite video game.\n\n\nSplitting data and training a default Multinomila Logit model on our data\ndata_df = raw_df.dropna().reset_index(drop=True)\ndata_df[target] = data_df[target].cat.codes\n# X = data_df[['bill_length_mm','bill_depth_mm','flipper_length_mm','body_mass_g']] \n\ndata_df_test = data_df.sample(frac=0.1,random_state=3)\ndata_df_train = data_df.drop(data_df_test.index)\n\n# Using MN logistic regression model using formula API\n# This would essentially bold down to pair wise logsitic regression\nlogit_model = sm.MNLogit.from_formula(\n    f\"{target} ~ {' + '.join([col for col in data_df_train.columns if col != target])}\", \n    data=data_df_train\n).fit_regularized()\n\n\nOptimization terminated successfully    (Exit mode 0)\n            Current function value: 0.17057119619320013\n            Iterations: 485\n            Function evaluations: 639\n            Gradient evaluations: 485\n\n\n\n\n\n\n\n\n\n\nNoneBase model summary for geeks\n\n\n\n\n\n\n\nDisplay summary\ndisplay(logit_model.summary())\n\n\n\nMNLogit Regression Results\n\n\nDep. Variable:\nNObeyesdad\nNo. Observations:\n1900\n\n\nModel:\nMNLogit\nDf Residuals:\n1756\n\n\nMethod:\nMLE\nDf Model:\n138\n\n\nDate:\nMon, 15 Dec 2025\nPseudo R-squ.:\n0.9122\n\n\nTime:\n16:07:33\nLog-Likelihood:\n-324.09\n\n\nconverged:\nTrue\nLL-Null:\n-3691.8\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n0.000\n\n\n\n\n\n\n\n\nNObeyesdad=1\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-11.2903\n3.25e+05\n-3.48e-05\n1.000\n-6.36e+05\n6.36e+05\n\n\nGender[T.Male]\n-3.4851\n0.817\n-4.268\n0.000\n-5.085\n-1.885\n\n\nFamil_Hist_Owt[T.yes]\n-0.8162\n0.655\n-1.246\n0.213\n-2.100\n0.468\n\n\nFAVC[T.yes]\n0.2636\n0.785\n0.336\n0.737\n-1.275\n1.802\n\n\nCAEC[T.Frequently]\n-8.2402\n2.312\n-3.564\n0.000\n-12.771\n-3.709\n\n\nCAEC[T.Sometimes]\n-6.2226\n2.232\n-2.787\n0.005\n-10.598\n-1.847\n\n\nCAEC[T.no]\n-8.5977\n2.889\n-2.976\n0.003\n-14.260\n-2.935\n\n\nSMOKE[T.yes]\n4.4919\n3.115\n1.442\n0.149\n-1.614\n10.598\n\n\nSCC[T.yes]\n-0.7294\n1.447\n-0.504\n0.614\n-3.565\n2.106\n\n\nCALC[T.Frequently]\n-12.6192\n3.25e+05\n-3.89e-05\n1.000\n-6.36e+05\n6.36e+05\n\n\nCALC[T.Sometimes]\n-13.2985\n3.25e+05\n-4.1e-05\n1.000\n-6.36e+05\n6.36e+05\n\n\nCALC[T.no]\n-14.1585\n3.25e+05\n-4.36e-05\n1.000\n-6.36e+05\n6.36e+05\n\n\nMTRANS[T.Bike]\n15.8909\n2489.580\n0.006\n0.995\n-4863.596\n4895.378\n\n\nMTRANS[T.Motorbike]\n3.9944\n47.659\n0.084\n0.933\n-89.416\n97.405\n\n\nMTRANS[T.Public_Transportation]\n4.4914\n0.995\n4.514\n0.000\n2.541\n6.441\n\n\nMTRANS[T.Walking]\n4.3554\n1.502\n2.900\n0.004\n1.412\n7.299\n\n\nAge\n0.3721\n0.097\n3.833\n0.000\n0.182\n0.562\n\n\nHeight\n-14.4208\n4.118\n-3.502\n0.000\n-22.492\n-6.349\n\n\nWeight\n1.0786\n0.146\n7.378\n0.000\n0.792\n1.365\n\n\nFCVC\n-0.7754\n0.429\n-1.806\n0.071\n-1.617\n0.066\n\n\nNCP\n-1.7094\n0.491\n-3.480\n0.001\n-2.672\n-0.747\n\n\nCH2O\n-1.7291\n0.578\n-2.992\n0.003\n-2.862\n-0.596\n\n\nFAF\n-0.1924\n0.280\n-0.688\n0.491\n-0.740\n0.356\n\n\nTUE\n-0.9320\n0.456\n-2.043\n0.041\n-1.826\n-0.038\n\n\nNObeyesdad=2\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n17.4309\nnan\nnan\nnan\nnan\nnan\n\n\nGender[T.Male]\n-14.0384\n1.983\n-7.079\n0.000\n-17.925\n-10.151\n\n\nFamil_Hist_Owt[T.yes]\n2.0527\n1.717\n1.195\n0.232\n-1.313\n5.418\n\n\nFAVC[T.yes]\n0.9668\n1.752\n0.552\n0.581\n-2.468\n4.401\n\n\nCAEC[T.Frequently]\n-10.0052\n4.352\n-2.299\n0.021\n-18.534\n-1.476\n\n\nCAEC[T.Sometimes]\n-1.0074\n3.427\n-0.294\n0.769\n-7.724\n5.709\n\n\nCAEC[T.no]\n-0.4896\n894.479\n-0.001\n1.000\n-1753.637\n1752.658\n\n\nSMOKE[T.yes]\n8.1410\n4.013\n2.029\n0.042\n0.277\n16.005\n\n\nSCC[T.yes]\n-7.6940\n152.983\n-0.050\n0.960\n-307.535\n292.147\n\n\nCALC[T.Frequently]\n-2.4516\nnan\nnan\nnan\nnan\nnan\n\n\nCALC[T.Sometimes]\n-7.5316\nnan\nnan\nnan\nnan\nnan\n\n\nCALC[T.no]\n-7.2301\nnan\nnan\nnan\nnan\nnan\n\n\nMTRANS[T.Bike]\n-11.9350\n8.09e+07\n-1.47e-07\n1.000\n-1.59e+08\n1.59e+08\n\n\nMTRANS[T.Motorbike]\n10.9226\n48.493\n0.225\n0.822\n-84.123\n105.968\n\n\nMTRANS[T.Public_Transportation]\n11.1756\n1.750\n6.387\n0.000\n7.746\n14.605\n\n\nMTRANS[T.Walking]\n1.7281\n2.759\n0.626\n0.531\n-3.679\n7.135\n\n\nAge\n0.8111\n0.132\n6.139\n0.000\n0.552\n1.070\n\n\nHeight\n-184.0385\n14.746\n-12.481\n0.000\n-212.939\n-155.138\n\n\nWeight\n3.9438\n0.288\n13.688\n0.000\n3.379\n4.508\n\n\nFCVC\n0.8915\n1.014\n0.879\n0.379\n-1.095\n2.878\n\n\nNCP\n-1.1415\n0.711\n-1.605\n0.109\n-2.536\n0.253\n\n\nCH2O\n-1.5390\n0.876\n-1.756\n0.079\n-3.256\n0.179\n\n\nFAF\n-1.5295\n0.591\n-2.586\n0.010\n-2.689\n-0.370\n\n\nTUE\n-0.5710\n0.840\n-0.680\n0.497\n-2.217\n1.075\n\n\nNObeyesdad=3\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-138.5068\n1.47e+07\n-9.41e-06\n1.000\n-2.89e+07\n2.89e+07\n\n\nGender[T.Male]\n-16.6365\n8.279\n-2.010\n0.044\n-32.863\n-0.410\n\n\nFamil_Hist_Owt[T.yes]\n2.3538\n11.601\n0.203\n0.839\n-20.384\n25.092\n\n\nFAVC[T.yes]\n-8.7785\n5.476\n-1.603\n0.109\n-19.512\n1.955\n\n\nCAEC[T.Frequently]\n-71.7022\nnan\nnan\nnan\nnan\nnan\n\n\nCAEC[T.Sometimes]\n-3.9034\n4.734\n-0.824\n0.410\n-13.183\n5.376\n\n\nCAEC[T.no]\n7.7265\n895.063\n0.009\n0.993\n-1746.566\n1762.019\n\n\nSMOKE[T.yes]\n3.5306\n19.342\n0.183\n0.855\n-34.379\n41.440\n\n\nSCC[T.yes]\n-19.4879\n154.607\n-0.126\n0.900\n-322.512\n283.536\n\n\nCALC[T.Frequently]\n-43.6020\n1.48e+07\n-2.95e-06\n1.000\n-2.9e+07\n2.9e+07\n\n\nCALC[T.Sometimes]\n-45.7496\n1.47e+07\n-3.11e-06\n1.000\n-2.88e+07\n2.88e+07\n\n\nCALC[T.no]\n-28.2183\n1.43e+07\n-1.97e-06\n1.000\n-2.81e+07\n2.81e+07\n\n\nMTRANS[T.Bike]\n0.0376\nnan\nnan\nnan\nnan\nnan\n\n\nMTRANS[T.Motorbike]\n-2.3812\n1.05e+11\n-2.27e-11\n1.000\n-2.06e+11\n2.06e+11\n\n\nMTRANS[T.Public_Transportation]\n22.5234\n6.664\n3.380\n0.001\n9.463\n35.584\n\n\nMTRANS[T.Walking]\n-5.3334\n33.279\n-0.160\n0.873\n-70.560\n59.893\n\n\nAge\n2.5106\n0.964\n2.605\n0.009\n0.621\n4.400\n\n\nHeight\n-278.9439\n44.201\n-6.311\n0.000\n-365.576\n-192.312\n\n\nWeight\n7.1539\n1.394\n5.132\n0.000\n4.422\n9.886\n\n\nFCVC\n4.1064\n3.285\n1.250\n0.211\n-2.333\n10.546\n\n\nNCP\n-1.5637\n2.424\n-0.645\n0.519\n-6.315\n3.187\n\n\nCH2O\n-13.4088\n5.560\n-2.412\n0.016\n-24.306\n-2.511\n\n\nFAF\n-9.8534\n4.356\n-2.262\n0.024\n-18.390\n-1.316\n\n\nTUE\n-5.6951\n3.292\n-1.730\n0.084\n-12.147\n0.757\n\n\nNObeyesdad=4\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-87.3214\nnan\nnan\nnan\nnan\nnan\n\n\nGender[T.Male]\n-200.3037\n5.41e+07\n-3.7e-06\n1.000\n-1.06e+08\n1.06e+08\n\n\nFamil_Hist_Owt[T.yes]\n-30.9252\nnan\nnan\nnan\nnan\nnan\n\n\nFAVC[T.yes]\n-53.1818\n3.98e+07\n-1.34e-06\n1.000\n-7.8e+07\n7.8e+07\n\n\nCAEC[T.Frequently]\n-28.5483\nnan\nnan\nnan\nnan\nnan\n\n\nCAEC[T.Sometimes]\n-21.5821\n5.38e+07\n-4.01e-07\n1.000\n-1.05e+08\n1.05e+08\n\n\nCAEC[T.no]\n-2.2000\n4.62e+29\n-4.76e-30\n1.000\n-9.06e+29\n9.06e+29\n\n\nSMOKE[T.yes]\n-6.0944\nnan\nnan\nnan\nnan\nnan\n\n\nSCC[T.yes]\n-12.3054\nnan\nnan\nnan\nnan\nnan\n\n\nCALC[T.Frequently]\n-6.2460\nnan\nnan\nnan\nnan\nnan\n\n\nCALC[T.Sometimes]\n-37.2004\n2.12e+08\n-1.76e-07\n1.000\n-4.15e+08\n4.15e+08\n\n\nCALC[T.no]\n-64.5032\nnan\nnan\nnan\nnan\nnan\n\n\nMTRANS[T.Bike]\n-0.2989\n1.92e+53\n-1.56e-54\n1.000\n-3.76e+53\n3.76e+53\n\n\nMTRANS[T.Motorbike]\n-0.2031\nnan\nnan\nnan\nnan\nnan\n\n\nMTRANS[T.Public_Transportation]\n-57.6854\n7.04e+07\n-8.2e-07\n1.000\n-1.38e+08\n1.38e+08\n\n\nMTRANS[T.Walking]\n-7.4464\n2.03e+15\n-3.66e-15\n1.000\n-3.98e+15\n3.98e+15\n\n\nAge\n-9.3747\n103.246\n-0.091\n0.928\n-211.733\n192.984\n\n\nHeight\n-174.4727\n592.866\n-0.294\n0.769\n-1336.469\n987.523\n\n\nWeight\n8.7405\n35.222\n0.248\n0.804\n-60.293\n77.774\n\n\nFCVC\n49.0613\n3.02e+04\n0.002\n0.999\n-5.91e+04\n5.92e+04\n\n\nNCP\n2.3650\n4572.743\n0.001\n1.000\n-8960.047\n8964.777\n\n\nCH2O\n-18.5809\n34.347\n-0.541\n0.589\n-85.900\n48.738\n\n\nFAF\n-65.1761\n262.887\n-0.248\n0.804\n-580.424\n450.072\n\n\nTUE\n-44.3721\n285.217\n-0.156\n0.876\n-603.387\n514.643\n\n\nNObeyesdad=5\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-12.5683\n3.25e+05\n-3.87e-05\n1.000\n-6.36e+05\n6.36e+05\n\n\nGender[T.Male]\n-6.8149\n1.085\n-6.282\n0.000\n-8.941\n-4.689\n\n\nFamil_Hist_Owt[T.yes]\n-0.5822\n0.790\n-0.737\n0.461\n-2.130\n0.966\n\n\nFAVC[T.yes]\n2.6008\n0.978\n2.660\n0.008\n0.684\n4.517\n\n\nCAEC[T.Frequently]\n-7.2298\n2.507\n-2.884\n0.004\n-12.143\n-2.316\n\n\nCAEC[T.Sometimes]\n-2.8197\n2.413\n-1.168\n0.243\n-7.550\n1.910\n\n\nCAEC[T.no]\n-3.8181\n3.143\n-1.215\n0.224\n-9.977\n2.341\n\n\nSMOKE[T.yes]\n3.1451\n3.296\n0.954\n0.340\n-3.314\n9.604\n\n\nSCC[T.yes]\n2.1647\n1.617\n1.339\n0.181\n-1.004\n5.334\n\n\nCALC[T.Frequently]\n-9.0315\n3.25e+05\n-2.78e-05\n1.000\n-6.36e+05\n6.36e+05\n\n\nCALC[T.Sometimes]\n-9.1446\n3.25e+05\n-2.82e-05\n1.000\n-6.36e+05\n6.36e+05\n\n\nCALC[T.no]\n-10.7708\n3.25e+05\n-3.32e-05\n1.000\n-6.36e+05\n6.36e+05\n\n\nMTRANS[T.Bike]\n19.0425\n2489.581\n0.008\n0.994\n-4860.446\n4898.531\n\n\nMTRANS[T.Motorbike]\n1.6235\n47.716\n0.034\n0.973\n-91.899\n95.146\n\n\nMTRANS[T.Public_Transportation]\n5.9777\n1.209\n4.946\n0.000\n3.609\n8.346\n\n\nMTRANS[T.Walking]\n4.3596\n1.776\n2.454\n0.014\n0.878\n7.841\n\n\nAge\n0.4878\n0.106\n4.597\n0.000\n0.280\n0.696\n\n\nHeight\n-50.0157\n6.721\n-7.442\n0.000\n-63.188\n-36.844\n\n\nWeight\n1.7920\n0.168\n10.651\n0.000\n1.462\n2.122\n\n\nFCVC\n-0.8369\n0.601\n-1.393\n0.164\n-2.014\n0.341\n\n\nNCP\n-1.4453\n0.554\n-2.608\n0.009\n-2.531\n-0.359\n\n\nCH2O\n-1.7648\n0.679\n-2.601\n0.009\n-3.095\n-0.435\n\n\nFAF\n-0.5613\n0.374\n-1.499\n0.134\n-1.295\n0.172\n\n\nTUE\n-0.7982\n0.555\n-1.439\n0.150\n-1.886\n0.289\n\n\nNObeyesdad=6\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-2.1693\n6.28e+06\n-3.45e-07\n1.000\n-1.23e+07\n1.23e+07\n\n\nGender[T.Male]\n-6.6857\n1.207\n-5.537\n0.000\n-9.052\n-4.319\n\n\nFamil_Hist_Owt[T.yes]\n1.9296\n1.076\n1.793\n0.073\n-0.179\n4.038\n\n\nFAVC[T.yes]\n-0.4617\n1.141\n-0.405\n0.686\n-2.698\n1.775\n\n\nCAEC[T.Frequently]\n-5.5324\n3.264\n-1.695\n0.090\n-11.930\n0.866\n\n\nCAEC[T.Sometimes]\n0.7854\n3.044\n0.258\n0.796\n-5.181\n6.752\n\n\nCAEC[T.no]\n1.7141\n3.934\n0.436\n0.663\n-5.997\n9.426\n\n\nSMOKE[T.yes]\n7.0398\n3.570\n1.972\n0.049\n0.043\n14.036\n\n\nSCC[T.yes]\n1.3664\n2.012\n0.679\n0.497\n-2.577\n5.309\n\n\nCALC[T.Frequently]\n-2.1001\n6.28e+06\n-3.34e-07\n1.000\n-1.23e+07\n1.23e+07\n\n\nCALC[T.Sometimes]\n-4.6772\n6.28e+06\n-7.45e-07\n1.000\n-1.23e+07\n1.23e+07\n\n\nCALC[T.no]\n-4.1972\n6.28e+06\n-6.68e-07\n1.000\n-1.23e+07\n1.23e+07\n\n\nMTRANS[T.Bike]\n-21.8420\n6.54e+09\n-3.34e-09\n1.000\n-1.28e+10\n1.28e+10\n\n\nMTRANS[T.Motorbike]\n3.2252\n47.781\n0.068\n0.946\n-90.423\n96.873\n\n\nMTRANS[T.Public_Transportation]\n8.8055\n1.416\n6.219\n0.000\n6.030\n11.581\n\n\nMTRANS[T.Walking]\n1.2540\n2.256\n0.556\n0.578\n-3.168\n5.676\n\n\nAge\n0.7030\n0.116\n6.086\n0.000\n0.477\n0.929\n\n\nHeight\n-104.6838\n9.021\n-11.605\n0.000\n-122.364\n-87.003\n\n\nWeight\n2.6259\n0.190\n13.819\n0.000\n2.253\n2.998\n\n\nFCVC\n0.1776\n0.764\n0.232\n0.816\n-1.320\n1.675\n\n\nNCP\n-1.8276\n0.608\n-3.007\n0.003\n-3.019\n-0.636\n\n\nCH2O\n-1.8930\n0.757\n-2.502\n0.012\n-3.376\n-0.410\n\n\nFAF\n-1.0280\n0.438\n-2.347\n0.019\n-1.887\n-0.169\n\n\nTUE\n0.1282\n0.670\n0.191\n0.848\n-1.186\n1.442\n\n\n\n\n\n\n\n\n\nEvaluating Model Performance\nTo gauge our models‚Äô effectiveness, we‚Äôll employ various metrics such as accuracy, precision, recall, and F1-score. A confusion matrix will help visualize how well our models perform in classifying outcomes‚Äîthink of it as a report card for your model!\n\n\nEvaluating the Logit model\n# Predict on test data\nbase_preds = logit_model.predict(data_df_test).idxmax(axis=1)\ny_test = data_df_test[target]\n\n# Evaluate the model\naccuracy_orig = accuracy_score(y_test, base_preds)\nreport_orig = classification_report(y_test, base_preds)\n\nprint(\"Accuracy:\", accuracy_orig)\nprint(\"Classification Report:\")\nprint(report_orig)\n\n\nAccuracy: 0.909952606635071\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.93      0.86      0.89        29\n           1       0.86      0.83      0.84        29\n           2       0.95      0.91      0.93        45\n           3       0.94      0.97      0.95        31\n           4       1.00      0.96      0.98        27\n           5       0.83      0.90      0.86        21\n           6       0.84      0.93      0.89        29\n\n    accuracy                           0.91       211\n   macro avg       0.91      0.91      0.91       211\nweighted avg       0.91      0.91      0.91       211\n\n\n\n\n\n\nLooking for some Improvments!\n\nFeature Selection Using CrossTab Sparsity\nNow comes the exciting part‚Äîusing CrossTab Sparsity to refine our feature selection process! It‚Äôs like cleaning up your closet and only keeping the clothes that spark joy (thank you, Marie Kondo). 1\n1¬†This is based on work in Unique Metric for Health Analysis with Optimization of Clustering Activity and Cross Comparison of Results from Different Approach. Paper LinkCode is here!\n\n\nStandared Steps for Feature Selection\n\nCalculate CrossTab Sparsity: For each feature against the target variable.\nSelect Features: Based on sparsity scores that indicate significant interactions with the target variable.\nRecreate Models: Train new models using only the selected features‚Äîless is often more!\n\nHere we go!!!\n\n\n\nDoing what needs to Done Code ;)\nsns.set_style(\"white\")\nsns.set_context(\"paper\")\n# Calculating Crostab sparsity for each Column\nresults = crosstab_sparsity(data_df_train.iloc[:,:-1],data_df_train[target],numeric_bin='decile')\n\n# presenting results for consumption\ndf_long = pd.melt(results['scores'], id_vars=['Columns'], value_vars=['seggregation', 'explaination', 'metric'],\n                  var_name='Metric', value_name='values')\n\n# Adding jitter: small random noise to 'Columns' (x-axis)\n# df_long['values_jittered'] = df_long['Value'] + np.random.uniform(-0.1, 0.1, size=len(df_long))\n\n# Create a seaborn scatter plot with jitter, more professional color palette, and transparency\nplt.figure(figsize=(12, 5))\nsns.scatterplot(x='Columns', y='values', hue='Metric', style='Metric',\n        data=df_long, s=100, alpha=0.7, palette='deep')\n\n# Title and labels\nplt.title('Metrics by Columns', fontsize=16)\nplt.xticks(rotation=45) \nplt.xlabel('Columns', fontsize=10)\nplt.ylabel('Value', fontsize=10)\n\n# Display legend outside the plot for better readability\nplt.legend(title='Metric', loc='upper right', fancybox=True, framealpha=0.5)\n\n# Show the plot\nplt.tight_layout()\nplt.show()\n\n\nCSP calculated with decile for breaks!\n\nScores for 7 groups(s) is : 140.96057955229762\n\n\n\n\n\n\n\n\n\n\n\n\nAnd Drum Rolls pelase!!!\nUsing just top 5 varaibles we are getting almost similar or better overall accuracy. This amounts to greatly simplifing the models and clearly explain why some variable are not useful for modeling.\n\n\nAnd finally training and evaluating with drum rolls\nlogit_model_rev = sm.MNLogit.from_formula(f\"{target} ~ {' + '.join(results['scores'].loc[:5,'Columns'].values)}\", \n    data=data_df_train\n).fit_regularized()\n\n# Predict on test data\nchallenger_preds = logit_model_rev.predict(data_df_test).idxmax(axis=1)\ny_test = data_df_test[target]\n\n# Evaluate the model\naccuracy_new = accuracy_score(y_test, challenger_preds)\nreport_new = classification_report(y_test, challenger_preds)\n\nprint(\"Accuracy:\", accuracy_new)\nprint(\"Classification Report:\")\nprint(report_new)\n\n\nSingular matrix E in LSQ subproblem    (Exit mode 5)\n            Current function value: nan\n            Iterations: 470\n            Function evaluations: 1227\n            Gradient evaluations: 470\nAccuracy: 0.9383886255924171\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.93      0.97      0.95        29\n           1       0.93      0.93      0.93        29\n           2       0.96      1.00      0.98        45\n           3       0.93      0.90      0.92        31\n           4       0.93      0.93      0.93        27\n           5       0.90      0.90      0.90        21\n           6       0.96      0.90      0.93        29\n\n    accuracy                           0.94       211\n   macro avg       0.94      0.93      0.93       211\nweighted avg       0.94      0.94      0.94       211\n\n\n\n/home/jitin/Documents/applications/perceptions/.venv/lib/python3.12/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n  warnings.warn(\"Maximum Likelihood optimization failed to \"\n\n\n\n\n\n\n\n\n\n\nNoneSummary of retrained model\n\n\n\n\n\n\n\nCode\ndisplay(logit_model_rev.summary())\n\n\n\nMNLogit Regression Results\n\n\nDep. Variable:\nNObeyesdad\nNo. Observations:\n1900\n\n\nModel:\nMNLogit\nDf Residuals:\n1858\n\n\nMethod:\nMLE\nDf Model:\n36\n\n\nDate:\nMon, 15 Dec 2025\nPseudo R-squ.:\nnan\n\n\nTime:\n16:07:34\nLog-Likelihood:\nnan\n\n\nconverged:\nFalse\nLL-Null:\n-3691.8\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\nnan\n\n\n\n\n\n\n\n\nNObeyesdad=1\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n58.1248\nnan\nnan\nnan\nnan\nnan\n\n\nTUE\n0.1130\nnan\nnan\nnan\nnan\nnan\n\n\nCH2O\n-0.8634\nnan\nnan\nnan\nnan\nnan\n\n\nFAF\n0.1425\nnan\nnan\nnan\nnan\nnan\n\n\nAge\n0.0579\nnan\nnan\nnan\nnan\nnan\n\n\nHeight\n-76.5735\nnan\nnan\nnan\nnan\nnan\n\n\nWeight\n1.3337\nnan\nnan\nnan\nnan\nnan\n\n\nNObeyesdad=2\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n328.4616\nnan\nnan\nnan\nnan\nnan\n\n\nTUE\n2.2275\nnan\nnan\nnan\nnan\nnan\n\n\nCH2O\n-1.4150\nnan\nnan\nnan\nnan\nnan\n\n\nFAF\n-1.3585\nnan\nnan\nnan\nnan\nnan\n\n\nAge\n0.1537\nnan\nnan\nnan\nnan\nnan\n\n\nHeight\n-426.3945\nnan\nnan\nnan\nnan\nnan\n\n\nWeight\n5.3584\nnan\nnan\nnan\nnan\nnan\n\n\nNObeyesdad=3\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n306.6447\nnan\nnan\nnan\nnan\nnan\n\n\nTUE\n-7.8630\nnan\nnan\nnan\nnan\nnan\n\n\nCH2O\n-21.0118\nnan\nnan\nnan\nnan\nnan\n\n\nFAF\n-11.3624\nnan\nnan\nnan\nnan\nnan\n\n\nAge\n2.4017\nnan\nnan\nnan\nnan\nnan\n\n\nHeight\n-710.3867\nnan\nnan\nnan\nnan\nnan\n\n\nWeight\n10.1072\nnan\nnan\nnan\nnan\nnan\n\n\nNObeyesdad=4\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n352.4249\nnan\nnan\nnan\nnan\nnan\n\n\nTUE\n-9.2469\nnan\nnan\nnan\nnan\nnan\n\n\nCH2O\n-20.6780\nnan\nnan\nnan\nnan\nnan\n\n\nFAF\n-14.7525\nnan\nnan\nnan\nnan\nnan\n\n\nAge\n2.1487\nnan\nnan\nnan\nnan\nnan\n\n\nHeight\n-758.2318\nnan\nnan\nnan\nnan\nnan\n\n\nWeight\n10.5011\nnan\nnan\nnan\nnan\nnan\n\n\nNObeyesdad=5\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n126.2892\nnan\nnan\nnan\nnan\nnan\n\n\nTUE\n0.5832\nnan\nnan\nnan\nnan\nnan\n\n\nCH2O\n-0.8764\nnan\nnan\nnan\nnan\nnan\n\n\nFAF\n-0.1920\nnan\nnan\nnan\nnan\nnan\n\n\nAge\n0.0719\nnan\nnan\nnan\nnan\nnan\n\n\nHeight\n-160.2982\nnan\nnan\nnan\nnan\nnan\n\n\nWeight\n2.3663\nnan\nnan\nnan\nnan\nnan\n\n\nNObeyesdad=6\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n207.3760\nnan\nnan\nnan\nnan\nnan\n\n\nTUE\n1.6561\nnan\nnan\nnan\nnan\nnan\n\n\nCH2O\n-0.6583\nnan\nnan\nnan\nnan\nnan\n\n\nFAF\n-0.1243\nnan\nnan\nnan\nnan\nnan\n\n\nAge\n0.1042\nnan\nnan\nnan\nnan\nnan\n\n\nHeight\n-266.6050\nnan\nnan\nnan\nnan\nnan\n\n\nWeight\n3.6160\nnan\nnan\nnan\nnan\nnan\n\n\n\n\n\n\n\n\n\n\n\nImpact on Model Accuracy\nAfter applying feature selection based on CrossTab Sparsity, we‚Äôll compare the accuracy of our new models against our baseline models. This comparison will reveal how effectively CrossTab Sparsity enhances classification performance.\n\nResults and Discussion: Unveiling Insights\nModel Comparison Table\nAfter implementing CrossTab Sparsity in our feature selection process, let‚Äôs take a look at the results:\n\n\nComparision Code\nmetrics = {\n    \"Metric\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"],\n    \"Baseline Model with all Parameters\": [\n        accuracy_score(y_test, base_preds),\n        precision_score(y_test, base_preds, average='weighted'),\n        recall_score(y_test, base_preds, average='weighted'),\n        f1_score(y_test, base_preds, average='weighted'),\n    ],\n    \"Challenger Model with only 5 Variables\": [\n        accuracy_score(y_test, challenger_preds),\n        precision_score(y_test, challenger_preds, average='weighted'),\n        recall_score(y_test, challenger_preds, average='weighted'),\n        f1_score(y_test, challenger_preds, average='weighted'),\n    ]\n}\ndisplay(pd.DataFrame(metrics).round(4).set_index('Metric').T)\n\n\n\n\n\n\n\n\nMetric\nAccuracy\nPrecision\nRecall\nF1-Score\n\n\n\n\nBaseline Model with all Parameters\n0.9100\n0.9123\n0.9100\n0.9103\n\n\nChallenger Model with only 5 Variables\n0.9384\n0.9384\n0.9384\n0.9381\n\n\n\n\n\n\n\nInsights Gained\nThrough this analysis, several key insights emerge:\n\n\nReduction of similar accuracy from 16 to 5 i.e 68.75% reduction\n\n\n\nFeature Interactions Matter: The selected features based on CrossTab Sparsity significantly improved model accuracy‚Äîlike finding out which ingredients make your favorite dish even better!\nSimplicity is Key: By focusing on relevant features, we enhance accuracy while simplifying model interpretation‚Äîbecause nobody likes unnecessary complexity.\nReal-World Applications: These findings have practical implications in fields such as environmental science where classification plays a critical role‚Äîhelping us make better decisions for our planet.\n\n\n\n\nConclusion: The Road Ahead\nIn conclusion, this blog has illustrated how CrossTab Sparsity can be a game-changer in classification tasks using the Obesity dataset. By leveraging this metric for feature selection, we achieved notable improvements in model performance‚Äîproof that sometimes less really is more!\nFuture Work: Expanding Horizons\nAs we look ahead, there are exciting avenues to explore:\n\nInvestigating regression problems using CrossTab Sparsity.\nComparing its effectiveness with other feature selection methods such as Recursive Feature Elimination (RFE) or comparision with other feature selection mehtods.\n\nBy continuing this journey into data science, we not only enhance our technical skills but also contribute valuable insights that can drive meaningful change in various industries.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Insights",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nDecision-First AI: Why Data Should Follow, Not Lead\n\n\n\nstrategy\n\nroi\n\nbusiness\n\nai\n\n\n\n\nOct 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrieKNN: Unleashing KNN‚Äôs Power on Mixed Data Types\n\n\n\nknn\n\nml\n\nmixed-data\n\n\n\n\nFeb 26, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nCrossTab Sparsity for Classification\n\n\n\nclassification\n\nmetric\n\nfeature selection\n\n\n\n\nJan 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nCrossTab Sparsity\n\n\n\nclustering\n\nanalysis\n\n\n\n\nMay 3, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nA flow to Test Your Hypothesis in Python\n\n\n\neda\n\nhypothesis\n\nanalysis\n\npython\n\n\n\n\nAug 10, 2021\n\n\n\n\n\n\n\n\n\n\n\nAdaptive Regression\n\n\n\nstrategy\n\n\n\n\nMay 1, 2018\n\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "consulting/index.html",
    "href": "consulting/index.html",
    "title": "Work Directly With Me",
    "section": "",
    "text": "After 14 years architecting AI systems across automotive, retail, insurance, FMCG, and telecommunications, I‚Äôve learned one thing:\nThe best solutions come from looking sideways.\nMost consultants stay within their industry expertise. I bring patterns from completely different domains to solve problems that seem intractable.\nHealthcare insights solving automotive challenges. Retail strategies transforming manufacturing operations. Mathematical frameworks from applied physics applied to business systems."
  },
  {
    "objectID": "consulting/index.html#the-approach",
    "href": "consulting/index.html#the-approach",
    "title": "Work Directly With Me",
    "section": "",
    "text": "After 14 years architecting AI systems across automotive, retail, insurance, FMCG, and telecommunications, I‚Äôve learned one thing:\nThe best solutions come from looking sideways.\nMost consultants stay within their industry expertise. I bring patterns from completely different domains to solve problems that seem intractable.\nHealthcare insights solving automotive challenges. Retail strategies transforming manufacturing operations. Mathematical frameworks from applied physics applied to business systems."
  },
  {
    "objectID": "consulting/index.html#what-well-work-on",
    "href": "consulting/index.html#what-well-work-on",
    "title": "Work Directly With Me",
    "section": "What We‚Äôll Work On",
    "text": "What We‚Äôll Work On\n\n\nAI Strategy & Architecture\nMap your AI maturity. Design scalable systems. Build transformation roadmaps that actually deploy.\n\n\nCross-Industry Pattern Recognition\nApply insights from adjacent industries. Solve problems others can‚Äôt see because they‚Äôre looking straight ahead.\n\n\nExecutive Team Alignment\nGet your leadership on the same page about AI transformation. No vendor pitches. Just architecture.\n\n\nImplementation Support\nI don‚Äôt just hand you a deck and leave. I work alongside your team until the system is live and scaled."
  },
  {
    "objectID": "consulting/index.html#who-this-is-for",
    "href": "consulting/index.html#who-this-is-for",
    "title": "Work Directly With Me",
    "section": "Who This Is For",
    "text": "Who This Is For\nFortune 500 enterprises with real transformation budgets.\nLeaders who are tired of prescriptive AI and ready for predictive strategy.\nOrganizations ready to commit to systemic change, not point solutions.\nThis isn‚Äôt for everyone. But if you‚Äôre the right fit, the ROI is undeniable."
  },
  {
    "objectID": "consulting/index.html#current-engagement-model",
    "href": "consulting/index.html#current-engagement-model",
    "title": "Work Directly With Me",
    "section": "Current Engagement Model",
    "text": "Current Engagement Model\nI work with a limited number of clients at any given time, typically serving as Acting CTO or Strategic AI Advisor across multiple accounts.\nTypical engagement: 6-12 months, embedded with your leadership team.\nInvestment: Custom scoped based on organization size and transformation goals."
  },
  {
    "objectID": "consulting/index.html#apply-for-consulting",
    "href": "consulting/index.html#apply-for-consulting",
    "title": "Work Directly With Me",
    "section": "Apply for Consulting",
    "text": "Apply for Consulting\nCurrent waitlist: [Q2 2025]\nIf you‚Äôre exploring a major AI transformation and want to discuss whether we‚Äôre a fit, let‚Äôs talk.\nSchedule a Discovery Call\n\nOr email me directly at jitin@jitinkapila.com"
  },
  {
    "objectID": "faq.html",
    "href": "faq.html",
    "title": "Frequently Asked Questions (FAQs)",
    "section": "",
    "text": "Uh OH! :(\n\n\n\n\nDo I need to know how to code?\n\n\n\nNo.¬†The AI Profit OS is designed for senior professionals, not engineers. Everything is built in Excel + simple tools you already use. If you can use Google Sheets, you can build these systems.\n\n\n\n\n\nI‚Äôm not technical. Will I be able to follow?\n\n\n\nAbsolutely. This training is designed for business leaders, not data scientists. I‚Äôve trained CFOs, COOs, and Senior Directors with zero technical background. If you can manage a P&L, you can operationalize AI.\n\n\n\n\n\nWhat if I can‚Äôt attend all 3 days live?\n\n\n\nYou‚Äôll get lifetime recording access. However, the live workshops where you build YOUR specific systems are where the real value comes from. We strongly recommend attending live.\n\n\n\n\n\nIs this just ChatGPT training?\n\n\n\nNo.¬†ChatGPT is one tool in the AI Profit OS, but the system is much bigger. You‚Äôll learn: (a) How to structure your workflow for AI leverage, (b) How to build repeatable systems (not one-off prompts), (c) How to measure ROI and scale what works.\n\n\n\n\n\nWhat industries does this work for?\n\n\n\nIf you‚Äôre a senior professional doing knowledge work, this works. I‚Äôve deployed AI systems across automotive, retail, FMCG, telecommunications, finance, and healthcare. The frameworks are industry-agnostic.\n\n\n\n\n\nWhat‚Äôs the refund policy?\n\n\n\nThe ‚ÄúUnheard Of‚Äù Guarantee (see above). If you implement the system and don‚Äôt save 10+ hours/week within 30 days, refund + ‚Çπ5,000 paid to you. After 30 days, no refunds - but you can transfer your seat to a colleague.\n\n\n\n\n\nCan I bring my team?\n\n\n\nYes! We offer team pricing for 3+ participants from the same organization. Email jitin@jitinkapila.com for custom team rates. (Recommended: Bring 2-3 key leaders to accelerate adoption.)\n\n\n\n\n\nHow is this different from other AI courses?\n\n\n\nMost AI courses teach theory or simple prompting. The AI Profit OS is a complete operational system you‚Äôll deploy in your workflow. You‚Äôre not learning about AI - you‚Äôre building leverage. By Day 3, you have working systems saving you time.\n\n\n\n\n\nWhat if AI changes? Will this become outdated?\n\n\n\nThe OLCD Protocol, AI-ART Matrix, and operational frameworks are model-agnostic. Whether it‚Äôs ChatGPT, Claude, Gemini, or whatever comes next, the system adapts. Plus, you get lifetime access to framework updates.\n\n\n\n\n\n\n\n\nI‚Äôm not sure this will work for my specific industry/role.\n\n\n\nThat‚Äôs exactly why Day 1 is focused on YOUR workflow and YOUR 90-day roadmap. The frameworks are industry-agnostic, but the implementation is personalized to your reality.\n\n\n\n\n\n‚Çπ7,999 feels expensive for a training.\n\n\n\nCompare it to: - (a) One hour of my consulting at ‚Çπ25,000/hour, -(b) The cost of being replaced by a 22-year-old who knows how to use AI, -(c) The ‚Çπ6.9Cr the grocery chain saved with this exact system. If you don‚Äôt save 10+ hours/week, I‚Äôll refund you AND pay you ‚Çπ8,000.\n\n\n\n\n\nWhat if I can‚Äôt implement this alone?\n\n\n\nYou won‚Äôt be alone. You‚Äôll join a private community of senior professionals building AI systems. Plus monthly office hours with me. And if you need hands-on implementation support, 1:1 consulting is available.\n\n\n\n\n\nHow do I know this isn‚Äôt just hype?\n\n\n\nLook at the results: ‚Çπ6.9Cr grocery chain savings. $11M automotive optimization. ¬£80K/month retail revenue lift. These are real deployments at Mercedes, L‚ÄôOr√©al, Maruti, British Telecom. This isn‚Äôt theory - it‚Äôs operational leverage.\n\n\n\n\n\nWhat if AI replaces me anyway?\n\n\n\nAI will replace professionals who DON‚ÄôT learn to operationalize it. Senior professionals who build AI systems become 10x more valuable. You‚Äôre not competing with AI - you‚Äôre building leverage ON TOP of your 10+ years of expertise.\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "policies.html#refund-policy",
    "href": "policies.html#refund-policy",
    "title": "Privacy",
    "section": "Refund Policy",
    "text": "Refund Policy"
  },
  {
    "objectID": "policies.html#terms-and-conditions",
    "href": "policies.html#terms-and-conditions",
    "title": "Privacy",
    "section": "Terms and Conditions",
    "text": "Terms and Conditions"
  }
]