---
title: "Corsstab Sparsity"
subtitle: "A label and data type agnostic metric for evaluating clustering performance!"

# Enable CC licence appendix
license: "CC BY"

# Default author
author:
  - name: Jitin Kapila
    url: https://www.percieveit.com/about


date: "2022-05-03"
categories: [eda, hypothesis]
keywords: [Metric, clustering]
lightbox: true
format:
  html:
    grid:
      margin-width: 350px 
reference-location: margin
citation-location: margin

draft: false
---


# Introduction  
Cluster analysis has always fascinated me as a window into the hidden structures of data. During my collaboration with Kumarjit Pathak, we grappled with a persistent challenge in unsupervised learning: **how to objectively evaluate clustering quality across different algorithms**. Traditional metrics like the Silhouette Index or Bayesian Information Criterion felt restrictive—they were siloed within specific methodologies, making cross-algorithm comparisons unreliable.  

This frustration led us to develop a **universal cluster evaluation metric**, detailed in our paper *"Cross Comparison of Results from Different Clustering Approaches"*. Our goal was to create a framework that transcends algorithmic biases, enabling:  
- Direct comparison of K-Means vs GMM vs DBSCAN vs PAM vs SOM vs Anything results  
- Identification of variables muddying cluster separation  
- Automated determination of optimal cluster counts  

In this blog, I’ll walk you through our journey—from conceptualization to real-world validation—and share insights that didn’t make it into the final paper.  


# The Birth of the Metric: A First-Person Perspective  

***Why Existing Methods Fell Short***  
Early in our research, we cataloged limitations of popular evaluation techniques:  

1. Method Dependency  
- Silhouette scores worked beautifully for K-Means but faltered with Gaussian Mixture Models (GMM).  
- Probability-based metrics like BIC couldn’t handle distance-based clusters.  

2. Noise Blindness  
Noisy variables often contaminated clusters, but traditional methods required manual outlier detection.  

3. Subjective Optimization* 
Elbow plots and dendrograms left too much room for human interpretation.  

## Our "Aha!" Moment - Crosstab Sparsity 

![Best Cluster for K-means Using Crosstab sparsity](Picture 1.png)

While analyzing cross-tab matrices of variable distributions across clusters, we noticed a pattern: **well-segregated clusters consistently showed higher frequencies along matrix diagonals**. This inspired our two-part metric:  

::: {.column-margin}
1. **Segregation Factor**:  
   ```python  
   # Simplified calculation from our codebase  
   median = np.median(cross_tab)  
   N_vk = np.sum(cross_tab > median)  # Count "well-segregated" instances  
   ```
    
2. **Explanation Factor**:  
   ```python  
   explanation = np.log(len(data) / (bins * clusters))  
   ```
     
:::


1. **Segregation Factor**: Measures how distinctly clusters separate data points. We used the median (not mean) to avoid skew from outlier-dominated matrices.
2. **Explanation Factor**: Quantifies how well clusters capture data variability. The logarithmic term penalizes overfitting—a critical insight from debugging early over-segmented clusters.  


**And the Final Formula**:  
For variable $v$ with $k$ clusters:  
$$
S_v^k = \underbrace{\frac{N_v^k}{\max(l, k)}}_{\text{Segregation}} \times \underbrace{\ln\left(\frac{N_d}{l \times k}\right)}_{\text{Explanation}}
$$  

where:  
- $N_v^k$: Segregated instances (values above cross-tab matrix median)  
- $l$: Number of value intervals for variable $v$  
- $N_d$: Total observations  

This formulation ensures **algorithmic invariance**, allowing comparison across methods like K-Means (distance-based) and GMM (probability-based). Also, now you can see from the formula two scenarios happens:
1. If each variable crosstab is too dense then their is no separation between classes
2. If each variable crosstab is too sparse then we loose on explaination.

Hence the curve reaches a maximum and then falls down giving use the sepratabiltiy that the cluster can produce:


# Case Study: Vehicle Silhouettes (Through My Eyes)  

### The Dataset That Almost Broke Us  
We tested our metric on a vehicle silhouette dataset with 18 shape-related features (e.g., compactness, circularity). Initially, inconsistent results plagued us—until we realized our binning strategy for continuous variables was flawed.  

::: {.column-margin}
**Key Adjustments**:  
- Switched from equal-width to **quantile-based binning** (10 bins per variable).  
- For categorical variables, retained native levels instead of coercing bins.  
:::

### The Breakthrough  

After refining the preprocessing:    
> Optimal Clusters: Our metric plateaued at $k=6$ , aligning perfectly with known vehicle categories (sedans, trucks, etc.).   
> Noise Detection: Variables like *Max.LWR* (length-width ratio) scored poorly, revealing inconsistent clustering. We later found this was due to manufacturers’ design variances.  

Finding best cluster for K-Means alone:
!["Best Cluster for PAM method Using Crosstab sparsity"](Kmeans on Vehicle.png)

Comparing all cluster methonds and find the optimal one:

![Optimal Cluster for many methods](Comparision across many methods.png)

**The chunkiest part** : Understanding your variable for separateness. This gives direct insight of what variable in your data is most critical separator.
 
 ![All kind of variable scored angainst Metrics](Variable segregation.png)


# Comparative Advantages and Creativty at Work  

**_Comparative Advantage Over Traditional Metrics_**

| **Feature/Scanairo**      | **Silhouette Index**       | **Davies-Bouldin**         | **Crosstab Sparsity**        |  
|---------------------------|-----------------------------|----------------------------|----------------------------|  
| Algorithm Agnostic    | ❌ (Distance-based only)     | ❌                          | ✔️                          |  
| Handles Mixed Data     | ❌                          | ❌                          | ✔️                          |  
| Identifies Noisy Vars | ❌                          | ❌                          | ✔️                          |  
| Optimal Cluster Detection | Manual elbow analysis   | Manual analysis            | Automated plateau detection|
| Mixed Algorithms       | Failed (GMM vs K-Means)     |  Failed (needs numerical data) |Achieved 92% consistency[1]  |  
| Noisy Variables        | Manual outlier removal      | Manual outlier removal    |  Auto-detected (e.g., Max.LWR) |  
| Optimal Cluster Detection | Subjective elbow plots    | Subjective to Elbow plots | Objective plateau detection  |  


<br>


Our creativity yeilding boons. We wnated a simple metric to judge different kind of cluster, but we got much more from our experiments and work on this metric:  

1. Variable-Level Diagnostics: Low $S_v^k$ scores pinpoint variables muddying cluster separation.  
2. Cross-Method Benchmarking: Compare K-Means (distance) vs GMM (probability) vs hierarchical vs partitional clustering fairly using a unified score.  
3. Scale Invariance: Logarithmic term makes scores comparable across datasets of varying sizes.  
4. Debug Cluster Quality: Identify and remove noisy variables preemptively  
5. Automate Model Selection: Objectively choose between K-Means, GMM, PAM, Agglomerative.  


# Lessons Learned and Future Vision  

**Few take aways from these experiments**  
1. Binning Sensitivity: Quantile-based binning was transformative. Equal-width bins distorted scores for skewed variables.  
2. Categorical Handling: Native levels for categoricals outperformed frequency-based grouping.  
3. Non-Parametric Appraoch: This approach allowed us to make sense of data without being tied down by assumptions. We have seen how this metric can be a game-changer for statisticians, providing insights not just into cluster behavior but also into rare event modeling.

The plots from these experiments not only clarify how clusters behave but also offer valuable insights for identifying outliers. I believe there's exciting potential to extend this metric into classification and value estimation modeling. Imagine using it as a loss function in both linear and non-linear methods to achieve better data segmentation! Thing for another blog someday!

### A Personal Reflection  
Developing this metric taught me that **simplicity often masks depth**. A two-component formula now underpins clustering decisions in industries we never imagined—from fraud detection to genomics. Yet, I’m most proud of how it democratizes cluster analysis: business analysts at our partner firms now optimize clusters without PhD-level stats.  


### Try It Yourself  

Quickstart Guide


1. **Preprocess Data**:  
   ```python  
   from sklearn.cluster import KMeans  
   import pandas as pd  

   # Cluster your data  
   kmeans = KMeans(n_clusters=6)  
   clusters = kmeans.fit_predict(X)  
   ```

2. **Calculate Scores**:  
   ```python  
   def calculate_Svk(df, variable, clusters, bins=10):  
       # Discretize variable  
       discretized = pd.qcut(df[variable], bins, labels=False)  
       cross_tab = pd.crosstab(clusters, discretized)
       l, k = cross_tab.shape  

       # Segregation factor  
       median = np.median(cross_tab.values)  
       N_vk = np.sum(cross_tab > median)  
       segregation = (N_vk / max(l, k))

       # Explanation factor  
       explanation = np.log(len(df) / (l * k))  

       return  segregation * explanation, segregation, explanation  
   ```


```{python}
#|eval: false
#|echo : false

#### Resources  
# - [GitHub Repo](https://github.com/jitinkapila/cluster-metric) with experiment code  
# - [Interactive Tutorial](https://colab.research.google.com) using the vehicle dataset  

```

*This blog synthesizes findings from our original paper, available [here](https://arxiv.org/abs/1810.03419). For a deeper dive into the math, check Section 3 of the paper.*  

**To my readers**: Have you tried implementing cross-algorithm clustering? Share your war stories in the comments—I’d love to troubleshoot together!

