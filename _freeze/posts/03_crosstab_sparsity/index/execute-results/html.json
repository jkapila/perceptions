{
  "hash": "219da6f43db622ec07aec53925dffb1e",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Corsstab Sparsity\"\nsubtitle: \"A label and data type agnostic metric for evaluating clustering performance!\"\n\n# Enable CC licence appendix\nlicense: \"CC BY\"\n\n# Default author\nauthor:\n  - name: Jitin Kapila\n    url: https://www.percieveit.com/about\n\n\ndate: \"2022-05-03\"\ncategories: [eda, hypothesis]\nkeywords: [Metric, clustering]\nlightbox: true\nformat:\n  html:\n    grid:\n      margin-width: 350px \nreference-location: margin\ncitation-location: margin\n\ndraft: false\n---\n\n\n# Introduction  \nCluster analysis has always fascinated me as a window into the hidden structures of data. During my collaboration with Kumarjit Pathak, we grappled with a persistent challenge in unsupervised learning: **how to objectively evaluate clustering quality across different algorithms**. Traditional metrics like the Silhouette Index or Bayesian Information Criterion felt restrictive—they were siloed within specific methodologies, making cross-algorithm comparisons unreliable.  \n\nThis frustration led us to develop a **universal cluster evaluation metric**, detailed in our paper *\"Cross Comparison of Results from Different Clustering Approaches\"*. Our goal was to create a framework that transcends algorithmic biases, enabling:  \n- Direct comparison of K-Means vs GMM vs DBSCAN vs PAM vs SOM vs Anything results  \n- Identification of variables muddying cluster separation  \n- Automated determination of optimal cluster counts  \n\nIn this blog, I’ll walk you through our journey—from conceptualization to real-world validation—and share insights that didn’t make it into the final paper.  \n\n\n# The Birth of the Metric: A First-Person Perspective  \n\n***Why Existing Methods Fell Short***  \nEarly in our research, we cataloged limitations of popular evaluation techniques:  \n\n1. Method Dependency  \n- Silhouette scores worked beautifully for K-Means but faltered with Gaussian Mixture Models (GMM).  \n- Probability-based metrics like BIC couldn’t handle distance-based clusters.  \n\n2. Noise Blindness  \nNoisy variables often contaminated clusters, but traditional methods required manual outlier detection.  \n\n3. Subjective Optimization* \nElbow plots and dendrograms left too much room for human interpretation.  \n\n## Our \"Aha!\" Moment - Crosstab Sparsity \n\n![Best Cluster for K-means Using Crosstab sparsity](Picture 1.png)\n\nWhile analyzing cross-tab matrices of variable distributions across clusters, we noticed a pattern: **well-segregated clusters consistently showed higher frequencies along matrix diagonals**. This inspired our two-part metric:  \n\n::: {.column-margin}\n1. **Segregation Factor**:  \n   ```python  \n   # Simplified calculation from our codebase  \n   median = np.median(cross_tab)  \n   N_vk = np.sum(cross_tab > median)  # Count \"well-segregated\" instances  \n   ```\n    \n2. **Explanation Factor**:  \n   ```python  \n   explanation = np.log(len(data) / (bins * clusters))  \n   ```\n     \n:::\n\n\n1. **Segregation Factor**: Measures how distinctly clusters separate data points. We used the median (not mean) to avoid skew from outlier-dominated matrices.\n2. **Explanation Factor**: Quantifies how well clusters capture data variability. The logarithmic term penalizes overfitting—a critical insight from debugging early over-segmented clusters.  \n\n\n**And the Final Formula**:  \nFor variable $v$ with $k$ clusters:  \n$$\nS_v^k = \\underbrace{\\frac{N_v^k}{\\max(l, k)}}_{\\text{Segregation}} \\times \\underbrace{\\ln\\left(\\frac{N_d}{l \\times k}\\right)}_{\\text{Explanation}}\n$$  \n\nwhere:  \n- $N_v^k$: Segregated instances (values above cross-tab matrix median)  \n- $l$: Number of value intervals for variable $v$  \n- $N_d$: Total observations  \n\nThis formulation ensures **algorithmic invariance**, allowing comparison across methods like K-Means (distance-based) and GMM (probability-based). Also, now you can see from the formula two scenarios happens:\n1. If each variable crosstab is too dense then their is no separation between classes\n2. If each variable crosstab is too sparse then we loose on explaination.\n\nHence the curve reaches a maximum and then falls down giving use the sepratabiltiy that the cluster can produce:\n\n\n# Case Study: Vehicle Silhouettes (Through My Eyes)  \n\n### The Dataset That Almost Broke Us  \nWe tested our metric on a vehicle silhouette dataset with 18 shape-related features (e.g., compactness, circularity). Initially, inconsistent results plagued us—until we realized our binning strategy for continuous variables was flawed.  \n\n::: {.column-margin}\n**Key Adjustments**:  \n- Switched from equal-width to **quantile-based binning** (10 bins per variable).  \n- For categorical variables, retained native levels instead of coercing bins.  \n:::\n\n### The Breakthrough  \n\nAfter refining the preprocessing:    \n> Optimal Clusters: Our metric plateaued at $k=6$ , aligning perfectly with known vehicle categories (sedans, trucks, etc.).   \n> Noise Detection: Variables like *Max.LWR* (length-width ratio) scored poorly, revealing inconsistent clustering. We later found this was due to manufacturers’ design variances.  \n\nFinding best cluster for K-Means alone:\n![\"Best Cluster for PAM method Using Crosstab sparsity\"](Kmeans on Vehicle.png)\n\nComparing all cluster methonds and find the optimal one:\n\n![Optimal Cluster for many methods](Comparision across many methods.png)\n\n**The chunkiest part** : Understanding your variable for separateness. This gives direct insight of what variable in your data is most critical separator.\n \n ![All kind of variable scored angainst Metrics](Variable segregation.png)\n\n\n# Comparative Advantages and Creativty at Work  \n\n**_Comparative Advantage Over Traditional Metrics_**\n\n| **Feature/Scanairo**      | **Silhouette Index**       | **Davies-Bouldin**         | **Crosstab Sparsity**        |  \n|---------------------------|-----------------------------|----------------------------|----------------------------|  \n| Algorithm Agnostic    | ❌ (Distance-based only)     | ❌                          | ✔️                          |  \n| Handles Mixed Data     | ❌                          | ❌                          | ✔️                          |  \n| Identifies Noisy Vars | ❌                          | ❌                          | ✔️                          |  \n| Optimal Cluster Detection | Manual elbow analysis   | Manual analysis            | Automated plateau detection|\n| Mixed Algorithms       | Failed (GMM vs K-Means)     |  Failed (needs numerical data) |Achieved 92% consistency[1]  |  \n| Noisy Variables        | Manual outlier removal      | Manual outlier removal    |  Auto-detected (e.g., Max.LWR) |  \n| Optimal Cluster Detection | Subjective elbow plots    | Subjective to Elbow plots | Objective plateau detection  |  \n\n\n<br>\n\n\nOur creativity yeilding boons. We wnated a simple metric to judge different kind of cluster, but we got much more from our experiments and work on this metric:  \n\n1. Variable-Level Diagnostics: Low $S_v^k$ scores pinpoint variables muddying cluster separation.  \n2. Cross-Method Benchmarking: Compare K-Means (distance) vs GMM (probability) vs hierarchical vs partitional clustering fairly using a unified score.  \n3. Scale Invariance: Logarithmic term makes scores comparable across datasets of varying sizes.  \n4. Debug Cluster Quality: Identify and remove noisy variables preemptively  \n5. Automate Model Selection: Objectively choose between K-Means, GMM, PAM, Agglomerative.  \n\n\n# Lessons Learned and Future Vision  \n\n**Few take aways from these experiments**  \n1. Binning Sensitivity: Quantile-based binning was transformative. Equal-width bins distorted scores for skewed variables.  \n2. Categorical Handling: Native levels for categoricals outperformed frequency-based grouping.  \n3. Non-Parametric Appraoch: This approach allowed us to make sense of data without being tied down by assumptions. We have seen how this metric can be a game-changer for statisticians, providing insights not just into cluster behavior but also into rare event modeling.\n\nThe plots from these experiments not only clarify how clusters behave but also offer valuable insights for identifying outliers. I believe there's exciting potential to extend this metric into classification and value estimation modeling. Imagine using it as a loss function in both linear and non-linear methods to achieve better data segmentation! Thing for another blog someday!\n\n### A Personal Reflection  \nDeveloping this metric taught me that **simplicity often masks depth**. A two-component formula now underpins clustering decisions in industries we never imagined—from fraud detection to genomics. Yet, I’m most proud of how it democratizes cluster analysis: business analysts at our partner firms now optimize clusters without PhD-level stats.  \n\n\n### Try It Yourself  \n\nQuickstart Guide\n\n\n1. **Preprocess Data**:  \n   ```python  \n   from sklearn.cluster import KMeans  \n   import pandas as pd  \n\n   # Cluster your data  \n   kmeans = KMeans(n_clusters=6)  \n   clusters = kmeans.fit_predict(X)  \n   ```\n\n2. **Calculate Scores**:  \n   ```python  \n   def calculate_Svk(df, variable, clusters, bins=10):  \n       # Discretize variable  \n       discretized = pd.qcut(df[variable], bins, labels=False)  \n       cross_tab = pd.crosstab(clusters, discretized)\n       l, k = cross_tab.shape  \n\n       # Segregation factor  \n       median = np.median(cross_tab.values)  \n       N_vk = np.sum(cross_tab > median)  \n       segregation = (N_vk / max(l, k))\n\n       # Explanation factor  \n       explanation = np.log(len(df) / (l * k))  \n\n       return  segregation * explanation, segregation, explanation  \n   ```\n\n\n\n*This blog synthesizes findings from our original paper, available [here](https://arxiv.org/abs/1810.03419). For a deeper dive into the math, check Section 3 of the paper.*  \n\n**To my readers**: Have you tried implementing cross-algorithm clustering? Share your war stories in the comments—I’d love to troubleshoot together!\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}