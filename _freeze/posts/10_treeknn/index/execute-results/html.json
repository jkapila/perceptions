{
  "hash": "bf8e602083149b563587d698989aeed6",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"TrieKNN: Unleashing KNN's Power on Mixed Data Types\"\ndescription: \"Discover TrieKNN, a novel approach to extend the K-Nearest Neighbors algorithm to datasets with both categorical and numerical features. Learn how it works, see it in action, and explore its potential.\"\ndate: \"2025-02-26\"\ncategories: [knn, ml, mixed-data]\ntags: [KNN, Trie, Mixed Data Types, Machine Learning, Classification, Categorical Data, Numerical Data]\nimage: pexels-anntarazevich-7299985.jpg \ndraft: false\n---\n\n![[Photo by Gelgas Airlangga](https://www.pexels.com/photo/shallow-focus-of-sprout-401213/)](pexels-gelgas-401213.jpg)\n\n<!-- [Photo by Anna Tarazevich](https://www.pexels.com/photo/strawberry-plant-on-a-black-container-7299985/)\n\n[Photo by Eva Bronzini](https://www.pexels.com/photo/succulent-plants-in-pot-shaped-soil-7127801/) -->\n\n\n:::{.callout-note}\n## In This Post\n\n-   We'll dissect the limitations of traditional KNN when faced with mixed data types.\n-   Introduce TrieKNN, a Trie-based approach that elegantly handles mixed data.\n-   Walk through the implementation and training of a TrieKNN model.\n-   Evaluate its performance and discuss its potential impact.\n:::\n\n## The Allure and Limitation of KNN\n\nIn the realm of machine learning, the K-Nearest Neighbors (KNN) algorithm stands out for its intuitive nature and ease of implementation. Its principle is simple: classify a data point based on the majority class among its 'k' nearest neighbors in the feature space. This non-parametric approach makes no assumptions about the underlying data distribution, rendering it versatile for various applications.\n[KNN](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) is very popular, but it comes with some limitations.\n\nHowever, KNN's Achilles' heel lies in its reliance on distance metrics, which are inherently designed for numerical data. Real-world datasets often contain a mix of numerical and categorical features, posing a significant challenge for KNN. How do you measure the distance between 'red' and 'blue,' or 'large' and 'small'?\n\n### Prior Art\n\nSeveral strategies have been proposed to adapt KNN for mixed data:\n\n-   **One-Hot Encoding:** Converts categorical features into numerical vectors, but can lead to high dimensionality.\n-   **Distance Functions for Mixed Data:** Develops and apply custom distance metrics that can handle both numerical and categorical features such as [HEOM and many others](https://conservancy.umn.edu/server/api/core/bitstreams/845f587d-079a-469b-97e9-411533fa666d/content).\n-   **Using mean/mode values**: Replace the missing values with mean/mode.\n\nThese methods often involve compromises, either distorting the data's inherent structure or adding computational overhead.\n\n## Enter TrieKNN: A Novel Approach\n\nWhat if we could cleverly sidestep the distance calculation problem for categorical features, while still leveraging KNN's power? TrieKNN offers just thatâ€”a way to perform KNN on any mixed data!\n\nTrieKNN combines the strengths of Trie data structures and KNN to handle mixed data types gracefully. Here's the core idea:\n\n1.  **Trie-Based Categorical Encoding:** A Trie is used to store the categorical features of the data. Each node in the Trie represents a category.\n2.  **Leaf-Node KNN Models:** At the leaf nodes of the Trie, where specific combinations of categorical features are found, we fit individual KNN models using only the numerical features.\n3.  **Weighted Prediction:** To classify a new data point, we traverse the Trie based on its categorical features. At each level, we calculate a weighted distance based on available data, ending in a probability score in each leaf node.\n\n\n### Why This Works\n\n-   **No Direct Distance Calculation for Categorical Features:** The Trie structure implicitly captures the relationships between categorical values.\n-   **Localized KNN Models:** By fitting KNN models at the leaf nodes, we ensure that distance calculations are performed only on relevant numerical features.\n-   **Scalability:** The Trie structure efficiently handles a large number of categorical features and values.\n\n## Building a TrieKNN Model\n\nLet's dive into the implementation. We'll start with the `TrieNode` and `Trie` classes, then move on to the KNN model and the training/prediction process.\n\n### Trie Implementation\n\n::: {#f64ed864 .cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nfrom collections import Counter\n\nclass TrieNode:\n    def __init__(self):\n        self.children = {}  # Dictionary to store child nodes\n        self.is_end_of_word = False  # True if the node is the end of a word\n        self.count = 0  # Count of how many times a word has been inserted\n        self.class_counts = {}  # Class counts\n        self.class_weights = {}\n        self.model = None  # Model at leaf nodes\n        self.indexes = []  # Store data indexes belonging to this leaf\n        self.labels = []  # Store data indexes belonging to this leaf\n        self.node_weight = None\n\nclass Trie:\n    def __init__(self):\n        self.root = TrieNode()  # Root node of the Trie\n        self.data_index = 0  # Initialize data index\n\n    def insert(self, word_val, model):\n        current_node = self.root\n        word, val = word_val\n        current_node.count += 1\n\n        # Adding class counts\n        if val not in current_node.class_counts:\n            current_node.class_counts[val] = 0\n        current_node.class_counts[val] += 1\n\n        for char in word:\n            # If the character is not in children, add a new TrieNode\n            if char not in current_node.children:\n                current_node.children[char] = TrieNode()\n            current_node = current_node.children[char]\n\n            # Adding count of instances\n            current_node.count += 1\n\n            # adding class counts\n            if val not in current_node.class_counts:\n                current_node.class_counts[val] = 0\n            current_node.class_counts[val] += 1\n\n        # Mark the end of the word and increment count\n        current_node.is_end_of_word = True\n        current_node.indexes.append(self.data_index)  # Store the data index\n        current_node.labels.append(val)\n        current_node.model = model\n        self.data_index += 1  # Increment data index\n\n    def search(self, word):\n        current_node = self.root\n        for char in word:\n            # If the character doesn't exist in the children, the word doesn't exist\n            if char not in current_node.children:\n                return False\n            current_node = current_node.children[char]\n\n        # Return True if it's the end of a word and the word exists\n        return current_node.is_end_of_word\n\n    def count_word(self, word):\n        current_node = self.root\n        for char in word:\n            # If the character doesn't exist, the word doesn't exist\n            if char not in current_node.children:\n                return 0, current_node.class_counts  # Correctly return class_counts\n            current_node = current_node.children[char]\n\n        # Return the count of the word\n        return current_node.count, current_node.class_counts\n\n    def display(self):\n        # Recursively display the tree\n        def _display(node, word):\n            if node.is_end_of_word:\n                print(f\"Data: {word}, Count: {node.count}, Indexes: {len(node.indexes)} Classes :{node.class_counts} weights:{len(node.class_weights)}\")  # Display indexes too\n            for char, child in node.children.items():\n                _display(child, word + char)  # corrected the display\n\n        _display(self.root, \"\")\n\n    def apply(self, func):\n        \"\"\"\n        Applies a function to all models in the leaf nodes.\n        \"\"\"\n        def _apply(node):\n            if node.is_end_of_word and node.model is not None:\n                func(node)\n            for child in node.children.values():\n                _apply(child)\n\n        _apply(self.root)\n\n    def apply_weight_to_indexes(self, weight):\n        \"\"\"\n        Applies a weight to the indexes based on the percentage of data available.\n        \"\"\"\n        def _apply_weight_to_indexes(node):\n            if node.is_end_of_word:\n                total_count = sum(self.root.children[child].count for child in self.root.children)\n                percentage = node.count / total_count if total_count > 0 else 0\n                weighted_indexes = [(index, weight * percentage) for index in node.indexes]\n                node.class_weights = weighted_indexes  # Corrected this line\n            for child in node.children.values():\n                _apply_weight_to_indexes(child)\n\n        _apply_weight_to_indexes(self.root)\n```\n:::\n\n\n### KNN Model\n\n::: {#77d420dd .cell execution_count=3}\n``` {.python .cell-code}\nclass KNNModel:\n    def __init__(self, k=5):\n        self.data = None\n        self.labels = []\n        self.k = k\n\n    def fit(self, data, indexes, labels):\n        # print(\"Fitting model with indexes:\", len(indexes), \"labels:\", len(labels))\n        self.data = data[indexes].astype(float)\n        self.labels = np.array(labels).astype(float)\n\n    def predict(self, data):\n        # print(\"Predicting with data:\", data)\n        dist_ind = np.sqrt(np.sum((self.data - data) ** 2, axis=1) ** 2)  # euclidean distance\n        main_arr = np.column_stack((self.labels, dist_ind))  # labels with distance\n        main = main_arr[main_arr[:, 1].argsort()]  # sorting based on distance\n        count = Counter(main[0:self.k, 0])  # counting labels\n        sums = np.array(list(count.values()))  # getting counts\n        return sums / np.sum(sums)  # prediction as probability\n```\n:::\n\n\n### Training and Evaluation\n\nHere's how we train and evaluate the TrieKNN model:\n\n::: {#cdc54cad .cell execution_count=4}\n``` {.python .cell-code}\n# Sample data\nn = 10000\ndata = np.array((np.random.choice(['Anything ', 'By ','Chance '], p=[0.6,0.1,0.3],size=n),\n                 np.random.choice(['can', 'go', 'here','lets', 'see', \"it\"], p=[0.1, 0.1, 0.1, 0.2, 0.4, 0.1], size=n),\n                 np.random.normal(3, 1, size=n),\n                 np.random.normal(5, 2, size=n))).T\ny_label = np.random.choice([0,1], p=[0.7, 0.3], size=n)\n\n# Trie training\ntrie = Trie()\nfor X, y in zip(data, y_label):\n    trie.insert((X[:2], y),None)\n\n# Apply weights to indexes\ntrie.apply_weight_to_indexes(0.5)\n\n# Fit models of leaf nodes\ndef add_model(node, data):\n    node.model = KNNModel()\n    node.model.fit(data, node.indexes, node.labels)\n\ndef traverse_and_add_model(node, data):\n    if node.is_end_of_word:\n        add_model(node, data)  # Add model to leaf node\n    for child in node.children.values():\n        traverse_and_add_model(child, data)\n\ntraverse_and_add_model(trie.root, data[:, 2:])\n\n```\n:::\n\n\n### Explanation\n\n-   We create sample data with mixed categorical and numerical features.\n-   We insert each data point into the Trie, using the categorical features as the path.\n-   After the Trie is built, we traverse it and fit a KNN model to the data points stored at each leaf node.\n-   Finally, we can predict the class of new data points by traversing the Trie and using the KNN model at the corresponding leaf node.\n\n## Results and Discussion\n\nLet us display the trie.\n\n::: {.column-margin}\n\n::: {#88dd6e09 .cell execution_count=5}\n``` {.python .cell-code}\ntrie.display()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nData: Anything see, Count: 2383, Indexes: 2383 Classes :{1: 698, 0: 1685} weights:2383\nData: Anything go, Count: 618, Indexes: 618 Classes :{0: 427, 1: 191} weights:618\nData: Anything lets, Count: 1259, Indexes: 1259 Classes :{1: 365, 0: 894} weights:1259\nData: Anything it, Count: 574, Indexes: 574 Classes :{1: 182, 0: 392} weights:574\nData: Anything can, Count: 592, Indexes: 592 Classes :{0: 411, 1: 181} weights:592\nData: Anything here, Count: 617, Indexes: 617 Classes :{0: 438, 1: 179} weights:617\nData: Chance see, Count: 1217, Indexes: 1217 Classes :{1: 358, 0: 859} weights:1217\nData: Chance it, Count: 300, Indexes: 300 Classes :{0: 214, 1: 86} weights:300\nData: Chance go, Count: 302, Indexes: 302 Classes :{0: 227, 1: 75} weights:302\nData: Chance lets, Count: 549, Indexes: 549 Classes :{1: 165, 0: 384} weights:549\nData: Chance here, Count: 284, Indexes: 284 Classes :{0: 188, 1: 96} weights:284\nData: Chance can, Count: 280, Indexes: 280 Classes :{0: 185, 1: 95} weights:280\nData: By see, Count: 421, Indexes: 421 Classes :{1: 112, 0: 309} weights:421\nData: By go, Count: 81, Indexes: 81 Classes :{0: 64, 1: 17} weights:81\nData: By can, Count: 107, Indexes: 107 Classes :{0: 80, 1: 27} weights:107\nData: By it, Count: 90, Indexes: 90 Classes :{0: 64, 1: 26} weights:90\nData: By here, Count: 107, Indexes: 107 Classes :{0: 75, 1: 32} weights:107\nData: By lets, Count: 219, Indexes: 219 Classes :{0: 149, 1: 70} weights:219\n```\n:::\n:::\n\n\n:::\n\nThe model predicted the following values:\n\n::: {.column-margin}\n\n::: {#a8477afe .cell execution_count=6}\n``` {.python .cell-code}\n# Prediction example\ndef predict_with_model(node):\n    predictions = node.model.predict(np.array([2,5]))\n    print(\"Predictions:\", predictions)\n\ntrie.apply(predict_with_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPredictions: [0.8 0.2]\nPredictions: [0.6 0.4]\nPredictions: [0.6 0.4]\nPredictions: [0.6 0.4]\nPredictions: [0.4 0.6]\nPredictions: [0.6 0.4]\nPredictions: [0.8 0.2]\nPredictions: [0.4 0.6]\nPredictions: [1.]\nPredictions: [0.4 0.6]\nPredictions: [0.4 0.6]\nPredictions: [1.]\nPredictions: [0.6 0.4]\nPredictions: [0.6 0.4]\nPredictions: [0.4 0.6]\nPredictions: [0.2 0.8]\nPredictions: [0.4 0.6]\nPredictions: [0.4 0.6]\n```\n:::\n:::\n\n\n:::\nThe predictions will vary on each run. From this we can see that we can use KNN on mixed data types.\n\n## Conclusion: A Promising Path Forward\n\nTrieKNN presents a compelling solution for extending the applicability of KNN to mixed data types. By leveraging the Trie data structure, it avoids direct distance calculations on categorical features, enabling the use of localized KNN models for numerical data.\n\nFurther research could explore:\n\n-   Optimizing the weighting scheme for combining predictions from different Trie levels.\n-   Comparing TrieKNN's performance against other mixed-data KNN approaches on benchmark datasets.\n-   Extending TrieKNN to handle missing data and noisy categorical features.\n\nTrieKNN opens up new possibilities for applying KNN in domains where mixed data types are prevalent, such as healthcare, e-commerce, and social science.\n\n\nResources and further reads:  \n1. [Nomclust R package](https://cran.r-project.org/web/packages/nomclust/nomclust.pdf)  \n2. [An Improved kNN Based on Class Contribution and Feature Weighting](https://ieeexplore.ieee.org/abstract/document/8337394)  \n3. [An Improved Weighted KNN Algorithm for Imbalanced Data Classification](https://ieeexplore.ieee.org/abstract/document/8780580)  \n4. [A weighting approach for KNN classifier](https://ieeexplore.ieee.org/abstract/document/6718270)  \n5. [Unsupervised Outlier Detection for Mixed-Valued Dataset Based on the Adaptive k-Nearest Neighbor Global Network](https://ieeexplore.ieee.org/abstract/document/9739702)  \n6. [A hybrid approach based on k-nearest neighbors and decision tree for software fault prediction](https://journalskuwait.org/kjs/index.php/KJS/article/download/18331/1253)  \n7. [Analysis of Decision Tree and K-Nearest Neighbor Algorithm in the Classification of Breast Cancer](https://pmc.ncbi.nlm.nih.gov/articles/PMC7173366/)  \n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}