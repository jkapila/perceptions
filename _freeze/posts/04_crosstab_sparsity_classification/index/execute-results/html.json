{
  "hash": "e15b1b815efb32e43a623fbe79144ca4",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Corsstab Sparsity for Classification\"\nsubtitle: \"Can our metric help us in making a claissification problem work better ?\"\n\n# Enable CC licence appendix\nlicense: \"CC BY\"\n\n# Default author\nauthor:\n  - name: Jitin Kapila\n    url: www.percieveit.com/about\n\ndate: \"2023-01-03\"\ncategories: [classification, metric, feature selection]\nkeywords: [metric, classification, python, modelling]\nlightbox: true\nformat:\n  html:\n    grid:\n      margin-width: 500px \n\ncode-fold: true\ncode-tools: true\n\n\ndraft: false\n---\n\n# Introduction: A Journey into Data\n\nPicture this: you’re standing on the icy shores of Antarctica, the wind whipping around you as you watch a colony of Palmer Penguins waddling about, oblivious to the data detective work you’re about to embark on. As a data science architect, you’re not just an observer; you’re a sleuth armed with algorithms and insights, ready to unravel the mysteries hidden within data. Today, we’ll transform raw numbers into powerful narratives using CrossTab Sparsity as our guiding compass. This blog post will demonstrate how this metric can revolutionize classification tasks, shedding light on many fascinating datasets—the charming Palmer Penguins and the serious Obes, cerdicards and many more.\n\n# The Power of CrossTab Sparsity\n\n### What is CrossTab Sparsity?\n\nCrossTab Sparsity isn’t just a fancy term that sounds good at dinner parties; it’s a statistical measure that helps us peer into the intricate relationships between categorical variables. Imagine it as a magnifying glass that reveals how different categories interact within a contingency table. Understanding these interactions is crucial in classification tasks, where the right features can make or break your model (and your day).\n\n**Why Does It Matter?**\n\nIn the world of data science, especially in classification, selecting relevant features is like picking the right ingredients for a gourmet meal—get it wrong, and you might end up with something unpalatable. CrossTab Sparsity helps us achieve this by:\n\n- Highlighting Relationships: It’s like having a friend who always points out when two people are meant to be together—understanding how features interact with the target variable.\n- Streamlining Models: Reducing complexity by focusing on significant features means less time spent untangling spaghetti code.\n- Enhancing Interpretability: Making models easier to understand and explain to stakeholders is like translating tech jargon into plain English—everyone appreciates that!\n\n# Data Overview: Our Data People at work here\n\n### The Datasets\n\nData 1: [Estimation of Obesity Levels Based On Eating Habits and Physical Condition](https://archive.ics.uci.edu/dataset/544/estimation+of+obesity+levels+based+on+eating+habits+and+physical+condition)\n\n_Little bit about the data:_ This dataset, shared on 8/26/2019, looks at obesity levels in people from Mexico, Peru, and Colombia based on their eating habits and physical health. It includes 2,111 records with 16 features, and classifies individuals into different obesity levels, from insufficient weight to obesity type III. Most of the data (77%) was created using a tool, while the rest (23%) was collected directly from users online.\n\nData 2: [Predict Students' Dropout and Academic Success](https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success)\n\n_Little bit about the data:_ This dataset, shared on 12/12/2021, looks at factors like students' backgrounds, academic path, and socio-economic status to predict whether they'll drop out or succeed in their studies. With 4,424 records across 36 features, it covers students from different undergrad programs. The goal is to use machine learning to spot at-risk students early, so schools can offer support. The data has been cleaned and doesn’t have any missing values. It’s a classification task with three outcomes: dropout, still enrolled, or graduated\n\n**Key Features**:\n\n- Multiclass: Both data set cater a multi class problems wiht `NObeyesdad` and `Target` columns\n- Mixed Data Type: A good mix of categorical and continuous varaibels are availabel for usage.\n- Sizeable: More than 2 K rows are availabel for testing.\n\n# Exploratory Data Analysis (EDA): Setting the Stage\n\nBefore we dive into model creation, let’s explore our dataset through some quick EDA. Think of this as getting to know your non-obese friends before inviting them to a party.\n\n### EDA for Obestiy Data\n\nHere’s a brief code snippet to perform essential EDA on the Obesity dataset:\n\n\n\n::: {#0c2ce4c9 .cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Loading data and genrating basic descriptives\"}\n# Load the Obesity data\n# raw_df = pd.read_csv('ObesityDataSet_raw_and_data_sinthetic.csv')\n# target = 'NObeyesdad'\n\n# Load Students data\n\n# Load Credit data\nraw_data = sm.datasets.get_rdataset(\"credit_data\",'modeldata')\nraw_df = raw_data.data\ntarget = 'Status'\n\n# # Load Palmer penguins data\n# raw_data = sm.datasets.get_rdataset(\"penguins\",'palmerpenguins')\n# raw_df = raw_data.data\n# target = 'species'\n\n\n# # Load Credit data\n# raw_data = sm.datasets.get_rdataset(\"CreditCard\",'AER')\n# raw_df = raw_data.data\n# target = 'card'\n\n\n# setting thigns up for aal the next steps\nraw_df[target] = raw_df[target].astype('category') \nprint('No of data points available to work:',raw_df.shape)\ndisplay(raw_df.head())\n\n\n# Summary statistics\ndisplay(raw_df.describe())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNo of data points available to work: (4454, 14)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Status</th>\n      <th>Seniority</th>\n      <th>Home</th>\n      <th>Time</th>\n      <th>Age</th>\n      <th>Marital</th>\n      <th>Records</th>\n      <th>Job</th>\n      <th>Expenses</th>\n      <th>Income</th>\n      <th>Assets</th>\n      <th>Debt</th>\n      <th>Amount</th>\n      <th>Price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>good</td>\n      <td>9</td>\n      <td>rent</td>\n      <td>60</td>\n      <td>30</td>\n      <td>married</td>\n      <td>no</td>\n      <td>freelance</td>\n      <td>73</td>\n      <td>129.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>800</td>\n      <td>846</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>good</td>\n      <td>17</td>\n      <td>rent</td>\n      <td>60</td>\n      <td>58</td>\n      <td>widow</td>\n      <td>no</td>\n      <td>fixed</td>\n      <td>48</td>\n      <td>131.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1000</td>\n      <td>1658</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>bad</td>\n      <td>10</td>\n      <td>owner</td>\n      <td>36</td>\n      <td>46</td>\n      <td>married</td>\n      <td>yes</td>\n      <td>freelance</td>\n      <td>90</td>\n      <td>200.0</td>\n      <td>3000.0</td>\n      <td>0.0</td>\n      <td>2000</td>\n      <td>2985</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>good</td>\n      <td>0</td>\n      <td>rent</td>\n      <td>60</td>\n      <td>24</td>\n      <td>single</td>\n      <td>no</td>\n      <td>fixed</td>\n      <td>63</td>\n      <td>182.0</td>\n      <td>2500.0</td>\n      <td>0.0</td>\n      <td>900</td>\n      <td>1325</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>good</td>\n      <td>0</td>\n      <td>rent</td>\n      <td>36</td>\n      <td>26</td>\n      <td>single</td>\n      <td>no</td>\n      <td>fixed</td>\n      <td>46</td>\n      <td>107.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>310</td>\n      <td>910</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Seniority</th>\n      <th>Time</th>\n      <th>Age</th>\n      <th>Expenses</th>\n      <th>Income</th>\n      <th>Assets</th>\n      <th>Debt</th>\n      <th>Amount</th>\n      <th>Price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>4454.000000</td>\n      <td>4454.000000</td>\n      <td>4454.000000</td>\n      <td>4454.000000</td>\n      <td>4073.000000</td>\n      <td>4407.000000</td>\n      <td>4436.000000</td>\n      <td>4454.000000</td>\n      <td>4454.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>7.986753</td>\n      <td>46.438707</td>\n      <td>37.080377</td>\n      <td>55.573417</td>\n      <td>141.687699</td>\n      <td>5403.979351</td>\n      <td>343.025924</td>\n      <td>1038.918276</td>\n      <td>1462.780198</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>8.174306</td>\n      <td>14.655462</td>\n      <td>10.984598</td>\n      <td>19.515634</td>\n      <td>80.748398</td>\n      <td>11574.418141</td>\n      <td>1245.991541</td>\n      <td>474.545999</td>\n      <td>628.128120</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>6.000000</td>\n      <td>18.000000</td>\n      <td>35.000000</td>\n      <td>6.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>100.000000</td>\n      <td>105.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>2.000000</td>\n      <td>36.000000</td>\n      <td>28.000000</td>\n      <td>35.000000</td>\n      <td>90.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>700.000000</td>\n      <td>1117.250000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>5.000000</td>\n      <td>48.000000</td>\n      <td>36.000000</td>\n      <td>51.000000</td>\n      <td>125.000000</td>\n      <td>3000.000000</td>\n      <td>0.000000</td>\n      <td>1000.000000</td>\n      <td>1400.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>12.000000</td>\n      <td>60.000000</td>\n      <td>45.000000</td>\n      <td>72.000000</td>\n      <td>170.000000</td>\n      <td>6000.000000</td>\n      <td>0.000000</td>\n      <td>1300.000000</td>\n      <td>1691.500000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>48.000000</td>\n      <td>72.000000</td>\n      <td>68.000000</td>\n      <td>180.000000</td>\n      <td>959.000000</td>\n      <td>300000.000000</td>\n      <td>30000.000000</td>\n      <td>5000.000000</td>\n      <td>11140.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.column-margin}\n\n::: {.callout collapse=\"true\"}\n# Some EDA for the data\n\n::: {#1316e322 .cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"EDA code\"}\n# Visualize target data distribution\nsns.countplot(data=raw_df, x=target, hue=target, palette='Set2',)\nplt.title(f'Distribution of {target} levels')\nplt.xticks(rotation=45)\nplt.show()\n\n# Visualize the distribution of numerical variables\nsns.pairplot(raw_df, hue=target)\nplt.show()\n\n\n# Heatmap to check for correlations between numeric variables\ncorr = raw_df.corr('kendall',numeric_only=True)\nsns.heatmap(corr, annot=True, cmap='coolwarm')\nplt.title('Kendall Correlation Heatmap')\nplt.show()\n\n# Gettign Categorical data\ncategorical_columns = raw_df.select_dtypes(include='object').columns\n\n# Plot categorical variables with respect to the target variable\nfor col in categorical_columns:\n    plt.figure(figsize=(12, 5))\n    sns.countplot(data=raw_df,x=col, hue=target)\n    plt.title(f\"Countplot of {col} with respect to {target}\")\n    plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=602 height=469}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-2.png){width=2224 height=2120}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-3.png){width=584 height=482}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-4.png){width=974 height=449}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-5.png){width=974 height=449}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-6.png){width=974 height=449}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-7.png){width=974 height=449}\n:::\n:::\n\n\n:::\n\n:::\n\n\n# Model Creation: Establishing a Baseline\n\nWith our exploratory analysis complete, we’re ready to create our baseline model using logistic regression with Statsmodels. This initial model will serve as our reference point—like setting up a benchmark for your favorite video game.\n\n::: {#28a4c23e .cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Splitting data and traing a default Multinomila Logit model on our data\"}\ndata_df = raw_df.dropna().reset_index(drop=True)\ndata_df[target] = data_df[target].cat.codes\n# X = data_df[['bill_length_mm','bill_depth_mm','flipper_length_mm','body_mass_g']] \n\ndata_df_test = data_df.sample(frac=0.1,random_state=3)\ndata_df_train = data_df.drop(data_df_test.index)\n\n# Using MN logistic regression model using formula API\n# This would essentially bold down to pair wise logsitic regression\nlogit_model = sm.MNLogit.from_formula(\n    f\"{target} ~ {' + '.join([col for col in data_df_train.columns if col != target])}\", \n    data=data_df_train\n).fit_regularized()\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOptimization terminated successfully    (Exit mode 0)\n            Current function value: 0.41901887640753865\n            Iterations: 134\n            Function evaluations: 168\n            Gradient evaluations: 134\n```\n:::\n:::\n\n\n::: {.column-margin}\n\n::: {.callout collapse=\"true\"}\n# Base model summary for geeks\n\n::: {#c9be95c6 .cell execution_count=5}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Display summary\"}\ndisplay(logit_model.summary())\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<table class=\"simpletable\">\n<caption>MNLogit Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>        <td>Status</td>      <th>  No. Observations:  </th>   <td>  3635</td>  \n</tr>\n<tr>\n  <th>Model:</th>                <td>MNLogit</td>     <th>  Df Residuals:      </th>   <td>  3612</td>  \n</tr>\n<tr>\n  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>    22</td>  \n</tr>\n<tr>\n  <th>Date:</th>            <td>Wed, 05 Feb 2025</td> <th>  Pseudo R-squ.:     </th>   <td>0.2643</td>  \n</tr>\n<tr>\n  <th>Time:</th>                <td>23:54:01</td>     <th>  Log-Likelihood:    </th>  <td> -1523.1</td> \n</tr>\n<tr>\n  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th>  <td> -2070.3</td> \n</tr>\n<tr>\n  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>1.577e-217</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n        <th>Status=1</th>          <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>Intercept</th>            <td>    0.6925</td> <td>    0.778</td> <td>    0.890</td> <td> 0.373</td> <td>   -0.832</td> <td>    2.217</td>\n</tr>\n<tr>\n  <th>Home[T.other]</th>        <td>    0.3632</td> <td>    0.594</td> <td>    0.612</td> <td> 0.541</td> <td>   -0.800</td> <td>    1.527</td>\n</tr>\n<tr>\n  <th>Home[T.owner]</th>        <td>    1.1680</td> <td>    0.574</td> <td>    2.036</td> <td> 0.042</td> <td>    0.044</td> <td>    2.293</td>\n</tr>\n<tr>\n  <th>Home[T.parents]</th>      <td>    0.9238</td> <td>    0.584</td> <td>    1.582</td> <td> 0.114</td> <td>   -0.221</td> <td>    2.068</td>\n</tr>\n<tr>\n  <th>Home[T.priv]</th>         <td>    0.4643</td> <td>    0.596</td> <td>    0.780</td> <td> 0.436</td> <td>   -0.703</td> <td>    1.632</td>\n</tr>\n<tr>\n  <th>Home[T.rent]</th>         <td>    0.5439</td> <td>    0.578</td> <td>    0.941</td> <td> 0.347</td> <td>   -0.589</td> <td>    1.677</td>\n</tr>\n<tr>\n  <th>Marital[T.married]</th>   <td>    0.7602</td> <td>    0.462</td> <td>    1.646</td> <td> 0.100</td> <td>   -0.145</td> <td>    1.665</td>\n</tr>\n<tr>\n  <th>Marital[T.separated]</th> <td>   -0.3945</td> <td>    0.515</td> <td>   -0.766</td> <td> 0.444</td> <td>   -1.404</td> <td>    0.615</td>\n</tr>\n<tr>\n  <th>Marital[T.single]</th>    <td>    0.3823</td> <td>    0.468</td> <td>    0.817</td> <td> 0.414</td> <td>   -0.535</td> <td>    1.300</td>\n</tr>\n<tr>\n  <th>Marital[T.widow]</th>     <td>    0.0874</td> <td>    0.594</td> <td>    0.147</td> <td> 0.883</td> <td>   -1.077</td> <td>    1.251</td>\n</tr>\n<tr>\n  <th>Records[T.yes]</th>       <td>   -1.8704</td> <td>    0.116</td> <td>  -16.113</td> <td> 0.000</td> <td>   -2.098</td> <td>   -1.643</td>\n</tr>\n<tr>\n  <th>Job[T.freelance]</th>     <td>   -0.2869</td> <td>    0.130</td> <td>   -2.206</td> <td> 0.027</td> <td>   -0.542</td> <td>   -0.032</td>\n</tr>\n<tr>\n  <th>Job[T.others]</th>        <td>   -0.5867</td> <td>    0.231</td> <td>   -2.538</td> <td> 0.011</td> <td>   -1.040</td> <td>   -0.134</td>\n</tr>\n<tr>\n  <th>Job[T.partime]</th>       <td>   -1.4870</td> <td>    0.138</td> <td>  -10.798</td> <td> 0.000</td> <td>   -1.757</td> <td>   -1.217</td>\n</tr>\n<tr>\n  <th>Seniority</th>            <td>    0.0876</td> <td>    0.009</td> <td>   10.010</td> <td> 0.000</td> <td>    0.070</td> <td>    0.105</td>\n</tr>\n<tr>\n  <th>Time</th>                 <td>    0.0005</td> <td>    0.004</td> <td>    0.134</td> <td> 0.894</td> <td>   -0.007</td> <td>    0.008</td>\n</tr>\n<tr>\n  <th>Age</th>                  <td>   -0.0083</td> <td>    0.006</td> <td>   -1.457</td> <td> 0.145</td> <td>   -0.020</td> <td>    0.003</td>\n</tr>\n<tr>\n  <th>Expenses</th>             <td>   -0.0177</td> <td>    0.003</td> <td>   -5.863</td> <td> 0.000</td> <td>   -0.024</td> <td>   -0.012</td>\n</tr>\n<tr>\n  <th>Income</th>               <td>    0.0078</td> <td>    0.001</td> <td>    9.622</td> <td> 0.000</td> <td>    0.006</td> <td>    0.009</td>\n</tr>\n<tr>\n  <th>Assets</th>               <td> 2.111e-05</td> <td> 7.91e-06</td> <td>    2.671</td> <td> 0.008</td> <td> 5.62e-06</td> <td> 3.66e-05</td>\n</tr>\n<tr>\n  <th>Debt</th>                 <td>   -0.0001</td> <td> 4.44e-05</td> <td>   -3.157</td> <td> 0.002</td> <td>   -0.000</td> <td>-5.31e-05</td>\n</tr>\n<tr>\n  <th>Amount</th>               <td>   -0.0021</td> <td>    0.000</td> <td>  -10.464</td> <td> 0.000</td> <td>   -0.003</td> <td>   -0.002</td>\n</tr>\n<tr>\n  <th>Price</th>                <td>    0.0010</td> <td>    0.000</td> <td>    6.648</td> <td> 0.000</td> <td>    0.001</td> <td>    0.001</td>\n</tr>\n</table>\n```\n:::\n:::\n\n\n:::\n\n\n:::\n\n\n### Evaluating Model Performance\n\nTo gauge our models' effectiveness, we’ll employ various metrics such as accuracy, precision, recall, and F1-score. A confusion matrix will help visualize how well our models perform in classifying outcomes—think of it as a report card for your model!\n\n::: {#1d8635f6 .cell execution_count=6}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Evaluating the Logit model\"}\n# Predict on test data\nbase_preds = logit_model.predict(data_df_test).idxmax(axis=1)\ny_test = data_df_test[target]\n\n# Evaluate the model\naccuracy_orig = accuracy_score(y_test, base_preds)\nreport_orig = classification_report(y_test, base_preds)\n\nprint(\"Accuracy:\", accuracy_orig)\nprint(\"Classification Report:\")\nprint(report_orig)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 0.8292079207920792\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.67      0.52      0.58        93\n           1       0.86      0.92      0.89       311\n\n    accuracy                           0.83       404\n   macro avg       0.77      0.72      0.74       404\nweighted avg       0.82      0.83      0.82       404\n\n```\n:::\n:::\n\n\n# Loking for some _Improvments!_\n\n### Feature Selection Using CrossTab Sparsity\n\nNow comes the exciting part—using CrossTab Sparsity to refine our feature selection process! It’s like cleaning up your closet and only keeping the clothes that spark joy (thank you, Marie Kondo). [^1]\n\n[^1]: This is based on work in Unique Metric for Health Analysis with Optimization of Clustering Activity and Cross Comparison of Results from Different Approach. [Paper Link](https://arxiv.org/abs/1810.03419)]\n\n::: {#b18e72ba .cell execution_count=7}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the code of Crosstab Sparsity\"}\ndef crosstab_sparsity(df, clusters, numeric_bin='decile', method=\"fd\", exhaustive=False):\n    \"\"\"\n    Crostab Sparcisty metric calculation based on our Paper\n\n\n    :param df : Data framr that needs to process\n    :param cluster: data that needs to segregate the variables\n    :param numeric_bin: Binning strategy of numerical variable. 'decial is default. \n                        ['decile','decile_20','hist_m','hist_n','num_val']\n    :param method: histogram splitter using for 'hist_m'. One of ['fd','rice','scott'],    \n                    defaults to 'fd'. Refer [Numpy](https://numpy.org/doc/stable/reference/generated/numpy.histogram_bin_edges.html#numpy.histogram_bin_edges) for more options! \n    :param exhaustive: Add crosstab tabel for further inspection!\n\n    \"\"\"\n    if isinstance(df, pd.DataFrame):\n        col_names = df.columns\n        df_flag = True\n    else:\n        col_names = np.arange(1, df.shape[1] + 1)\n        df_flag = False\n\n    if df.shape[0] != len(clusters):\n        raise ValueError('Clusters and number of observations are different. Cannot Proceed!')\n\n    n_d = df.shape[0]\n    k = len(np.unique(clusters))\n\n    # Initialize the score dataframe\n    score_df = pd.DataFrame({\n        'Columns': col_names,\n        'l': -1, 'n_v': -1, 'med_n_v': -1, 'min_n_v': -1, 'avg_n_v': -1, 'max_n_v': -1,\n        'k': k, 'n_d': n_d, 'seggregation': -1, 'explaination': -1, 'inv_seggregation': -1, 'inv_explaination': -1\n    })\n\n    if isinstance(numeric_bin, int):\n        parts = numeric_bin\n        numeric_bin = 'num_val'\n        print(\"CSP calculated with user-defined breaks!\\n\")\n    else:\n        print(f\"CSP calculated with {numeric_bin} for breaks!\\n\")\n\n    if exhaustive:\n        cstables = {}\n\n    for col_ in col_names:\n        if df_flag:\n            col_val = df[col_]\n            if np.issubdtype(col_val.dtype, np.number):\n\n                # Determine bins\n                if numeric_bin == 'decile':\n                    bins = np.unique(np.percentile(col_val.dropna(), np.arange(0, 101, 10)))\n                elif numeric_bin == 'decile_20':\n                    bins = np.unique(np.percentile(col_val.dropna(), np.arange(0, 101, 5)))\n                elif numeric_bin == 'hist_m':\n                    bins = np.histogram(col_val.dropna(), bins=method)[1]\n                elif numeric_bin == 'hist_n':\n                    bins = np.histogram(col_val.dropna())[1]\n                elif numeric_bin == 'num_val':\n                    bins = np.histogram(col_val.dropna(), bins=parts)[1]\n                else:\n                    bins = np.histogram(col_val.dropna(), bins=10)[1]\n\n                # Create categorical bins\n                col_val = pd.cut(col_val, bins=bins, include_lowest=True)\n\n        # Create contingency table\n        cstable = pd.crosstab(col_val, clusters)\n\n        # Calculate the med value and other metrics\n        med_ = np.median(cstable.values.flatten())\n        l = cstable.shape[0]\n        n_v = np.sum(cstable.values > med_)\n\n        # Update the score_df\n        score_df.loc[score_df['Columns'] == col_, \"avg_n_v\"] = np.mean(cstable.values)\n        score_df.loc[score_df['Columns'] == col_, \"min_n_v\"] = np.min(cstable.values)\n        score_df.loc[score_df['Columns'] == col_, \"max_n_v\"] = np.max(cstable.values)\n        score_df.loc[score_df['Columns'] == col_, \"med_n_v\"] = med_\n        score_df.loc[score_df['Columns'] == col_, \"l\"] = l\n        score_df.loc[score_df['Columns'] == col_, \"n_v\"] = n_v\n\n        score_df.loc[score_df['Columns'] == col_, \"seggregation\"] = n_v / max(l, k)\n        score_df.loc[score_df['Columns'] == col_, \"inv_seggregation\"] = max(l, k) / n_v\n        score_df.loc[score_df['Columns'] == col_, \"explaination\"] = np.log(n_d / (l * k))\n        score_df.loc[score_df['Columns'] == col_, \"inv_explaination\"] = np.log((l * k) / n_d)\n\n        if exhaustive:\n            cstables[col_] = cstable\n\n    # Calculate metrics\n    score_df['metric'] = score_df['seggregation'] * score_df['explaination']\n\n    print(f'Scores for {k} groups(s) is : {sum(score_df[\"metric\"])}')\n\n    # Sort by 'metric' in descending order\n    score_df = score_df.sort_values(by='metric', ascending=False).reset_index(drop=True)\n\n    # Create the result dictionary\n    csmetric = {\n        'score': sum(score_df['metric']),\n        'n_clusters': k,\n        'n_d': n_d,\n        'scores': score_df\n    }\n\n    if exhaustive:\n        tabs = {col_: cstables[col_] for col_ in score_df['Columns']}\n        csmetric['cstables'] = tabs\n\n    return csmetric\n\n```\n:::\n\n\n### Standared Steps for Feature Selection\n\n1. **Calculate CrossTab Sparsity**: For each feature against the target variable.\n2. **Select Features**: Based on sparsity scores that indicate significant interactions with the target variable.\n3. **Recreate Models**: Train new models using only the selected features—less is often more!\n\nHere we go!!!\n\n::: {#73246df7 .cell execution_count=8}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Doing what needs to Done Code ;)\"}\nsns.set_style(\"white\")\nsns.set_context(\"paper\")\n# Calculating Crostab sparsity for each Column\nresults = crosstab_sparsity(data_df_train,data_df_train[target])\n\n# presenting results for consumption\ndf_long = pd.melt(results['scores'], id_vars=['Columns'], value_vars=['seggregation', 'explaination', 'metric'],\n                  var_name='Metric', value_name='Value')\n\n# Adding jitter: small random noise to 'Columns' (x-axis)\ndf_long['values_jittered'] = df_long['Value'] + np.random.uniform(-0.1, 0.1, size=len(df_long))\n\n# Create a seaborn scatter plot with jitter, more professional color palette, and transparency\nplt.figure(figsize=(10, 5))\nsns.scatterplot(x='Columns', y='values_jittered', hue='Metric', style='Metric',\n        data=df_long, s=100, alpha=0.7, palette='deep')\n\n# Title and labels\nplt.title('Metrics by Columns', fontsize=16)\nplt.xticks(rotation=45) \nplt.xlabel('Columns', fontsize=10)\nplt.ylabel('Value', fontsize=10)\n\n# Display legend outside the plot for better readability\nplt.legend(title='Metric', bbox_to_anchor=(1.05, 1), loc='upper left')\n\n# Show the plot\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCSP calculated with decile for breaks!\n\nScores for 2 groups(s) is : 78.3919737430718\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-2.png){width=949 height=470}\n:::\n:::\n\n\n### And Drum Rolls pelase!!!\nUsing just top 5 varaibles we are getting almost similar or better overall accuracy. This amounts to greatly simplifing the models and clearly explain why some variable are not useful for modeling.\n\n::: {#d9b49346 .cell execution_count=9}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"And finally training and evaluating with drum rolls\"}\nlogit_model_rev = sm.MNLogit.from_formula(f\"{target} ~ {' + '.join(results['scores'].loc[:5,'Columns'].values)}\", \n    data=data_df_train\n).fit_regularized()\n\n# Predict on test data\nchallenger_preds = logit_model_rev.predict(data_df_test).idxmax(axis=1)\ny_test = data_df_test[target]\n\n# Evaluate the model\naccuracy_new = accuracy_score(y_test, challenger_preds)\nreport_new = classification_report(y_test, challenger_preds)\n\nprint(\"Accuracy:\", accuracy_new)\nprint(\"Classification Report:\")\nprint(report_new)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOptimization terminated successfully    (Exit mode 0)\n            Current function value: 0.4721163743771395\n            Iterations: 120\n            Function evaluations: 130\n            Gradient evaluations: 120\nAccuracy: 0.8242574257425742\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.67      0.46      0.55        93\n           1       0.85      0.93      0.89       311\n\n    accuracy                           0.82       404\n   macro avg       0.76      0.70      0.72       404\nweighted avg       0.81      0.82      0.81       404\n\n```\n:::\n:::\n\n\n:::{.column-margin}\n\n::: {.callout collapse=\"true\"}\n# Summary of retrained model\n\n::: {#cc865229 .cell execution_count=10}\n``` {.python .cell-code}\ndisplay(logit_model_rev.summary())\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<table class=\"simpletable\">\n<caption>MNLogit Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>        <td>Status</td>      <th>  No. Observations:  </th>   <td>  3635</td>  \n</tr>\n<tr>\n  <th>Model:</th>                <td>MNLogit</td>     <th>  Df Residuals:      </th>   <td>  3619</td>  \n</tr>\n<tr>\n  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>    15</td>  \n</tr>\n<tr>\n  <th>Date:</th>            <td>Wed, 05 Feb 2025</td> <th>  Pseudo R-squ.:     </th>   <td>0.1711</td>  \n</tr>\n<tr>\n  <th>Time:</th>                <td>23:54:02</td>     <th>  Log-Likelihood:    </th>  <td> -1716.1</td> \n</tr>\n<tr>\n  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th>  <td> -2070.3</td> \n</tr>\n<tr>\n  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>3.140e-141</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n        <th>Status=1</th>          <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>Intercept</th>            <td>    1.6116</td> <td>    0.670</td> <td>    2.407</td> <td> 0.016</td> <td>    0.299</td> <td>    2.924</td>\n</tr>\n<tr>\n  <th>Records[T.yes]</th>       <td>   -1.6467</td> <td>    0.103</td> <td>  -16.039</td> <td> 0.000</td> <td>   -1.848</td> <td>   -1.445</td>\n</tr>\n<tr>\n  <th>Job[T.freelance]</th>     <td>   -0.3687</td> <td>    0.117</td> <td>   -3.152</td> <td> 0.002</td> <td>   -0.598</td> <td>   -0.139</td>\n</tr>\n<tr>\n  <th>Job[T.others]</th>        <td>   -1.2584</td> <td>    0.205</td> <td>   -6.125</td> <td> 0.000</td> <td>   -1.661</td> <td>   -0.856</td>\n</tr>\n<tr>\n  <th>Job[T.partime]</th>       <td>   -1.9656</td> <td>    0.127</td> <td>  -15.461</td> <td> 0.000</td> <td>   -2.215</td> <td>   -1.716</td>\n</tr>\n<tr>\n  <th>Marital[T.married]</th>   <td>    0.4401</td> <td>    0.426</td> <td>    1.032</td> <td> 0.302</td> <td>   -0.396</td> <td>    1.276</td>\n</tr>\n<tr>\n  <th>Marital[T.separated]</th> <td>   -0.4259</td> <td>    0.482</td> <td>   -0.884</td> <td> 0.377</td> <td>   -1.370</td> <td>    0.518</td>\n</tr>\n<tr>\n  <th>Marital[T.single]</th>    <td>    0.1827</td> <td>    0.434</td> <td>    0.421</td> <td> 0.674</td> <td>   -0.668</td> <td>    1.034</td>\n</tr>\n<tr>\n  <th>Marital[T.widow]</th>     <td>    0.1837</td> <td>    0.552</td> <td>    0.333</td> <td> 0.739</td> <td>   -0.898</td> <td>    1.265</td>\n</tr>\n<tr>\n  <th>Home[T.other]</th>        <td>    0.3608</td> <td>    0.536</td> <td>    0.673</td> <td> 0.501</td> <td>   -0.690</td> <td>    1.411</td>\n</tr>\n<tr>\n  <th>Home[T.owner]</th>        <td>    1.3946</td> <td>    0.519</td> <td>    2.687</td> <td> 0.007</td> <td>    0.377</td> <td>    2.412</td>\n</tr>\n<tr>\n  <th>Home[T.parents]</th>      <td>    0.8861</td> <td>    0.527</td> <td>    1.683</td> <td> 0.092</td> <td>   -0.146</td> <td>    1.918</td>\n</tr>\n<tr>\n  <th>Home[T.priv]</th>         <td>    0.4901</td> <td>    0.540</td> <td>    0.907</td> <td> 0.364</td> <td>   -0.569</td> <td>    1.549</td>\n</tr>\n<tr>\n  <th>Home[T.rent]</th>         <td>    0.3673</td> <td>    0.520</td> <td>    0.706</td> <td> 0.480</td> <td>   -0.652</td> <td>    1.386</td>\n</tr>\n<tr>\n  <th>Debt</th>                 <td>-6.929e-05</td> <td> 3.79e-05</td> <td>   -1.828</td> <td> 0.068</td> <td>   -0.000</td> <td> 4.99e-06</td>\n</tr>\n<tr>\n  <th>Time</th>                 <td>   -0.0220</td> <td>    0.003</td> <td>   -6.878</td> <td> 0.000</td> <td>   -0.028</td> <td>   -0.016</td>\n</tr>\n</table>\n```\n:::\n:::\n\n\n:::\n\n:::\n\n\n# Impact on Model Accuracy\n\nAfter applying feature selection based on CrossTab Sparsity, we’ll compare the accuracy of our new models against our baseline models. This comparison will reveal how effectively CrossTab Sparsity enhances classification performance.\n\n### Results and Discussion: Unveiling Insights\n\n**Model Comparison Table**\n\nAfter implementing CrossTab Sparsity in our feature selection process, let’s take a look at the results:\n\n::: {#6a9ae553 .cell execution_count=11}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Comparision Code\"}\nmetrics = {\n    \"Metric\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"],\n    \"Baseline Model with all Parameters\": [\n        accuracy_score(y_test, base_preds),\n        precision_score(y_test, base_preds, average='weighted'),\n        recall_score(y_test, base_preds, average='weighted'),\n        f1_score(y_test, base_preds, average='weighted'),\n    ],\n    \"Challenger Model with only 5 Variables\": [\n        accuracy_score(y_test, challenger_preds),\n        precision_score(y_test, challenger_preds, average='weighted'),\n        recall_score(y_test, challenger_preds, average='weighted'),\n        f1_score(y_test, challenger_preds, average='weighted'),\n    ]\n}\ndisplay(pd.DataFrame(metrics).round(4).set_index('Metric').T)\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>Metric</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1-Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Baseline Model with all Parameters</th>\n      <td>0.8292</td>\n      <td>0.8189</td>\n      <td>0.8292</td>\n      <td>0.8211</td>\n    </tr>\n    <tr>\n      <th>Challenger Model with only 5 Variables</th>\n      <td>0.8243</td>\n      <td>0.8113</td>\n      <td>0.8243</td>\n      <td>0.8119</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n**Insights Gained**\n\nThrough this analysis, several key insights emerge:\n\n::: {#021d7bff .cell execution_count=12}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Column Reduction %\"}\nn_orig = raw_df.shape[1]-1\ndisplay(f'Reduction of similar accuracy from {n_orig} to 5 i.e {(n_orig-5)*100/n_orig:.2f}% reduction')\n```\n\n::: {.cell-output .cell-output-display}\n```\n'Reduction of similar accuracy from 13 to 5 i.e 61.54% reduction'\n```\n:::\n:::\n\n\n1. **Feature Interactions Matter**: The selected features based on CrossTab Sparsity significantly improved model accuracy—like finding out which ingredients make your favorite dish even better!\n2. **Simplicity is Key**: By focusing on relevant features, we enhance accuracy while simplifying model interpretation—because nobody likes unnecessary complexity.\n3. **Real-World Applications**: These findings have practical implications in fields such as environmental science where classification plays a critical role—helping us make better decisions for our planet.\n\n### Conclusion: The Road Ahead\n\nIn conclusion, this blog has illustrated how CrossTab Sparsity can be a game-changer in classification tasks using the Obesity dataset. By leveraging this metric for feature selection, we achieved notable improvements in model performance—proof that sometimes less really is more!\n\n**Future Work: Expanding Horizons**\n\nAs we look ahead, there are exciting avenues to explore:\n\n- Investigating regression problems using CrossTab Sparsity.\n- Comparing its effectiveness with other feature selection methods such as Recursive Feature Elimination (RFE) or LASSO regression.\n\nBy continuing this journey into data science, we not only enhance our technical skills but also contribute valuable insights that can drive meaningful change in various industries.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}